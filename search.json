[{"title":"How to solve problems?","url":"/2022/09/12/How-to-solve-problems/","content":"How to solve problems? (TBC)Nowadays, people are hired not only to realize functions, but more to solve problems. Besides that, in personal life, we are around by various problems. One day, if we are given a problem, how to solve it effectively? And how to be a good problem solver?\n1.  Types of ProblemsBefore rushing into how to solve problems, let’s start with the types of problems.\n\nThis idea is coming form Problem-Solving: The Owner’s Manual by Pierce Howard, which can be combined with Bohn’s talking about the link between causes and effects in class. Figure 1 shows the types of problems with the general activities to solve them in blue rectangles. Problems can be divided into two parts: solutions are known and unknown. In the first part where solutions are known, the ideal type is the best solution to a problem is obvious based on complete knowledge, then we implement it. If the best solution is not obvious, where we have solutions with certain outcomes, then we should compare and decide one best solution. Sometimes, we have solutions with uncertain outcomes. What we must react is to decide which solution has the highest probability of success. One example here can be how to choose the best design of the garden for maximum residents’ satisfaction. If the potential solutions are in a jumbled list without priority, what we do can be determining the priority order based on the effects. But complete knowledge situation rarely happens. Most times problems come in a way we are not familiar with. If the cause is unknown, the first thing we should do is finding the cause. If the cause is known or irrelevant, we can generate ideas that could fix the problem. So, the messages to take can be different based on the types of problems, which depends on our state of knowledge. \n2.  The process to solve problemsProblem solving is a process, consisting of 4 key stages, which can be divided to problem definition, solutions generation, evaluating and choosing best solution, and solution validation.\n\nFigure 2: 4 stages to solve problems\nTo solve problem effectively and creatively, the first stage should be defining the problem we want to solve. Identify the stakeholders who have a say in the way we solve this problem and what they want. Then describe the vision which should include is the trouble trying to solve now. By asking whys to get the potential root causes we need to address. Then try to write down what we want to achieve in the future. Sometimes we can even get the key factors based on prior knowledge and experience. Some tools may help a lot here, like fishbone diagram to the causes. Some people tend to answer the first problem without checking if this is the right we really want to solve.\nIn the next stage, generating solutions. After discovering the real problem that we really want to solve, try to concentrate on generating many solutions such as a target 20 potential alternatives and should not evaluate them at all. Many times, an idea that would have been thrown away immediately, if evaluating properly, can develop into an excellent solution.\nThe third stage should be evaluating and choosing the best solution. In evaluating step, some constraints should be taken into consideration like cost, safety, policy, etc. Write the good points and bad points of every solution and state the unique point after looking through all the constraints and targets for all potential solutions. Sometimes, numbers and facts will help us decide the final solutions if the solution can show certain outcomes. For solutions with uncertain outcomes, decision to the best one may be made by feeling, which can be the result of our skills, experience, and knowledgement, place in the right limiting circumstance. Some tools like SWOT can help us to shortlist our potential choices. Besides that, we can also ask our team members for voting. If all the solutions cannot meet the needs, we may go back to the start of the cycle to check the problem statement and go the journey again.\nThe last stage should be validating the final solution. The beginning of this stage is deciding how we will make the solution happen. Testing will be made serval times and confirming that the solution has been carried out as planned. Ask our stakeholders’ feedbacks and evaluate the results. Then make the next plan to improve and integrate.\n3.  How to be a creative problem solver?After learning how to apply different models and tools to solve problems, maybe we can solve new problems we are not familiar, but can we always get a creative and effective solution? The answer is no. But we can always get useful solutions by developing our personality traits. \nAfter the journey to understand what creativity is, I was totally attracted by these brilliant artists and innovators, leading me on a trip to discover what makes us creative, where ideas come from, and how they come to life.\nCreativity is the ability to see a world in a grain of sand and a heaven in a wild flower, hold infinity in the palm of your hand and eternity in an hour. (William Blake) The ability to perceive and produce novel ideas or products that are useful to society. Creativity is the art of problem solving.\n\nFigure 3 shows the 4 stages of creativity and problem solving.\nThe process starts with preparation, which is all about gathering information. In this stage, we gather related information and materials by conducting users research and empathize with them in order to get the real pain points and define the problem. At this stage, we can use a variety of ideation methods to understand and build problems and ideas from various angles.\nStage 2 is incubation, where let mind wander. After collecting information in stage 1, take a step away from the ideas before working on them. Open your mind to all ideas unconsciously, and new connections come slowly.\nNext comes the illumination, where shows Eureka. After a period of incubation, insights arise from the deeper layers of the mind, and the ideas show up suddenly. It’s the sudden Eureka moment! \nThe fourth stage is about verification and production. Following the Eureka moment, ideas enter your mind and it’s time to evaluate, test, develop, and verify them. At this stage, you will prototype and test your idea to see if it meets the user needs defined in the preparation stage, get feedbacks and learning, then polish it up.\nFrom these 4 stages, the creativity process oscillates between convergent and divergent thinking. Incubation and illumination are about inspiration, whereas preparation and validation are about generation. To go forward, we should seek the balance between dreaming and doing. Personal traits will show how we will perform the choice. For example, doers will shine in the preparation and verification stages, and dreamers will be the stars in incubation and illumination stages. If you are a doer, sometimes you may find you can really get things done in a brilliant way but lack the new ideas since focusing on the productivity. For a long-term growth, you can read, travel, meet different people, and open your mind to different views. Clean out the space for dream and imagination. For dreamers, who may be stuck too much in their marvelous ideas, try to practice more and apply the skills into real world. \nQuote to end, ‘Know thyself’. We are on a lifelong journey to learn and grow.\n","tags":["Notes"]},{"title":"How to Analyze DeFi Projects","url":"/2023/09/12/How-to-Analyze-DeFi-Projects/","content":"How to Analyze DeFi ProjectsIntroduction:\n\nThis article focuses on explaining the thought process and not on commenting on or analyzing any specific project.\nBefore diving into a deep analysis, understanding the fundamentals of a DeFi project is extremely important.\nTracking popular metrics of DeFi protocols can help investors assess the health of the project and evaluate whether it is undervalued or overvalued.\nBy understanding token economics, investors can make wiser decisions about which tokens to buy, sell, and hold.\nThe technology underpinning DeFi projects is a key factor in determining its feasibility and effectiveness.\nThe team’s pursuit of security is of paramount importance.\nInvestors need to be able to identify projects that align with the use cases they believe in and evaluate the team behind them.\nThe strength of a project’s community and the extent to which it is adopted by users are also important indicators of success.\n\nIntroduction to DeFiThe explosive growth of Decentralized Finance (DeFi) has fundamentally transformed the financial field, offering a variety of innovative products and services. As a result, individuals can now access unprecedented financial services and products without having to go through intermediaries like banks or traditional financial institutions.\nDeFi has made finance more accessible, allowing anyone with an internet connection to participate in activities like lending, trading, and earning interest on their digital assets. However, as more and more projects enter the market, understanding how to effectively analyze DeFi projects has become crucial. This means that investors should conduct thorough due diligence on a project before acquiring its tokens.\nUnderstanding the Basics of DeFiBefore diving into a deep analysis, understanding the fundamentals of a DeFi project is essential. DeFi refers to financial applications built on blockchain networks, aiming to offer open, permissionless, and trustless alternatives to traditional financial services. These projects usually provide services like lending, decentralized exchanges, and liquidity mining.\nConsidering the open and permissionless nature of this technology, various individuals and teams are building applications that cover many use cases, such as giving users unprecedented control over their personal data and complete ownership of their digital assets. For those new to the DeFi field, they should approach these projects with caution and prudence.\nWhy is Due Diligence (Risk Management) in DeFi Important?Due to the unique nature and risks of the rapidly evolving field of DeFi, conducting due diligence before investing or purchasing tokens is extremely important. DeFi investments often come with risks such as potential scams, hacking attacks, and the loss of part or all of the investor’s principal. By thoroughly evaluating a project’s fundamentals, team, technology, and security measures, one can avoid potential scams or bad projects. By conducting due diligence, investors can make decisions, reduce risks, and identify investment opportunities that align with their investment goals and risk tolerance. Moreover, it will help build the necessary knowledge and understanding foundation to invest in the complex and rapidly growing field of DeFi.\nThis requires individuals involved in the DeFi field to consider the following factors when engaging with new projects. \nFundamentals of the ProjectUnderstanding the core aspects of a project, such as its purpose, utility, value proposition, and roadmap, is key. Assessing the market size and growth potential of the project is also essential. Reading the whitepaper can provide insights into the project’s objectives, use cases, and technology. The track record of team members reveals their ability to build and scale the product. Finally, the roadmap can be used to judge whether the project is proceeding as planned. It can serve as a supplement to other research, determining the likelihood of the project reaching its milestone goals.\nPopular metrics of DeFi ProtocolsThere are several popular indicators that can provide valuable references to understand the performance and potential of a DeFi project. These include the token’s Total Value Locked (TVL), 24-hour trading volume, inflation rate, price, and market cap.\nTotal Value Locked (TVL)Total Value Locked (TVL) is the total amount of funds locked in a DeFi protocol. You can view TVL as the total amount deposited by liquidity providers into a protocol. For example, as of July 2023, Uniswap’s TVL exceeds 3.8 billion USD, meaning users have deposited more than $3.8 billion into Uniswap. You can review TVL to assess the overall interest in a specific DeFi protocol or use it to compare which DeFi protocols have “locked” more value. Attracting substantial TVL is often a sign of a strong project.\nMarket CapThe market cap reflects the total value of circulating tokens in a DeFi protocol. It is calculated by multiplying the current price of the token by the number of tokens in circulation. The market cap provides an overall assessment of the protocol’s size and prominence. However, it must be considered in conjunction with other factors such as the utility of the protocol and competition.\n24-hour Trading VolumeThe 24-hour trading volume measures the total value of transactions executed in a specific DeFi protocol over a 24-hour span. A high 24-hour trading volume indicates that the project is more active and has better liquidity, reflecting an active ecosystem and increased market interest. Conversely, a low 24-hour trading volume shows less buying and selling of the protocol’s tokens, and lower liquidity. Low 24-hour trading volume may signify a lack of interest or participation from traders and investors.\nInflation RateAnother important indicator is the inflation rate, referring to the rate at which new tokens are generated within the DeFi protocol. A lower inflation rate might be favorable, as it indicates a more controllable token supply, possibly increasing in value over time, such as ETH transitioning to a deflationary state after moving to POS, and BTC halving, where the new BTC output will be halved. However, assessing the overall token economy and allocation model to ensure a balance between incentivizing participants and maintaining token scarcity is crucial. This topic will be further explored in this article.\nMarket Cap to Total Value Locked Ratio (TVL Ratio)The Market Cap to TVL ratio provides investors with information about the relationship between a DeFi project’s valuation and demand. For example, if a protocol’s token market cap is low but its TVL is high, it may mean that the token is undervalued. If a token’s TVL ratio is below 1, it may be more attractive, suitable for potential investors.\nNumber of AddressesThe number of wallet addresses is an indicator that measures the number of DeFi protocol participants. Similar to the user numbers of traditional Web2 applications, it is used to gauge user engagement. An increase in address numbers may be a good sign of growing popularity and adoption. It should be noted that this metric might be manipulated, as in the case of airdrops where an investor might have multiple addresses. Investors should use this metric in conjunction with other indicators.\nTokenomicsTokenomics is a term that originates from the fusion of “token” and “economics.” It is a key concept in the cryptocurrency domain, referring to the economic principles and incentive systems for managing the creation, distribution, and value of tokens within a specific blockchain system.\nHere are some aspects that tokenomics considers:\n\nToken Supply and Issuance: This involves the total number of existing or to-be-issued tokens. Some tokens have a fixed supply, while others may have a mechanism to mint new tokens. Issuance mechanisms can significantly influence the price and overall economy of the tokens. Fixed supply may lead to scarcity and possibly increase the price but might limit the system’s flexibility. Mintable tokens offer greater flexibility but could lead to inflation if mismanaged. For example, MATIC originally had a fixed supply cap of 10 billion, while the POL white paper allows for a specific rate of annual increase beyond the initial supply of 10 billion (all converted from MATIC).\n\nToken Distribution: How tokens are distributed among users can greatly impact the health and sustainability of the token economy. Fair and reasonable distribution helps maintain a healthy and active user base. Additionally, tokens may be distributed according to a vesting schedule, meaning they can be used within specific periods. This can encourage long-term holding and contribute to the token’s stability.\n\nToken Utility: Tokens usually serve specific functions within their ecosystem. They can provide services, voting rights, or other benefits to users. The utility of a token is a key factor in determining its value. Tokens that provide services, voting rights, or other benefits are more likely to retain value than those without clear use cases.\n\nMining and Staking Incentives: Many blockchain projects reward users for contributing to network security and stability with newly minted tokens, a practice known as mining or staking. Participation in incentive measures like staking rewards can encourage user engagement. However, this must be balanced to avoid vicious inflation or other unintended consequences.\n\nToken Burning: Some projects may choose to periodically “burn” tokens, permanently removing them from circulation to reduce supply and potentially increase the value of the remaining tokens.\n\nGovernance: Many tokens also serve as governance tokens, granting holders voting rights over the project’s future decisions.\n\nEconomic Policies: Tokenomics involves implementing economic policies at the protocol level, influencing how tokens are issued, distributed, and managed.\n\nSecurity: The security of tokens and their underlying blockchain technology is crucial. This includes the security of smart contracts managing the tokens and the overall security of the blockchain.\n\n\nTokenomics is vital as it shapes the dynamics of digital assets and projects. By understanding tokenomics, investors can make more informed decisions about buying and holding particular tokens. It can also help project developers design more sustainable and successful token economies. Equally important, remember that the above-mentioned aspects are just things to consider; the “best” token economy may vary depending on the specific goals of the project. For instance, tokens aimed at incentivizing participation in decentralized applications may have different requirements compared to tokens intended to act as stablecoins.\nAssessing DeFi TechnologyThe technology that underpins DeFi (Decentralized Finance) projects is a critical factor in determining their feasibility and effectiveness. One of the key aspects to consider is the blockchain platform on which the project is built. For example, Ethereum is a popular choice, providing a robust and mature infrastructure for DeFi applications. Meanwhile, with the development of Layer 2 solutions, more and more projects are seeking collaboration with OP and ARB. Other Layer 1 blockchain have also emerged, offering promising options for scalable DeFi applications. Investors seeking to acquire DeFi tokens should weigh the pros and cons of the underlying blockchain to make an informed decision.\nAnother crucial aspect is the assessment of the smart contracts that drive the DeFi projects. Smart contracts are self-executing contracts on the blockchain, with the terms of the agreement directly written into code. Although audits may not necessarily guarantee security, it is vital for DeFi projects to regularly audit their smart contracts to reduce potential vulnerabilities and protect user funds. Reviewing the smart contract audit history of a project can give insights into the team’s efforts to protect users.\nInteroperability is another key aspect of DeFi tokens, ensuring seamless communication and interaction between different platforms and protocols. It allows tokens to be easily transferred, traded, and used across multiple decentralized applications (DApps) and blockchain networks. Investors should check the interoperability of DeFi tokens before deciding to invest.\nSecurityGiven the frequent occurrence of security vulnerabilities in the DeFi (Decentralized Finance) sector, assessing a project’s security measures is crucial. Regular audits by reputable third-party companies, which indicate that the project is actively identifying potential vulnerabilities to ensure the security of smart contracts, are a positive sign. Considering the history of DeFi projects being targeted by hackers and the subsequent panic that ensues, robust security measures are a necessity.\nIn addition to audits, incentives similar to bounty hunters exist, such as bug bounty programs, which serve as another indicator of the project’s serious commitment to security. In a bug bounty program, the organization sets specific standards, guidelines, and rewards for individuals who discover and responsibly disclose valid security vulnerabilities. When bug finders identify vulnerabilities, they can submit a detailed report to the organization, outlining the issue, its impact, and the steps to reproduce the flaw. The organization then verifies the reported vulnerability, and if valid, offers a bounty or predetermined monetary reward to the bug-finder. This proactive approach to security encourages a community of security researchers to help improve the overall safety and resilience of the DeFi project.\nTeamThe following are common points to consider in team assessment for stock investments, which may be enlightening for evaluating teams within DeFi (Decentralized Finance) projects:\n\nAssessing the Skills, Experience, and Qualifications of Team Members: Look for signs of success in similar enterprises or industries. The collective ability of the team should align with the company’s mission and goals.\nResearching the Team’s Compensation Structure: This includes salary, bonuses, and stock options (or token allocation in the context of DeFi). Stock options can inflate compensation figures and affect shareholder value. High compensation that doesn’t align with the company’s performance might be a warning signal.\nObserving Insider Buying and Stock (or Token) Repurchases: If management is purchasing company stock (or tokens), it might indicate their confidence in the company’s future. Conversely, if management is selling stock (or tokens), it may signal potential problems.\nPaying Attention to How Management Communicates with Shareholders and the Public: Transparency, regular updates, and a clear vision for the company’s future are all signs of strong leadership. Questions can be asked through social media, community channels, and AMAs (Ask Me Anything sessions).\nCompany Policies and Mission Statements: These can provide deep insights into management’s values and strategic direction. Look for evidence of ethical practices and a focus on long-term profitability.\nBeing Alert to Potential Red Flags: Such as past fraudulent activities or ongoing legal issues.\n\nIn the context of DeFi projects, these traditional considerations take on new dimensions, given the decentralized and often pseudonymous nature of blockchain projects. Still, careful analysis of the team’s background, alignment with project goals, compensation structure, and communication strategy can provide valuable insights into the project’s credibility, alignment with stakeholders’ interests, and long-term viability.\nCommunity and Social MediaThe strength of a project’s community and user adoption is a key indicator of its potential success. These metrics provide valuable insights into the project’s viability, future growth prospects, and overall market acceptance.\n\n**Emotions and Engagement: ** Emotion is an essential factor in trading and also a reflection of the strength of a project’s community. Engaging with the project’s community is vital to deeply understanding their sentiment, active participation, and overall enthusiasm. This interaction allows you to assess the levels of support, engagement, and trust within the community.\n**Social Media Platforms: ** Platforms like Twitter, Telegram, Discord, and even WeChat provide abundant information about the project community. Investors should monitor discussions, comments, and activity levels on these platforms, looking for signs of positive sentiment, active discussions, and growing follower numbers.\nPositive Sentiment: Positive sentiment indicates community trust and enthusiasm for the project. It’s often reflected in the tone and content of discussions within social media channels.\nCommunity Activities: Participation in community forums, AMAs (Ask Me Anything sessions), and active community channels often indicates a thriving community that is engaged and supportive of the project.\nWarning Signs: On the flip side, investors should be cautious about manipulated sentiment, paid influencers, or unnatural spikes in activity that could be signs of artificial hype or misleading marketing strategies.\nImportance of Community in DeFi: In the world of decentralized finance (DeFi), community plays an even more significant role. Decisions related to the development and direction of a project often involve community voting and active participation from the stakeholders. A strong community can foster innovation, support the project through challenges, and contribute to the decentralized ethos that underpins these projects.\n\nEvaluating the community and social media presence of a DeFi project is a multifaceted task. It requires a nuanced understanding of various platforms, the ability to discern genuine sentiment from manipulation, and an appreciation for the role of the community in driving project success. Combined with other assessment factors like technology, security, and team evaluation, community analysis offers a holistic view of a project’s potential and can guide wise investment decisions.\nVC investmentVenture Capital (VC) investment in a crypto project can have substantial impacts, both positive and negative. Here’s an analysis of how VC investment may affect a crypto or decentralized finance (DeFi) project:\nPositive Impacts\nIncreased Capital: VC funding provides the necessary capital to scale the project, develop technology, hire talent, and execute the roadmap. It can accelerate growth and enable the project to reach milestones more quickly.\n\nNetwork and Expertise: VCs often come with vast industry networks and strategic insights. These connections can help in forming partnerships, gaining market insights, and providing mentorship to the team.\n\nTrust: Backing from renowned VCs can bring trust to the project. It signals trust and often attracts further institutional and retail interest.\n\nMarketing and Community Building: VC investment can facilitate more substantial marketing efforts and community engagement, which can result in better visibility and adoption.\n\n\nNegative Impacts\nControl and Decision-making: Depending on the terms, VC investment may come with conditions that affect control and decision-making within the project. VCs might seek seats on the board or have a say in major decisions, potentially leading to conflicts with the original vision of the founders.\n\nToken Distribution and Economics: If not managed carefully, the token distribution to VCs might affect the token economics, potentially leading to an oversupply or concentration of tokens with a few entities. This can impact price stability and community dynamics.\n\nExit Pressure: VC funds usually seek returns on their investment within a defined period. This can lead to pressures on the project to deliver financial results, sometimes at the expense of long-term innovation or community interests.\n\nPotential Misalignment with Decentralization Ethos: In some cases, VC involvement might be seen as contrary to the decentralized ethos of crypto projects, especially if there’s significant control exerted by the VCs. This could lead to community backlash or a shift away from the project’s original decentralized mission.\n\n\nVC investment in a crypto project is a multifaceted aspect that carries both opportunities and challenges. While it provides much-needed capital and potential strategic advantages, it also introduces complexities related to control, token economics, and alignment with the project’s vision and values.\nProjects considering VC investment must carefully assess and negotiate the terms to ensure alignment with their long-term goals, community interests, and decentralized principles if they pertain to the specific project. Transparency with the community and stakeholders about the nature and terms of the investment can also be vital in maintaining trust and alignment.\nConclusionAnalyzing DeFi projects is a critical process in making informed investment decisions in the rapidly expanding field of decentralized finance. While the factors outlined above are essential for making wise choices, what’s even more vital is that investments should never exceed what one can afford to lose. Investors should proceed with caution, not only fully understanding the technical foundation, security measures, team competence, community engagement, and social media impact of the project but also scrutinizing potential risks and market volatility. VC investment may bring new opportunities and challenges, and this too requires careful assessment. Ultimately, investing in DeFi or any crypto projects should be based on comprehensive research and understanding, coupled with clear investment strategies and risk management plans. While pursuing returns, maintaining caution and responsibility is of paramount importance.\n","tags":["DeFi","Investment"]},{"title":"Prompt Engineering Process","url":"/2025/05/12/Prompt-Engineering-Process/","content":"Prompt Engineering: A Comprehensive Guide to Design, Implementation and OptimizationIntroductionPrompt Engineering is defined as the process of structuring or crafting inputs (prompts) to guide generative Artificial Intelligence (AI) models, particularly Large Language Models (LLMs), towards producing the best possible outputs. It’s about carefully designing text inputs that the model can interpret and understand effectively to achieve specific goals. \nThis process is distinct from traditional software development as it focuses on interacting with pre-trained models through precise instructions. The effectiveness and relevance of an AI system’s response are directly impacted by the design of the prompt.\nBased on established practices, the process for designing, developing, and refining prompts can be understood as an iterative cycle involving initial design, practical development, and continuous refinement based on feedback and testing. It’s rare to achieve the perfect output on the first attempt, so continuous tweaking and testing are essential.\nThe Complete Prompt Engineering Workflow1. Initial Design and Crafting1.1 Define Clear Goals and Objectives\nStart with a clear understanding of what you want the AI to produce\nDefine your success criteria with specific metrics\nBe specific about the desired outcome\n\n1.2 Draft the Initial Prompt\nCraft a clear and focused prompt that sets specific expectations\nUse precise language and avoid vague instructions\nDefine what you want the model to deliver\nFor specialized tasks (e.g., coding), specify exactly what you need (e.g., “Write a Python function to calculate Fibonacci sequence”)\n\n1.3 Structure the InputOrganize the input into logical sections. A recommended starting structure includes:\n\nRole and Objective\nInstructions\nSub-categories for more detailed instructions\nReasoning Steps\nOutput Format\nExamples\nContext\nFinal Instructions with prompt to think step by step\n\nYou can add or remove sections as needed. Organizing instructions into high-level sections like “Response Rules” or “Instructions” with bullet points is also recommended. You can access more info from this blog\n1.4 Incorporate Key ElementsClarity and Specificity\nEnsure the prompt is clear and specific\nVague inputs lead to inconsistent or irrelevant results\nSpecify constraints like word count or format\nClarify terms that might be open to interpretation\n\nContext\nProvide necessary background information, specific data, or external documents\nFor long contexts, consider placing instructions at both beginning and end\nConsider the mix of internal and external knowledge required\nFor coding tasks, include relevant files or codebase structure for debugging\n\nInstructions\nInclude clear, detailed instructions\nFor complex workflows, add an ordered list\nInstruct the model to follow steps sequentially\nInclude explicit specifications around what to do or not to do\n\nRole-Playing\nConsider giving the model a specific role to tailor responses\nThis can be done using “system prompts”\nFor technical tasks, specify expertise level (e.g., “Act as a senior Python developer”)\n\nExamples (Few-shot/Multishot)\nInclude examples in the prompt to demonstrate the desired tone, format, or context\nEnsure important behavior shown in examples is also cited in your rules\n\nReasoning Steps (Chain-of-Thought)\nFor tasks requiring complex reasoning, structure the prompt to encourage step-by-step problem solving\nInclude phrases like “Let’s think step by step”\nCreate dedicated sections for reasoning strategy\nParticularly useful for debugging, math problems, or logical reasoning\n\nOutput Format\nClearly specify the desired output format\nFor coding, specify whether you want just code or code with explanations\n\nDelimiters/Tags\nUse delimiters or tags (like XML tags for Claude) to organize the prompt\nDistinguish different sections clearly\n\nTask-Specific Structures\nStructure will vary depending on the task:\nText-to-text: Focus on context, examples, and desired format\nText-to-image: Specify medium, style, lighting, color, texture, and use negative prompts\nTool manipulation: Clearly name and describe tools and their parameters\nCode generation: Specify language, style, patterns, and testing requirements\n\n\n\n2. Development and Implementation2.1 Create the Prompt\nWrite the prompt based on the design principles\nTranslate the desired outcome and structure into specific text input\n\n2.2 Utilize Tools\nUse available tools like prompt generators or platforms\nConsider platforms like Orq.ai for organization and version control\nLeverage collaborative editing features where available\n\n2.3 Initial Testing\nRun the initial prompt through the AI model\nGet a first output to evaluate\nSave this output for later comparison\n\n3. Refinement and Iteration3.1 Review and Evaluate Output\nCheck the AI’s response against your success criteria\nReview for accuracy, relevance, format, and completeness\nFor specialized tasks (e.g., code generation), evaluate additional criteria:\nAccuracy: Does the output work as intended?\nQuality: Does it adhere to best practices or standards?\nSecurity: Does it introduce vulnerabilities?\nBias: Does it reflect biases from training data?\nReasoning: If steps were requested, is the logic sound?\n\n\n\n3.2 Identify Issues and Failure ModesCommon problems include:\n\nIrrelevant outputs from vague requests\nInconsistent results\nOutputs that are too lengthy or lacking detail\nErrors in reasoning or format\n\nTask-specific issues (e.g., for AI coding):\n\nOverreliance: Output produced without the user fully understanding it\nBlack Box Suggestions: Outputs that are difficult to interpret\nVagueness Complaints: The model claims the request is too vague\nContext Deficiency: The model needs more information\nConflicting Instructions: Contradictory or ambiguous directions\nRepetitive Output: The model repeats sample phrases verbatim\nExtraneous Content: Unnecessary text or formatting\nInsufficient Testing: The model doesn’t validate edge cases\nTool Use Errors: Problems with function calls or tool usage\n\n3.3 Adjust the PromptImprove Instructions\nCheck for conflicting, underspecified, or wrong instructions\nAdd explicit specifications for desired behavior\nFor complex tasks, provide an ordered list of steps\nBe clear about what to do and what not to do\n\nRefine Structure/Elements\nAdjust constraints\nClarify terms\nAdd more examples\nSpecify level of detail\nRefine the overall structure\nFor specialized outputs, clarify format requirements\n\nAddress Failure Modes\nExplicitly instruct the model to mitigate common failures\nIf the model hallucinates information: instruct it to ask for information when uncertain\nIf sample phrases cause repetition: instruct it to vary expressions\nIf there’s too much extraneous text: specify a professional, concise tone\nControl formatting with explicit instructions\n\nRefine Reasoning\nAddress systematic planning and reasoning errors\nAdd more explicit instructions for Chain-of-Thought\nFor complex logic tasks, enhance step-by-step guidance\nEncourage breaking down problems into logical components\nRequest reflection on outcomes\nExplicitly instruct rigorous testing for edge cases\n\nImprove Context and Constraints\nProvide necessary context more clearly\nUse delimiters or tags to organize different sections\nDefine constraints explicitly (e.g., “do NOT guess or make up an answer” if tool use fails)\n\n3.4 Test and Repeat\nTest the refined prompt\nCompare new output against earlier iterations\nIdentify improvements\nDocument changes and remaining issues\nContinue iterating until satisfactory results are achieved\n\n3.5 Gather Feedback\nCollect input, especially from users\nUse structured surveys, open-ended inquiries, focus groups\nImplement feedback collection mechanisms like in-app ratings\nProvide transparent updates on how feedback influenced changes\n\n3.6 Structured Experimentation\nUse structured experimentation rather than guesswork\nConduct A/B testing to evaluate variations\nIntegrate analytics tools for data-driven guidance\nDocument iteration results methodically\n\n3.7 Document Changes\nKeep a record of each prompt version and its output\nUse version control tools where available\nNote both successful and unsuccessful approaches\n\n3.8 Ongoing Assessment\nSchedule regular evaluations after major adjustments\nReassess prompt effectiveness periodically\nUpdate prompts as the model, use cases, or requirements evolve\n\n4. Advanced Techniques and Scaling4.1 Leverage Advanced Techniques\nExplore advanced methods:\nFurther refine Chain-of-Thought approaches\nImplement Few-Shot Learning with diverse examples\nUse self-refine prompting (with carefully designed feedback loops)\nApply retrieval-augmented generation for knowledge-intensive tasks\n\n\n\n4.2 Automated Optimization\nFor scaling applications, consider automated approaches:\nFeedback-driven self-evolving prompts\nReinforcement learning to adjust prompts dynamically\nAutomated prompt optimization tools\nA/B testing frameworks for prompt variations\n\n\n\nConclusionPrompt engineering is not a static, one-time interaction but a learning process where testing and refining prompts is essential to achieve optimal performance. The iterative nature of this process is key to success, and each “bad case” should be viewed as an opportunity to learn and improve.\nThis methodical approach helps:\n\nAlign results with specific goals\nIdentify and fix problems early\nImprove control over complex tasks\nEnsure consistency across similar tasks\nBuild more reliable and effective AI interactions\n\nThe most successful prompt engineers combine technical knowledge with experimentation, persistence, and a willingness to iterate until achieving the desired results. By following the structured process outlined in this guide, you can develop prompts that consistently produce high-quality outputs aligned with your specific needs.\n","tags":["Prompt Engineering","LLM"]},{"title":"RAG与LoRA","url":"/2025/05/05/RAG%E4%B8%8ELoRA/","content":"RAG 与 LoRA 技术详解本文档总结了 RAG (Retrieval-Augmented Generation) 和 LoRA (Low-Rank Adaptation) 两种在大型语言模型 (LLM) 领域常用的技术。\nRAG (Retrieval-Augmented Generation) - 检索增强生成核心思想RAG 结合了信息检索和自然语言生成，在 LLM 生成响应前，先从外部知识库检索相关信息，并将这些信息作为上下文提供给 LLM，以生成更准确、更可靠的回答。\n解决的问题缓解 LLM 面临的挑战：\n\n知识静态性: 无法获取最新信息。\n幻觉: 编造错误信息。\n领域知识缺乏: 专业知识不足。\n无法访问私有/实时数据。\n缺乏可解释性: 难以溯源信息来源。\n更新成本高: 重新训练 LLM 成本高昂。\n\n工作流程\n 输入 (Input): 用户提问或提示。\n检索 (Retrieval):\n  查询编码 (Query Encoding): 将输入转为向量。\n  相似性搜索 (Similarity Search): 在向量数据库中搜索相似的文本块。\n  获取相关文本 (Retrieve Relevant Chunks): 得到 Top-K 相关文本块。\n\n\n增强 (Augmentation):\n  构建上下文提示 (Contextual Prompt Construction): 将检索到的文本与原始查询组合成新提示。\n\n\n生成 (Generation):\n  LLM 推理 (LLM Inference): 将增强提示输入 LLM。\n  生成响应 (Generate Response): LLM 基于内部知识和外部上下文生成回答。\n\n\n\n关键组成部分\n  检索器 (Retriever): 编码器、知识库（向量数据库）、索引。\n  生成器 (Generator): 大型语言模型 (LLM)。\n  知识库构建流程 (Indexing Pipeline): 数据加载、文本切分、向量化、索引存储。\n\n优势\n  知识实时性: 可通过更新知识库获取最新信息。\n  减少幻觉: 提供事实依据。\n  提高准确性与相关性: 基于相关上下文生成。\n  领域适应性: 易于适应特定领域。\n  可解释性与可溯源性: 可追踪信息来源。\n  成本效益: 更新知识库比重训 LLM 便宜。\n  数据隐私: 私有数据可存于外部库。\n\n挑战\n  检索质量: 检索效果是瓶颈。\n  文本块切分策略: 影响上下文和精度。\n  检索与生成的平衡。\n  处理冲突信息。\n  系统复杂性。\n  延迟: 检索增加响应时间。\n  成本: 向量数据库和检索仍有成本。\n\n\nLoRA (Low-Rank Adaptation) - 低秩适应背景与动机解决大型模型完全微调 (Full Fine-tuning) 带来的问题：\n\n计算资源消耗大 (显存、算力)。\n存储成本高 (每个任务一个完整副本)。\n训练时间长。\n\nLoRA 是一种参数高效微调 (Parameter-Efficient Fine-Tuning, PEFT) 技术。\n核心思想假设模型权重在适应新任务时的变化量 ($\\Delta W$) 是低秩 (Low-Rank) 的。因此，可以通过学习两个小的低秩矩阵 $A$ 和 $B$ (其中 $\\Delta W = BA$) 来表示这个变化量，而无需更新全部原始权重 $W_0$。\n工作原理\n  冻结原始模型权重 $W_0$。\n  在选定的层（如注意力 QKV 矩阵）旁边添加低秩适配器矩阵 $A$ ($r \\times k$) 和 $B$ ($d \\times r$)，其中 $r$ 是秩 ($r \\ll \\min(d, k)$)。\n  只训练 $A$ 和 $B$ 的参数。\n  前向传播时计算：$h = W_0 x + \\alpha B A x$。\n\n关键组成部分与超参数\n  适配器矩阵 A 和 B: 新增的可训练参数。\n  秩 (Rank, r): 控制适配器大小的关键超参数 (如 4, 8, 16, …)。\n  缩放因子 ($\\alpha$): 控制适配器影响程度。\n  目标模块 (Target Modules): 选择应用 LoRA 的层。\n\n优势\n  极高参数效率: 显著降低显存需求。\n  减少存储: 只需存储小型适配器权重。\n  可能更快的训练: 低显存允许更大批量。\n  无额外推理延迟: 训练后可将 $BA$ 合并回 $W_0$ ($W = W_0 + BA$)，推理时无额外计算。\n  可媲美的性能: 效果接近完全微调。\n  便捷的任务切换: 易于加载不同适配器。\n\n挑战\n  超参数选择: $r$, $\\alpha$, 目标模块需要调优。\n  并非万能: 对某些任务可能不如完全微调。\n  合并操作: 需要额外步骤来消除推理延迟。\n\n应用场景\n  LLM 下游任务微调。\n  模型风格、领域知识适应。\n  文生图模型（Stable Diffusion）的风格/人物学习。\n  高效实现对齐技术（如 SFT）。 \n\n","tags":["RAG","LoRA"]},{"title":"THE LEAN PRODUCT PROCESS","url":"/2022/09/12/THE-LEAN-PRODUCT-PROCESS/","content":"THE LEAN PRODUCT PROCESSThe Lean Product Playbook written by Dan Olsen shows a detailed guide on how to find product markets and make products people love. Unlike some books that are long in theory and short in practice, this is the first book I’ve read that really explains how to apply Lean Startup concepts in a practical, step-by-step manner. I’m so glad to have the book when I have the passion to build something inspired. \nAnd this note is more about listing some topics I really love in this book.\nCore conceptsThe journey starts with why products failed and followed by the product-market fit, which captures the essence of what it means to build a great product. \n\nProduct-market fit:\nMy definition of product-market fit…is that you have to build a product that creates significant customer value. This means that your product meets real customer needs and does so in a way that is better than the alternatives.\n\nI used to pay more attention to some ideas or concepts like advertisement, marketing, business models, and how to increase the customer lifetime value, and this definition tells me there is a distinction between creating and capturing values in the process. In order to capture values, you have to create it first.\nThe other core concept in this book is the Problem Space and Solution Space. \n\nAny product that you actually build exists in solution space, as do any product designs that you create—such as mockups, wireframes, or prototypes.\nProblem space is where all the customer needs that you’d like your product to deliver live.\n\nAnd the customer needs can be expanded to some other concepts like job-to-be-done, customer desires, or customer pain points. I attained many courses who starts with idetifying the job-to-be-done, which emphasizes the importace of finding what not be done or well to solve customers real problems. And sometimes I find that some blogs or books often talk about the differences between the “what” versus the “how.” The “what” describes the benefits that the product should give the customer—what the product will accomplish for the user. The “how” is the way in which the product delivers the “what” to the customer. The “how” is the design of the product and the specific technology used to implement the product. “What” is problem space and “how” is solution space.\nThe Lean product processThe content of product management process can be divided into these steps:\n\nDetermine your target customers. This is the bottom of our pyramid since different customers will have different needs and even those who have the same needs may have different views on their relative importance. \n\nIdentify underserved customer needs. Before we mover to design a solution, we have to validate the job-to-be-done, customer pain points or customer desires. Since customer needs can seem somewhat fuzzy when we interview them, and they are not skilled at describe the roots of the problem.\n\nDefine your value proposition. At this point, we have identified several important customer needs, now we need to decide which ones our product or service will address. \n\nSpecify your MVP feature set. Now we have a clear understanding of our value proposition, we have decided on the feature set for our MVP, identifying functionality required to validating we are heading on the right direction.\n\nCreate your MVP prototype. At this stage, we want to create a user experience or prototype that we can show to customers so that we can test our ideas.\n\nTest your MVP with customers. In this part, we conduct both qualitative and quotative user test to see how it compares to the target value. \n\nThen Iterate to improve product-market fit. I learned a loop called ‘build-measure-learn’, which means we want to use what we have learned after we receive a round of feedbacks to modify our hypotheses and MVP so that we can test them with customers again. We want to iterate quickly with the goal of improving product-market fit.\n\n\nSome tasks will be performed in these steps:\n\nUnderstanding and representing user needs.\n\nDefining a vision for a product.\n\nMonitoring the market and developing competitive analyses.\n\nAligning stakeholders around the vision for the product.\n\nPrioritizing product features and capabilities.\n\nCreating a shared brain across larger teams to empower independent decision-making.\n\n\nThere are some benefits of Lean or agile management. First, because we are planning in smaller increments, we can react to changes in the market or new information more quickly. Second, our product can reach customers earlier, which means that we can start hearing feedbacks from customers on our actual products sooner, which helps guide our subsequent product management efforts. Third, teams can reduce their margin of error in estimating scope by working in smaller batch sizes.\nTBC… Some concepts in Agile will be added, and some tools will be here in the future after I have the experience of the whole loop. Some other ideas can be the link and differences between design thinking and the lean product process. From a lifelong learning perspective, how to be a good problem-solver?\n","tags":["Notes","PM"]},{"title":"The road to Navigation","url":"/2024/12/12/The-road-to-Navigation/","content":"Global path planning algorithmDijkstraWhat is Dijkstra’s Algorithm?Dijkstra’s algorithm is fundamental in robotics, especially for path planning, where robots need to find the shortest path from one point to another.\nDijkstra’s algorithm finds the shortest path between nodes in a graph. In robotics, nodes represent positions, and edges (connections between nodes) represent possible paths with associated costs (distances, time, or effort).\nA*Local path planning algorithmDWATEB","tags":["Robotics","Path Planning","Navigation"]},{"title":"What is a Product Roadmap?","url":"/2022/01/12/What-is-a-Product-Roadmap/","content":"What is Product Roadmap?Product Roadmap is a long-term product development plan that gives all product stakeholders the infomation they need to coordinate plans.\nWhy we need a Product Roadmap?A product roadmap provides a prediction to the product development process.\nWho are your product stakeholders?\nCustomers\nCustomers-facing groups like sales, marketing, and customer support\nInvestors, and sponsors\nArchitects, designers, and engineers.\nHuman resources\nLegal team\n……\n\nWe need both Product backlog and Product roadmap.\nYou estimate the dates of the product milestones on the roadmap using a completely independent set of top-down estimates from your product development leader. In the end, these estimates are inaccurate, of course. But as long as they’re close, they’ll serve the purposes of the product roadmap, to allow coordination among the different product stakeholders. Increasingly, your backlog will be directed by the product roadmap,which is one of the main ways you ensure that the activities of the product development team are aligned with the needs and goals of the whole company.Laying the grooundworkWhy the process is important?\n A roadmap has no value if the stakeholders are not aligned in advance.\n The fail usually comes from engineers, designers, and the whole team not being exposed to the project early.What are the product development purposes?\nServe the business.\nSupport overall business strategy.\nWin customer adoption, loyalty.\nGenerate revenue.How to create alignment?\nInclude stakeholders early.\nAsk for, address, feedback.\nSend updates.What a successful Roadmap need?\nSound strategy.\nRealistic.\nFully supported.How to avoid failures?\nSpend time with stakeholders at the beginning of the process.\nAsk for underlying thoughts.\nExplain the importance of including others.\nEstimate the development time.Select your stakeholders\nBusiness leader: allocate the resources and prioritize the tasks are necessary to the product sucess.\nSales leader: They have the target to hit. And they have the ability to rally resources to make the roadmap sucessful.\nProduct development leader: They are response for organizing and motivting their team to hit the roadmap milestones.\nWhen working across groups, use the sales leader as the your business leader.\n\nCustomers researchThere are some question have to figure out:\n\nWhat decisions do your customer need to make in choosing your product?\nWhat problem are they trying to solve?\nWhat other options are available to them?\n\nAnd luckily,  there are some ways you can get to your customers:\n\nInitiate a research group.\nAsk your customer yourself like make an appointment, a phone call, a zoom meeting, etc. Reach out to your customers directly may be the best way to get insights.\nParticipate in sales or customer service meetings.\n\nStart with product strategyA product strategy describes how your company will achieve its business goals. And a product strategy should answer the following questions:\n\nWhat are your goals?\nHow will you measure your sucess?\nWho are your target customers? Who are you really want to win?\nWhat are the key needs of your target customers you want to meet?\nWhat benifit do you provide to customers?\nWho are your competitors?\nWhat options do your customers have? the alternatives?\nWhat is the key different between you and your competitors?\n\nDecision-making and building alignmentIdentify key milestones\n\nSome milestones may address the pain points of existing customers, increasing their usage and reduce the churn. And other milestones may allow your customers use your product in new ways. Some may even do not benifit your customers directly, like open new distribution channel, reducing costs.\n\nRecord the strategy objectives it supports.\nWhen you finished brainstorms your milestones, give it a priority and meet with your business leader. An one-to-one meeting may increase the chance they share true feedbacks.\n\nEstimate leves of effort\nFirst, estimate the development capacity of your team. It usually in the form of developer days, weeks, and months.\nThe next is estimate the development time for each milestones. One thing we have to know is not all time is spent on developping. It may cost plenty of time to fix bugs, maintain products, and engineering.\n\nBuilding the strawman —— the first version\nThe first step is to roughly sequence your milestones in terms of their value in supporting your product strategy.\nThe next step is to schedule these milestones into your roadmap. You can start with the most basic approach to scheduling. Look at your first, highest priority milestone. Use the effort estimate provided by your product development leader,as well as the development capacity of the team, to figure out when on the calendar you could expect it to be delivered.\nNext, you pick up the second highest priority milestone and assuming that the first one is completed on time, schedule that one in the same manner.\nOnce done, it may look like this.\n\n​        \n\nIf these milestones are for different products,  you may separate them like this.\n\nHere comes the product roadmap meeting\nExplain the goal\nQuickly review your product strategy \nReview the development capacity of your team\nWalk through your product roadmap strawman\nAsk the team waht they wish was different\nModify the roadmap directly, so everyone can see the consequences.\n\nThis process reminds me of the RACI chart, and get all the group members in a warm meeting.\n","tags":["Notes","PM","Lifelong_Learning"]},{"title":"Why I love Coursera","url":"/2022/08/03/Why-I-love-Coursera/","content":"What is my favourite application?Today, I realized I spent more than 10 hours on Coursera every week for the past year and I really enjoyed this platform. Coursera is a platform that you can build skills with online courses from top institutions and universities. I like it since I can learn a lot from top universities whenever and wherever I am, no time, places limits at a low cost or even free. I learnt a lot  courses on it and I really want to introduce it to you. \nHow did I know Coursera?I still remember the first time I met Coursera when I was learning Deep Learning taught by Andrew Ng, who was a famous professor in Stanford University. At the end of his class in deeplearning.ai, he would introduce us Coursera created by Daphne Koller and himself. MOOC (Massive Open Online Courses) was bringing educational and vocational courses online and Coursera totally won my fragemental time on PC. Similarly, Weixin Reading introduced online reading to the public in China, and became the most visited app on my phone when I was free. \nWhat is the customer journey map?The general process of an individual user can be divided into the following steps.\n\nLog in as individual user or enterprise users\nFind the courses you want by searching jobs, skills, or universities\nRead the introduction of the course and enroll\nTake the course with your customize schedule\nTake all discussions, tests, peer value, and quiz\nEarn a certificate provided by Coursera and institutions\nShare your learning and certificates to LinkedIn or other platforms\n\nBusiness model of CourseraWho are the customers? (Customer Segments)Coursera provides services to two main kinds customer segments. The first one is educational institutions like universitties and companies, which wants to share course resources with a fully access. The other one can be the end-users who want to learn some skills in specific fields but may find it difficult to get access to educational resources. \nThe following types of people can be their potential end-users:\n\nHigh school students who want to take college courses in advance.\nCollege students who want to take online courses rather than offline courses.\nCollege students who want to learn extracurricular skills.\nFresh graduate students who want to decide or change career path.\nPeople who already have a job want to strengthen their existing knowledge.\nPeople who already have jobs and want to learn new skills.\n\nTry to imagine this scenario: more and more people realize the importance of life-long learning. As a fresh graduate, Allen is pursuing  a full-time job related to product management, business development. For a better fit, he chooses courses related to digital product managemt, Data analysis, and project managemtn on Coursera and spends 10 hours per week on it. \nWhat value does Coursera deliver to customers? (Value Propositions)When it comes to what customer needs Coursera want to satisfy, it allows end-users like me easy and complete access to top educational online resources that  many constomers may find unaffordable or inaccessable through its website and application.\nSharing high-level courses also means that universities and insititutions can reach a greater number of consumers more easily, futhering their individual brand identities. \nHow does Coursera make money? (Revenue Streams)Courserea starts its journey with the goal of providing high-level education resources for free to anyone looking for it. Now, Coursera charges from several channels to improve its services. \nFree accessThere are many ways to get access to some courses for free since some courses are provided to everyone for free. Besides that,  people can choose audition when choose whether to pay for the course so that  they still have access to most course materials for this course without a certificate.\nCertificatesMost courses are offered with no charge. However, people who want to earn a certificate of completion which is a trusted and shareable way to showcase their new skills can choose to pay the course fee. This feature was landed in 2013, giving paying students access to graded assignments, homework, and examinations. If completed satisfactorily, they can earn a verified certificate at the end of the course, and they can visit the course in the future with no limits. The course fee varies between from $39 to $89 depending on the course. Nowadays, people can choose to be a membership with no limits to a sea of courses at a cost of $39 per month. \nIf you have planty time during some period so that you can finish the course within the7-days trail, once finished, you can also get the certificate and have no limits when revisit the course in the future.\nSpecializationsThis feature is a bundle of different but related courses designed to help customers  deepen their expertise in a specific area. For example, DeFi: The future Finance Specialization consisted of 4 courses aims to provide learners a much deeper understanding of both the infrastructure of DeFi and leading applications. Customers can pay monthly subscription fees between $39 to $89, depending on the specialization, for access to all resources and specialization certificates.\nCoursera for BusinessCoursera for Business enables businesses to offer their employees or teams access to a sea of courses to boost their expertise. Compared to traditional certificates, Coursera provides this service with a lower fee of $400 per employee per year. Enterprise packages comes with higher cost than personal or teams’ plan. Coursera for Campus provides silimar option to the government and universities.\nSince higher price and growing customers in this segment, it has quickly become a key channel of revenue.\nUniversity DegreesCollaborated with top universities world wide, Coursera offers students a chance to complete a fully recognized Bachelor’s or Master’s Degree, including MBA from UIUC. The price is far lower compared to degrees in campus, but with the same instructors who teach in person. Scholarships are provided. It’s reported that Coursera generated $12.0M in revenue in Q1, 2021 with 13,493 students enrolled. \nCompetitve analysisSWOT AnalysisStrength\nStrong relationships with its key partners to offer colourful and high-level courses.\nFirst-move advantages: kind of becoming market standard. After Coursera, many universities or educational angency like MOOC China, and MIT opensource, built their own platform which follows these standards. What’s more, the earlier Coursera ran into the market, the bigger market share it would take. Some MOOC platforms like Xue Tang Zai Xian created by Chinese government and universities has limited market usually in domestic rather than a golbal market. \nGood environment to spread courses. Multi-lanuage courses are provided to learners from all over the world. At the same time, Coursera advocates learners to take part in translation event so that subttitle will be translate into their mother language and would be offered after reviews.\nDiverse revenue channels. Multiple revenue model enables Couresera great resources to not only keep competitve to the threat of newcomers but also invest into R&amp;D.\n\nWeakness\nServices Coursera provided are more focus on solution space, which means more about what can they do. Sometimes they ignore the interests of external stakeholders.\nPoor visual design. Terrible design on both Andriod and iPhone.\n\nOpportunities\nCovid-19 has changed many people’s lifestyle. Some universities promoted hybrid teaching model, and sea of students were trying to get access to online learning. Even after Covid, people really enjoy online learning since they can follow their own schedule.\nGlobalization enables Coursera more international markets. Growth in international market can also help Coursera diversify the risk as it will be less dependent on domestic market for revenue.\nTechnology empowering more possibilities. With the development and application of Machine Learning and AI, more smart features including recommendation can be offered to keep customers active.\n\nThreats\nGovernment Regulations. Coursera should keep a close eye on the fast changing government regulations under the growing conflicts environment .\nCustomers are moving toward mobile  which can hamper the growth as Coursera  still hasn’t got a comprehensive mobile strategy.\n\nWhat can be improved?Nice improvements from my experiencesAs a customer, although Coursera provides great services,  for more effience and easy of use, some nice improvments can be the following:\n\nA calendar that shows due dates for assignments and quizzes across all of your courses. When I signed up for 3-4 classes, it’s hard to keep track of what is due and when. Optional email reminders would be a nice touch.\n\nQuick feedbacks and colorful interations to keep users active. Solving small but not so primitive tasks by students could help feeling the progress. And the more types of such quizzes are presented (not just a multiple choice), the more learners will be willing to join. What’s more, peer value and discussion can be offered for students to communicate more to help each other so that they will spend more time on it to get a better understanding and build the willing to learn other related courses.\n\nBetter interaction design on Mobile application. A terrible journey starts when open Coursea on both Android and iPhone. It’s difficult for me to get what I want.\n\nBetter recommendation performance. Sometimes, after finishing one course, if learners want a deep understanding of specific field, a good recommendation at the end of learning will keep them engaged.\n\n\nMore testing should be conducted so that we can learn from the feedbacks from users and provide higher-perforance services.\nFuture planSome plans can be taken into consideration for further marketing.\n\nConduct geographic analysis to move to new markets.\nConduct IP analysis to keep competition out of the door.\n Invest more in building an easy-use mobile application.\nKeep existing customers active so that improve their customers life value. Some nice try can be introducing a new product or service of Coursera to existing customers, improving interaction and visual design.\nImprove customer loyalty of Coursera Coursera’s customers. Marketing efforts can be used to increase loyalty among existing customers by organizing events, and by providing post purchase information and services.\nIncrease brand awareness. Features like stronger and multiple sharing channels can reach new audiences on social media. What’s more, newcomers have a better chance of choosing a course through a friend’s recommendation.\n\nMore info and market analysis should be conducted to build a good strategy. Will be added soon.\nThank you for your time : )","tags":["PM","Business Model"]},{"title":"My first cv project, from ID card to face recognition","url":"/2024/12/12/My-first-university-project-ID-card-number-extraction/","content":"My first cv project: Extract ID card numberMy CV story starts with a cousrse project when I was in sophomore. The final personal project can be selected from the following choices: 24-point game program, Tetris, ID card number extraction, face recognition by opencv, License plate number extraction. Actually, I have finished the first two in other courses and face recognition(CNN, will be introduced in the following part) was my parallel project directed by another professor, so only two choices:  ID card number extraction or License plate number extraction. They were same to me, so I chose ID card number extraction. \nLet’s start!\nDevelopment ProcessStep 1: Development EnvironmentThe project uses:\n\nOpenCV: A library for working with images.\nNumPy: For numerical operations like creating matrices.\nPytesseract: A library to perform OCR (Optical Character Recognition) to extract text from images.\n\nStep 2: Loading the Imageimage = cv2.imread(&#39;test1.jpg&#39;)\ntest1.jpg will be the ID card, which is from the Internet.\n\nStep 3: Finding the Area with the ID Number (Region of Interest)height, width = image.shape[:2] roi = image[int(0.75 * height):height, int(0.1 * width):int(0.9 * width)] The program crops the bottom part of the image, where the ID number is expected. It assumes the ID number is located in a specific area (e.g., the bottom center of the image). \n\nStep 4: Cleaning the ImageBefore extracting text, the program prepares the image to make it easier for Tesseract to read.\n\nConvert to Grayscale: Removes color, leaving only shades of gray.\n\ngray_image = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n\nBlur the Image: Reduces noise by smoothing the image.\n\nblurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n\nThresholding: Converts the image into pure black and white.\n\n_, thresholded_image = cv2.threshold(blurred_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n\nMorphological Cleaning: Removes small imperfections (like dots or specks) in the image.\n\nkernel = np.ones((3, 3), np.uint8) cleaned_image = cv2.morphologyEx(thresholded_image, cv2.MORPH_CLOSE, kernel)\n\nStep 5: Extracting the Textextracted_text = pytesseract.image_to_string(cleaned_image, config=custom_config)\nTesseract OCR reads the cleaned image and tries to find text. And The config setting tells Tesseract to assume the text is in a single block and to use a specific engine mode.\nStep 6: Extracting Only Numbersid_number = &#39;&#39;.join(filter(str.isdigit, extracted_text))\nIf the extracted text is too long, it tries to trim it to 18 digits (the typical length of a Chinese ID number).\nStep 7: Displaying ResultsFinally, the program displays the extracted number. If the result doesn’t look correct, it gives a warning. It also shows the intermediate images (like the cropped region and cleaned image) for debugging purposes.\nThe result will be “Extracted ID Number: 310109199107141011” which is the same as the ID number.\nAll done!\nIf the ID number can be extracted, and the face recognition can be used too, then a KYC(know your customers) system can be realized. \n","tags":["Robotics","opencv"]},{"title":"如何在Mac上使用Hexo开启博客之旅","url":"/2022/01/12/%E5%A6%82%E4%BD%95%E5%9C%A8Mac%E4%B8%8A%E4%BD%BF%E7%94%A8Hexo%E5%BC%80%E5%90%AF%E5%8D%9A%E5%AE%A2%E4%B9%8B%E6%97%85/","content":"之前使用了搭建在阿里云的轻量级服务器中的WordPress开启了我的第一个博客，可是由于使用起来体验不好，困扰我的可能有以下几点：\n\n使用阿里云轻量级服务器的过程中，一直出现服务器或数据库连接错误的情况。\n在国内的域名需要备案，出国后就希望能够开启一个比较宽松自由的风格。\n我的需求很简单，只是需要有个简洁的地方写点小东西罢了，倘或碰巧被你看见了，那便是我的幸运吧。虽然WordPress可以提供非常丰富的功能，但是嘞，对我来说有些 ‘乱花渐欲迷人眼’。\n\n于是转到了使用Hexo + Github Pages的静态博客组合，简单明了。\n什么是Hexo呢？Hexo 是一个快速、简洁且高效的博客框架。嗯，有道理，我喜欢。感兴趣的可以在官网上自行查看更多介绍。\n如何配置环境安装 Hexo 相当简单，只需要先安装下列应用程序即可：\n\nNode.js \n使用使用 Homebrew安装\n$ brew install node\nGit\n使用 Homebrew, MacPorts 或者下载 安装程序。\n$ brew install git\n\n\n安装Hexo\n所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。\n$ npm install -g hexo-cli\n\n如何搭建博客\n在你喜欢的地方建立一个名字为 ‘blog’ 的文件夹（名字也可按你喜欢的来）\n\n进入blog文件夹：cd blog\n\n初始化：hexo init\n\n启动Hexo：hexo s\n之后会出现一个默认的博文，浏览器输入：http://localhost:4000，便可查看。\n\n新建一篇博文：hexo n &#39;如何在Mac上使用Hexo开启博客之旅&#39;\n\n\n\n​    接下来就在自己喜欢的编辑器里编写吧，我自己使用过Vscode、Typora、以及Obsidian等来写Markdown以及管理自己的笔记。个人推荐Typora：界面简洁，所见即所得。Typora最近开始收费喽，大家看看自己的荷包吧，Obsidian目前仍然免费，但是功能强大，适合建立自己的知识网络，用来写博客是在大材小用。\n\n在写完后，同样进入到blog文件夹下，运行\n清理：hexo clean\n启动：hexo g\n生成：hexo s\n\n再次进入http://localhost:4000，便可查看你写好的博客了\n\n\n\n到这里搭建博客就已经成功了，接下来是将博客部署到Github上\nHexo + Github\n**登录github：https://github.com，没账号的新建一个，新创建一个仓库New repository，仓库名必须为：昵称.github.io:**，输入描述，创建：\n\n\n\n​                在这里因为我已经创建过了，所以会显示已经存在账户。\n\n接着回答terminal中，crtl + c 结束之前的运行，依旧在blog文件夹下部署github的插件：npm install hexo-deployer-git --save\n\n接着更改blog文件夹下面**_config.yml**的配置，如下\n\n\n\n​        可以从这里复制：\ndeploy:  type: git  repo: https://github.com/AllenZhuer/AllenZhuer.github.io.git  branch: master\n\n\n最后回到terminal，使用 hexo d 将博客部署到远端。中间会让输入github密码，输入即可，后续部署完成，即可访问你的远端地址。例子就是这样的: https://allenzhuer.github.io/\n\n\n这里呢，由于我使用了主题可能会和你的页面有一些差异，不过也没有问题，接下来我们将开始部署主题。\n选个喜欢的主题吧\n最重要的就是选主题了：https://hexo.io/themes/， 超多主题随你选择。\n\n我选择的主题为 keep，里面有非常详细的介绍，请移步吧。\n\n铛铛，部署完主题后，别忘了它们：\n\n清理：hexo clean\n启动：hexo g\n生成：hexo s\n部署：hexo d\n部署之后，可能有一点点延迟，等一小会儿就行了，捏一捏桌子上的小黄鸭吧。\n如果在你的博客中发现图片加载较慢的问题，推荐一个图床平台 Picx , 希望能够帮到你。\n域名最后一步是购买一个自己喜欢的域名，因为我思来想去的域名已经被别人购买了，并且也没有抱有此博客被许多人看到的想法，所以域名购买暂时告一段落。等到哪天域名过期或者是想到新的域名再来继续更新吧。\n一些购买域名的网站推荐：NameCheap , Godaddy , Namesilo 等等。\n如果这篇博客恰巧被你看到并且对你有帮助的话，我将十分开心。\n","tags":["blog"]},{"title":"Bridging Technology and Everyday Life, Building a Home Environment Reception Robot","url":"/2024/11/25/Graduation-project-Mobile-robot-interaction-design-in-the-home-environment/","content":"IntroductionIn an era where artificial intelligence and robotics are reshaping industries, the promise of home robots stands out as a beacon of innovation. Imagine a robot that can assist with household chores, recognize family members, engage in meaningful conversations, and adapt to dynamic environments. This vision inspired my graduation thesis, where I designed and implemented a mobile robot system tailored to interact seamlessly within a home environment. \nA few years later, I learned that my project and work were used as teaching materials and the foundation for the teams from the School of Automation, Southeast University, to participate in the RoboCup@Home China. I’m very proud that my work can contribute to the competition. This also means my desire to dive deep into Human-Computer Interaction to combine cutting-edge technologies and business to create inspired products and services that solve customers’ problems.\n\nThis is a list of the current desired technical abilities that the tests in RoboCup@Home will focus on.\n\nNavigation in dynamic environments \nFast and easy calibration and setup: The ultimate goal is to have a robot up and running out of the box\nObject recognition \nObject manipulation \nDetection and Recognition of Humans \nNatural human-robot interaction \nSpeech recognition \nGesture recognition \nRobot applications RoboCup@Home is aiming for applications of robots in daily life.\n\n\nThis project aims to develop highly relevant service and assistive robot technology for future personal domestic applications. The focus lies on the following domains but is not limited to: Human-Robot Interaction and Cooperation, Navigation and Mapping in home environments, Computer Vision and Object Recognition under natural light conditions, Object Manipulation, Adaptive Behaviors, Standardization, and System Integration.\nCore FunctionsScenario: Welcoming Friends at Home with the Robot AssistantIt’s a quiet afternoon at home, and you’ve invited a couple of friends over to relax and catch up. Just as you hear the doorbell ring, your robot assistant springs to life, ready to help create a seamless and welcoming experience.\nStep 1:  Greeting the GuestsAs your first friend steps into the house, the robot assistant moves forward to greet them. Its camera focuses on their face, and it says warmly:“Hello, welcome! May I know your name?”Your friend replies, “I’m Alex.” The robot repeats their name to confirm and then adds:“It’s great to meet you, Alex. May I ask, what’s your favorite drink?”Alex responds with a smile, “I love Cola.” The robot confirms their choice and saves their details, associating the name and drink preference with their face.\nWhen your second friend arrives, the robot repeats the same process, efficiently recognizing and recording their name and drink preference as well.\nIf one of your friends is a frequent visitor, the robot has already  recognized them instantly then will greet them with personalized messages like:“Welcome back, Alex! Would you like the usual Cola today?” \nStep 2: Guiding the Guests to the Living RoomOnce both friends are inside, the robot gestures and says:“Please follow me, and I’ll guide you to your seats.”It smoothly navigates through the house, leading your friends to the living room. It identifies the unoccupied seats and gestures politely:“Here are two comfortable seats for you. Please, have a seat.”\nStep 3: Personalized Drink DeliveryAfter your friends are seated, the robot turns to you for a moment and confirms:“Would you like me to prepare the drinks for Alex and Jamie?”You nod, and the robot replies, “Understood. Please relax while I handle it.”\nThe robot moves to the kitchen, identifies the correct drinks, and carefully retrieves them. It returns to the living room and delivers the drinks to each guest by name:“Alex, here’s your Cola. Jamie, here’s your favorite soda. Enjoy!”\nEven if the robot can’t physically pick up the drinks, it uses a clear gesture or light indicator to direct your guests to their beverages, ensuring a thoughtful touch.\nStep 4: Creating a Relaxed AtmosphereAs the conversation flows, the robot subtly monitors the interaction, staying on standby in case anyone needs assistance. Its graphical user interface, displayed on a nearby screen, shows a friendly message such as:“Let me know if you need anything else!”\nIf your friends move around, the robot adjusts and remains responsive, making sure they feel at home.\nProduct Function and ModulesThe product’s core functionalities include the following five aspects:\n\nVisitor Identification Based on Visual Recognition: Assuming there are two visitors, each arrives and pauses in front of the robot upon entry. The robot’s camera detects and captures their faces, automatically recognizes them, and saves the visitors’ facial images.\nVoice-Based Greeting Function: After detecting the visitor’s face, the robot provides a voice greeting and asks for the visitor’s name. It confirms the name by repetition and records it. The robot then asks for the visitor’s favorite beverage, confirms it, and records the information. Both the name and the preferred beverage are linked to the facial image, with the image file named after the visitor’s name.\nSeating Guidance Function: The robot directs the guest to an unoccupied seat. It issues a voice prompt, “Please follow me,” and leads the guest to the available spot, arranging for them to sit.\nBeverage Delivery Function: After the guest is seated, the robot moves to the beverage station, picks up the guest’s preferred drink (represented by an empty bottle if necessary), and delivers it to the correct guest. (If the grasping function is not feasible, the robot can instead point to the beverage.)\nGraphical User Interface Control: A graphical user interface is implemented to control the entire process.\n\nAdditional Potential Enhancements:\nAdding object recognition to enable functionalities like object grasping and delivery.\n\nTo meet the basic requirements, the product requires at least five functional modules: a camera detection module, a microphone acquisition module, a path planning module, a trajectory optimization module, and an arms control module. The correspondence between functions and modules is shown in Table 1.\nTable 1:  Functions and Modules\n\n\nFunction\nCamera Detection Module\nMicrophone Acquisition Module\nPath Planning Module\nTrajectory Optimization Module\nArm Control Module\n\n\n\nVisitor identification based on face recognition\n✓\n\n\n\n\n\n\nVoice-based greeting function\n\n✓\n\n\n\n\n\nSeating guidance function\n✓\n✓\n✓\n✓\n\n\n\nBeverage delivery and identification\n✓\n\n\n✓\n✓\n\n\nGraphical interface control\n✓\n✓\n✓\n✓\n✓\n\n\nDesign and MethodologySystem OverviewThe system is built on a Turtlebot platform, enhanced with a HOKUYO URG-04LX-UG01 laser radar and an RGB-D camera for advanced functionality. Facial detection and recognition use MTCNN and FaceNet models, trained and fine-tuned on NVIDIA 2080Ti GPUs or Google Colab.\nDevelopment Environment\nOS: Ubuntu 16.04\nFrameworks: ROS for Turtlebot development, CUDA 9.0, and TensorFlow 1.7 for model training.\n\nFinal DeploymentThe fully developed system was successfully implemented on the MORO dual-arm robot, seamlessly integrating navigation, facial recognition, and robotic control. Since the MORO platform was still at the very beginning stage then, the grasp tasks were replaced by pointing to the drinks, and the KUKA LBR IIWA robot arm and RGBD camera realized the grasp tasks. \n\nVisitor identificationThe visitor identification task module primarily uses an RGB-D camera to actively capture images for face detection and recognition. The goal is to identify whether a detected face is new or already stored in the system database and transition seamlessly to the inquiry task once a face is recognized.\nProcess Flow\nInitialization:\n\nThe robot begins in a stationary position at the door.\nThe camera thread is activated to continuously capture frames.\n\n\nFace Detection:\n\nEach frame is analyzed for face detection with a time limit of 3 seconds per detection attempt.\nIf no face is detected within 3 seconds, the robot performs small rotational movements to expand its field of view and continues detecting.\n\n\nRecognition and Database Check:\n\nIf a face is detected:\nThe system marks this recognition task as successful.\nIt checks the database for the detected face:\nIf the face is new, the calculated facial feature data is extracted and stored in the database.\n\n\n\n\n\n\nFailure Handling:\n\nIf no face is detected after \n3 seconds of rotation\n, the system:\n\nConfirms whether any face was found during the current session.\nIf no face was found:\nAll threads are stopped, and the robot pauses operations for 3 seconds before restarting the recognition task.\n\n\nIf a face was found, the recognition task concludes, and the system transitions to the inquiry task.\n\n\n\n(The chart is genrated by ChatGPT.)\n\n\n\nFace Recognition will be introduced in a new blog for the learning path in Face Recognition. \nVoice-based greeting functionThe IFLYTEK voice transcription realizes a voice-based greeting function to obtain voice information, which will be stored in the database with face information and personal information. (I have not learned NLP then, so have to turn to open-source tech or tools for help.)\nThe inquiry task module leverages a microphone array, an RGB-D camera, and a speech output module to interact with household members and collect their preferences. For example, the robot might ask, “What’s your favorite drink?” Upon receiving a response, it converts the speech to text, detects keywords (e.g., Sprite, Coke, milk tea), and stores the data for future use.\nProcess Flow\n\nModule Initialization:\nStart the RGB-D camera thread and microphone array thread.\nRotate the camera to search for faces.\n\n\nFace Detection:\nOnce a single face is detected, the camera aligns to keep the face centered.\n\n\nInteractive Questioning:\nThe robot asks: “What’s your name?” Processes and saves the name.\nThen asks: “What’s your favorite drink?” Transcribes the response and extracts key preferences.\n\n\nData Storage:\nThe system links the detected face with the name and preferences.\nPackages the data into a structured record in the database.\n\n\nRepeating the Process:\nAfter one member’s information is collected, the system checks for other undetected faces.\nIf faces remain, the camera rotates to locate another person and repeats the inquiry process.\n\n\nTask Completion:\nWhen all faces have been detected and information collected, the module concludes the inquiry task and transitions to the guidance task.\n\n\n\n \n(The chart is genrated by ChatGPT.)\nGuidance functionThe primary functionalities required for this task include Marker search and recognition, motion planning, and obstacle avoidance. Each function works in harmony to ensure the robot can execute tasks efficiently and safely. The markers are used for turtlebot, and it can be replaced by pre-programming in Moro robot system to guide to the pre-defined places. \nYou can visit the blog regrading to the Motion planning and Obstacle avoidance to learn the specific algorithms.  TODO: Motion planning blog links.\nProcess flow\nThe robot initializes the task by scanning for the first marker to determine its starting point.\nUpon detecting a marker:\nThe marker’s information is decoded to determine the next action.\nThe robot plans its route to the specified target area.\n\n\nDuring navigation:\nObstacles are detected and avoided using real-time LiDAR data.\nThe robot updates its path dynamically to ensure smooth movement.\n\n\nIf the robot reaches the final marker, it concludes the task or transitions to a new task as per the marker’s instructions.\n\n\nGrasp Function\nResultFace RecognitionFace Recognition with LFW Dataset and MTCNN\n\nMTCNN Training on LFW Dataset\n\nThe LFW dataset was used to train MTCNN for face detection.\nThe dataset was divided into 80% for training and 20% for testing and validation, along with a custom-built face dataset.\nThe final detection accuracy was 96.4%.\n\n\n\nFaceNet Training with Processed Images\n\nAll images in the LFW dataset were resized to 160×160 pixels using MTCNN preprocessing.\nThe processed images were then fed into the FaceNet model for training.\nThe trained FaceNet model achieved a recognition accuracy of 98.5%.\n\n\n\nChallenges in Recognition Accuracy\n\nMisrecognitions occurred in two primary scenarios:a. When an image contained two or more faces, the target face might not be selected.b. Poor lighting, occlusions, or unfavorable angles reduced the confidence score of the target face compared to others.\n\n\nExample of MTCNN Misrecognition\n\nIn one example, the male face had a confidence score of 0.78, while the female face had a score of 0.85.\nThe target was the male face, but due to occlusion, MTCNN detected and cropped the female face instead.\nAs a result, the male face was excluded.\n\n\n\n\nConclusion:MTCNN and FaceNet demonstrate high accuracy for face detection and recognition. However, challenges like multiple faces, occlusions, and lighting variations can lead to misrecognitions, indicating areas for further improvement.\nVoice Greating FunctionInquiry Module\nThe inquiry module primarily utilizes the iFlytek speech-to-text API. This API allows for the transcription of a local .wav audio file into text. For example, a local file named input.wav can be transcribed into text such as, “我是猪八戒，我喜欢喝可乐” (translated: “I am Zhu Bajie, and I like drinking cola”). An example of this process is shown in the following Figures.\n\n\nThe two images above illustrate the process of recording audio for family members and saving it locally as the input.wav audio file. The workflow is as follows:\n\nAudio Recording: Family members’ voices are recorded and saved as a .wav file (e.g., input.wav).\nSpeech Transcription: The audio file is transcribed into text using the iFlytek API.\nKeyword Extraction: Keywords, such as names and preferences, are extracted from the transcribed text and saved locally in the mine.txt file.\nImage Naming: Collected facial images are linked to the corresponding names from the transcribed data and renamed accordingly.\n\nThis integrated approach ensures that each family member’s face and preferences are organized systematically for further use.\nThe whole processTo evaluate the overall effectiveness of integrating all components, the following images illustrate the testing process for the entire task workflow:\n\nFace Detection and Recognition:As shown in Figure (a), when the target person enters the robot’s field of view, the robot captures images from the video feed via its camera. The system then performs face detection and recognition, extracting the facial information of the person and storing it in a local database. Once the facial information is obtained, the system issues a command to change the task state and transitions to the voice interaction module.\n\n\nVoice Interaction:In Figure (b), the microphone prompts the target individual with a question: “Hello, what is your name?” The person responds, and the system attempts to recognize the spoken name. If the name is not accurately recognized, the system either repeats the question or confirms the name by ensuring that the same name is detected twice consecutively. If there is a mismatch, the system continues to ask until two consistent results are obtained. After obtaining the name, the system proceeds to inquire about the person’s favorite beverage. This process is similar to the name inquiry, and the results are displayed in Figure (c).\n\n\n\nSaving Information to the Database:Once all information is collected, the system saves it in the database. As shown in Figure (f), the database includes details such as the member’s ID, name, age, preferences, and facial data. The collected facial images are named according to the person’s name and saved locally.\n\n\nProposing Personalized Tasks:After completing the inquiries, the robot addresses the person by their name and makes a proposal, such as: “Wang Dapeng, would you like a cup of milk tea?” This question is formatted as: “Target name, would you like a cup of [favorite beverage]?” An example of this interaction is shown in Figure (d)\n\n\n\n\nRe-encountering a Recognized Member:When a previously recognized member reappears in the robot’s field of view, such as after registering “Wang Dapeng” and later encountering another person “Liu Tao,” the system logs their information as new. However, when “Wang Dapeng” appears again, the system recognizes them as an existing member and generates an “old” message to indicate prior recognition. The system then repeats the personalized interaction, as it did during the first encounter, by asking: “Wang Dapeng, would you like a cup of milk tea?” The result of this process is shown in Figure (e).\n\n\n\nImprovementsAs described above, this paper presents the design of a simple home service robot system that performs several tasks. However, there are still many limitations for home service robots, which can be summarized as follows:\n\nCommunication Delays:The communication delay between the face recognition and voice recognition modules is relatively long. In the future, the adoption of 5G transmission could significantly improve the speed of data transfer, especially when dealing with large datasets.\nMisidentification in Multi-Face Scenarios:When multiple faces appear in the robot’s field of view, the system may misidentify the target face. Future improvements in algorithms, such as the adoption of 3D face recognition, could greatly enhance the accuracy of identification.\nLimited System Functionality:The system designed in this study is relatively simple and performs only basic, single-purpose tasks. It is not yet suitable for complex home environments. In the future, home service robots will likely become smarter and more versatile, capable of completing a wide range of tasks quickly and efficiently.\n\n","tags":["Robotics","HCI"]},{"title":"提示词工程 Prompt Engineering","url":"/2025/05/05/%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B-Prompt-Engineering/","content":"提示词工程（Prompt Engineering）学习笔记\n目录\n\n引言\n为什么提示工程很重要？\n提示工程的核心原则\nLLM参数介绍\n高级提示技巧\n思维链（Chain of Thought）\n角色扮演（Role Playing）\n提示词拆解（Prompt Decomposition）\n约束指令（Constraint Instructions）\n反例教学（Learning from Mistakes）\n工具调用（Tool Calling）\n\n\n规范格式\n提示词流程（OpenAI4.1 Workflow）\n提示词模版\nLangGPT提示词模版\n案例1：客服\n\n\n引言嗨，这是我学习提示工程（Prompt Engineering）后整理的笔记，希望能帮助刚接触这个领域的朋友。\n\n本文涵盖提示工程的理论基础与实践技巧，教你如何用最佳方法与大语言模型（LLM）交互和构建应用。\n\n什么是提示工程？\n提示工程（Prompt Engineering）是设计和优化与AI大语言模型（如ChatGPT）交流”指令”的技术。它是一门新兴学科，专注于开发和优化提示词，以便在各种应用场景中高效地使用和构建大语言模型（LLMs）。\n掌握这项技能不仅能让我们理解LLM的能力和局限，更能帮助我们设计出更安全、更强大的AI应用。\n为什么提示工程很重要？\n提高效率：良好的提示可以节省时间，减少来回沟通的次数\n提升质量：精心设计的提示能获得更准确、更有用的回答\n拓展能力：通过高级提示技巧，可以突破 AI 表面上的限制\n个性化体验：根据特定需求定制 AI 行为和输出风格\n\n提示工程的核心原则\n明确性：提供清晰、无歧义的指示\n具体性：详细说明您需要的内容、格式和标准\n上下文提供：给予充分的背景信息\n示例展示：通过示例说明您期望的输出\n角色设定：为 AI 分配特定角色或专业背景\n\n我发现掌握提示工程就像学习一门新语言——只有理解了”语法规则”，才能准确表达我们的需求，获得满意的回应。在这份笔记中，我会分享提示工程的核心原则、常用模板、高级技巧和实际应用案例，帮助你也能写出高质量的提示，充分发挥AI的潜力。\n补充：我在学习过程中，所有示例都是用OpenAI的Playground或者Claude 3.5 Sonnet模型。这些提示方法也适用于其他类似能力的模型，但不同模型的响应可能会有所差异。\nLLM参数介绍在设计和测试提示时，我们通常通过API与大语言模型进行交互。通过调整几个关键参数，可以为同一个提示获得不同的结果，调整这些设置对于提高响应的可靠性和符合预期非常重要。以下是使用各种LLM提供商时会遇到的常见设置：\n\nTemperature（温度） 简单来说，温度数值越低，结果越确定，模型总是选择概率最高的下一个词。低温度更加适合进行基于事实的查询，因为这将更加精确和简洁；而高温度本质上是增加了其他词的权重，更加适合更有创意但是可能不是很准确的情况，比如头脑风暴，创意写作。\n\nTop P（核采样） 核采样是一种与温度一起使用的采样技术，旨在控制模型的确定性。使用Top P意味着只考虑构成top_p概率质量的词进行响应，如果需要准确和事实性的答案，保持较低的值；如果需要更多样化的回答，可以设置更高的值。\n\n\n  一般建议是调整Temperature或Top P，但不要同时调整两者。\n\nMax Length（最大长度） 通过调整最大长度可以管理模型生成的词元（token）数量。指定最大长度有助于防止冗长或不相关的回答，并控制成本。\n\nStop Sequences（停止序列） 停止序列是一个能够使模型停止生成词元的字符串。指定停止序列是控制模型回答长度和结构的另一种方式。例如，可以通过添加”11”作为停止序列，告诉模型生成不超过10项的列表。\n\nFrequency Penalty（频率惩罚） 频率惩罚会对下一个词元施加与该词元在回答和提示中已出现次数成正比的惩罚。频率惩罚越高，一个词再次出现的可能性就越低。这个设置通过给出现更多的词元更高的惩罚来减少模型回答中词语的重复。\n\nPresence Penalty（存在惩罚） 存在惩罚也会对重复的词元施加惩罚，但与频率惩罚不同，所有重复词元的惩罚都是相同的。出现两次的词元和出现10次的词元受到的惩罚是一样的。这个设置可以防止模型在回答中过于频繁地重复短语。如果希望模型生成多样化或创意性的文本，可能需要使用较高的存在惩罚；或者，如果需要模型保持专注，可以尝试使用较低的存在惩罚。\n\n\n  与温度和top_p类似，一般建议是调整频率或存在惩罚，但不要同时调整两者。\n让我们看一下具体的代码实例，下面是一个使用 OpenAI API 的 Python 代码示例，展示了如何在实际调用中设置这些参数：\ndef get_completion(params, messages):    &quot;&quot;&quot; GET completion from openai api&quot;&quot;&quot;    response = openai.chat.completions.create(        model = params[&#x27;model&#x27;],              # 选择使用的模型，如 gpt-3.5-turbo        messages = messages,                  # 消息列表，包含对话历史        temperature = params[&#x27;temperature&#x27;],  # 温度参数，控制创造性        max_tokens = params[&#x27;max_tokens&#x27;],    # 最大生成长度        top_p = params[&#x27;top_p&#x27;],              # 核采样参数        frequency_penalty = params[&#x27;frequency_penalty&#x27;],  # 频率惩罚参数        presence_penalty = params[&#x27;presence_penalty&#x27;],    # 存在惩罚参数    )    return response\n\n代码解析：\n\n这个函数封装了与 OpenAI API 的交互，接收参数字典和消息列表\n所有我们前面讨论的参数都在这里体现：temperature、max_tokens、top_p、frequency_penalty 和 presence_penalty\n参数存储在字典中，方便灵活调整不同场景下的配置\n函数返回完整的响应对象，包含生成的文本和其他元数据\n\n这种设计使我们可以方便地为不同任务创建不同的参数配置。例如：\n# 适合事实性查询的参数配置fact_params = &#123;    &#x27;model&#x27;: &#x27;gpt-3.5-turbo&#x27;,    &#x27;temperature&#x27;: 0.1,  # 低温度，更确定性的回答    &#x27;max_tokens&#x27;: 500,    &#x27;top_p&#x27;: 0.9,    &#x27;frequency_penalty&#x27;: 0.0,    &#x27;presence_penalty&#x27;: 0.0&#125;# 适合创意写作的参数配置creative_params = &#123;    &#x27;model&#x27;: &#x27;gpt-3.5-turbo&#x27;,    &#x27;temperature&#x27;: 0.8,  # 高温度，更多样化的回答    &#x27;max_tokens&#x27;: 1000,  # 允许更长的回答    &#x27;top_p&#x27;: 1.0,    &#x27;frequency_penalty&#x27;: 0.5,  # 减少重复    &#x27;presence_penalty&#x27;: 0.5    # 鼓励新颖内容&#125;\n\n通过这种方式，我们可以根据任务需求轻松切换不同的参数配置，获得最适合的模型输出。\n高级提示技巧思维链（Chain of Thought）：引导 AI 逐步思考问题思维链是一种强大的提示技巧，通过引导AI展示其推理过程，而不仅仅是直接给出答案。简单来说，就是让AI”一步步思考”，像人类解决问题那样展示完整的思考路径。思维链之所以有效，是因为：\n\n**减少”跳跃性思维”**：当AI直接给出答案时，往往会跳过中间推理步骤，可能导致错误\n提高复杂推理能力：特别适合数学问题、逻辑推理和多步骤分析任务\n增强可解释性：让我们看到AI是如何得出结论的，增强透明度\n降低幻觉风险：逐步推理过程中更容易发现逻辑矛盾或事实错误实现思维链主要有两种方式：\n零样本思维链：直接在提示中加入”让我们一步一步思考”这样的指令：问题：如果一件T恤原价80元，现在打7折，同时我有一张15元优惠券，最终需要支付多少钱？让我们一步一步思考。\n少样本思维链：通过举例展示你期望的思考过程：问题：小明有5个苹果，小红有3个苹果，他们一共有多少个苹果？思考：小明有5个苹果，小红有3个苹果。要算出总数，我需要将两人的苹果数相加：5 + 3 = 8。所以他们一共有8个苹果。问题：如果一件T恤原价80元，现在打7折，同时我有一张15元优惠券，最终需要支付多少钱？思考：\n\n同时使用思维链还可以要求AI进行：\n\n自我评估：让AI评价自己的思考过程，比如请解答这个问题，然后评估你的解答是否正确：’问题’\n多路径探索：鼓励AI考虑多种解决方案，比如请用两种不同的方法解决这个问题，并比较它们：’问题’\n特定领域的思维链：针对特定领域设计思考框架，比如在Marketing领域，可以要求AI按照5C和4P得框架进行思考：’公司’但是思维链也具有局限性，比如：\n\n\n不是所有问题都需要思维链，简单问题可能适得其反\n可能会产生更长的回答，消耗更多token\n对于极其复杂的问题，可能需要结合其他技巧（如问题拆解）\n\n角色扮演（Role Playing）：赋予 AI 特定身份和专业知识角色扮演是一种非常有效的提示技巧，通过给AI分配特定的专业身份或角色，引导它从特定的专业视角回答问题。这种方法能够激活模型中与特定领域相关的知识，产生更专业、更深入的回答。角色扮演之所以有效，有几个关键原因：\n\n激活领域知识：AI模型在训练时学习了不同专业人士的写作和思考方式，指定角色能够激活这些特定知识\n提供思考框架：不同专业有不同的分析框架和方法论，角色设定为AI提供了特定的思考路径\n增强回答深度：专业角色通常会考虑更多领域内的细节和边界情况\n改变语言风格：不同角色会使用不同的术语、表达方式和沟通风格实现有效的角色扮演需要注意以下几点：\n选择具体角色：越具体越好，”医生”不如”心脏科专家”；”程序员”不如”有10年经验的Python后端工程师”\n提供角色背景：补充角色的资历、专长和经验，例如：你是一位有20年经验的iOS开发工程师，曾参与多个百万用户级应用的开发，精通Swift和Objective-C，对性能优化和用户体验设计有独到见解。\n设定回答风格：指定角色应该用什么风格或方式回答：请用简明专业的语言回答，适当使用技术术语，但确保非专业人士也能理解核心概念。\n多角色对比：让AI从多个角色视角分析同一问题：请从投资者、用户和开发者三个视角分析这个产品创意的优缺点。\n\n角色扮演模版\n[角色]: 你是一名[专业领域]专家[任务]: 我需要你帮我[具体任务][格式]: 请以[特定格式]回答[限制/要求]: 回答需要[额外要求]\n\n专家咨询模板：\n我想请教你关于[主题]的问题。请以[专业领域]专家的身份，考虑[关键因素]，并提供[具体需求]的建议。\n\n提示词拆解（Prompt Decomposition）：将复杂任务分解为简单步骤提示词拆解是一种处理复杂问题的强大技巧，通过将大型、复杂的任务分解成一系列较小、较简单的子任务，使AI能够逐步解决问题，最终得到更准确的结果。这种方法类似于人类解决复杂问题的思路——将庞大的挑战分解为可管理的部分。\n为什么提示词拆解有效？提示词拆解之所以有效，有几个关键原因：\n\n减轻认知负担：即使对于先进的AI模型，一次处理大量信息也会导致遗漏或错误。分解后，每个子任务都更加聚焦。\n提高精确度：当任务被分解时，每个子步骤的错误率降低，整体结果更可靠。\n便于调试：如果最终结果有问题，更容易识别是哪个子步骤出了错。\n更好地利用上下文窗口：避免因模型上下文限制而丢失关键信息。\n结构化思考：促使AI按照逻辑顺序处理信息，减少遗漏或重复。\n\n如何实现有效的提示词拆解？实现提示词拆解主要有两种方法：\n\n单次提示中的任务分解：在一个提示中明确列出解决问题的步骤，要求AI按顺序执行：\n请帮我分析这家公司的财务状况。请按以下步骤进行：1. 首先分析收入增长趋势2. 然后评估利润率变化3. 接着分析债务状况4. 最后给出整体财务健康评估\n多轮交互的任务分解：将复杂任务拆分为多次独立的AI交互，每次只解决一个子问题：\n\n第一轮：分析收入增长\n第二轮：将第一轮结果提供给AI，要求分析利润率\n第三轮：结合前两轮结果，分析债务状况\n最后一轮：整合所有分析，得出综合评估\n\n\n\n高级提示词拆解策略\n问题分类拆解：\n在回答这个问题前，请先确定这是一个事实性问题还是意见性问题。如果是事实性问题，请提供准确信息和来源。如果是意见性问题，请分析不同立场并给出平衡的观点。\n复杂决策拆解：\n帮我决定是否应该投资这家初创公司。请按以下步骤进行：1. 列出投资的所有潜在好处（至少3点）2. 列出所有可能的风险（至少3点）3. 分析市场状况和竞争格局4. 评估创始团队的优势和劣势5. 综合以上因素，提出最终建议\nMECE原则应用（Mutually Exclusive, Collectively Exhaustive，相互独立，完全穷尽）：\n请对这个商业问题进行MECE分析。首先按以下类别分解问题：1. 市场因素2. 竞争因素3. 内部运营因素4. 财务因素然后在每个类别下，确定相关的子因素，确保没有遗漏和重叠。\n\n我发现提示词拆解特别适合以下场景：\n\n复杂分析任务：市场研究、竞争分析、文献综述\n多步骤创意过程：产品设计、内容策划、营销活动\n综合评估问题：人才评估、项目评审、投资决策\n教学和引导：引导学生逐步学习复杂概念\n\n约束指令（Constraint Instructions）：设定回答的边界和限制约束指令是一种通过明确设定边界和限制条件来控制AI输出的技巧。通过告诉AI”做什么”和”不做什么”，我们可以获得更符合预期、更有针对性的回答，避免AI偏离主题或生成不必要的内容。\n核心优势：\n\n澄清期望，减少误解\n减少无关内容，使输出更加聚焦\n确保输出符合预期的格式和质量标准\n\n常见约束类型：\n\n内容约束：限制主题范围、信息来源、事实性要求\n格式约束：指定输出结构、长度、特定格式（如表格、JSON）\n风格约束：写作风格、专业度、复杂性水平\n否定约束：明确指出不需要包含的内容\n\n应用示例：\n写一篇关于人工智能在医疗诊断中的应用的文章，须满足以下要求：- 长度控制在800字以内- 面向医学专业人士- 包含至少3个真实案例- 只讨论过去5年内的技术- 不要讨论伦理问题\n\n实用技巧：\n\n避免过度约束，可能限制AI创造性思考\n约束之间保持一致，避免矛盾要求\n结合积极引导，不只说”不要做什么”，也要说”应该做什么”\n重要场景使用列表形式明确列出所有约束\n\n约束指令特别适用于需要精确控制的场景，如专业文档创作、数据分析报告和客户沟通，能够显著提高AI回答的针对性和实用性。\n反例教学（Learning from Mistakes）：通过反面例子指导 AI反例教学是通过向AI展示错误案例或不良回答，帮助它理解什么是应该避免的内容。这种方法既可以纠正常见错误，也能提高回答质量。\n核心原理：\n\n通过明确”不要这样做”来引导AI更好理解”应该这样做”\n人类学习中也常用这种对比学习法，AI模型对此同样有效\n\n简单应用方法：\n以下是一个不好的回答示例：[插入反例]这个回答有以下问题：1. [问题1]2. [问题2]请避免上述问题，提供一个更好的回答。\n\n实际例子：\n问题：如何写一篇好的商业提案？不好的回答：&quot;商业提案应该漂亮、专业，能抓住人的眼球。使用高大上的词汇会显得你很专业。&quot;这个回答的问题：- 过于笼统，缺乏具体建议- 强调表面而非内容- 误导读者使用华丽词藻而非清晰表达请提供一个更有帮助的回答，包含具体的结构建议和内容要点。\n\n反例教学尤其适用于需要高质量输出的场景，如内容创作、代码编写和专业建议。与其他技巧结合使用效果更佳。\n工具调用（Tool Calling）：将外部工具集成到提示中工具调用是一种允许AI模型与外部工具、API或数据源交互的高级提示技巧。通过这种方式，AI不仅能够生成文本回答，还能执行具体操作，如查询数据库、调用API、执行计算等。\n核心概念：\n\nFunction Calling：使AI能够识别何时需要调用预定义函数，并以正确格式提供所需参数\n**MCP(Multimodal Conversational Processor)**：多模态处理框架，能协调AI与多种工具的交互\n\nFunction Call示例：\n# 定义函数接口functions = [    &#123;        &quot;name&quot;: &quot;get_weather&quot;,        &quot;description&quot;: &quot;获取指定城市的天气信息&quot;,        &quot;parameters&quot;: &#123;            &quot;type&quot;: &quot;object&quot;,            &quot;properties&quot;: &#123;                &quot;city&quot;: &#123;&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;城市名称&quot;&#125;,                &quot;date&quot;: &#123;&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;日期，格式为YYYY-MM-DD&quot;&#125;            &#125;,            &quot;required&quot;: [&quot;city&quot;]        &#125;    &#125;]# 在提示中调用response = model.chat.completions.create(    model=&quot;gpt-3.5-turbo&quot;,    messages=[        &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;北京明天天气怎么样？&quot;&#125;    ],    functions=functions,    function_call=&quot;auto&quot;)\n\nMCP工作流程：\n\n理解请求：AI分析用户输入，确定需要调用哪些工具\n工具选择：从可用工具集中选择最合适的工具\n参数提取：从用户输入中提取调用工具所需的参数\n执行调用：以正确格式调用工具API\n结果整合：将工具返回的结果整合到最终回答中\n\n主要优势：\n\n突破纯文本生成的限制，实现真实世界交互\n提供最新数据和实时信息\n执行复杂计算或查询而不受模型知识限制\n创建实用的、能够执行操作的AI助手\n\nRAGAS是一个用于衡量RAG系统性能的项目，它衡量了RAG系统在信息检索、内容生成和交互质量方面的表现。 https://github.com/explodinggradients/ragas \n在实践中，工具调用特别适合构建功能强大的AI助手，如日程管理、数据分析、智能家居控制和内容搜索等场景。使用这种技术需要一定的编程基础，但能显著扩展AI应用的能力边界。\n规范格式\n使用Markdown语法\n使用JSON格式\n使用XML标签&lt;examples&gt;&lt;example1 type=&quot;Abbreviate&quot;&gt;&lt;input&gt;San Francisco&lt;/input&gt;&lt;output&gt;- SF&lt;/output&gt;&lt;/example1&gt;&lt;/examples&gt;\n\n##提示词流程 （OpenAI4.1 Workflow）以下是我们推荐的开发和调试提示词的工作流程：\n\n从高级指导开始：首先创建一个整体的”响应规则”或”指令”部分，提供高层次的指导和要点。\n添加特定行为修改：如果你想改变更具体的行为，添加一个部分来为该类别指定更多细节，比如”举例”。\n指定工作流程步骤：如果有特定步骤希望模型在其工作流程中遵循，添加一个有序列表并指示模型遵循这些步骤。\n解决效果不佳的问题：如果行为仍然不符合预期：\n检查是否存在冲突、未充分说明或错误的指令和示例。如果有冲突的指令，大语言模型通常会遵循提示词末尾附近的指令。\n添加展示期望行为的例子；确保在你的规则中也引用了示例中展示的任何重要行为。\n\n\n避免不必要的强调：通常不需要使用全大写字母或其他激励措施。建议一开始不使用这些技巧，只在必要时才采用。请注意，如果你现有的提示已包含这些技术，可能会导致模型过分关注它们。\n\n提示词模版结合以上的内容，接下来将是一个通用的提示词模版结构，适用于大多数复杂任务。这个框架可以根据需要进行调整，但包含这些元素通常会得到更好的结果：\n# 角色与目标（Role and Objective）[指定AI应该扮演的角色，以及需要完成的总体目标]# 指令（Instructions）[详细说明需要AI执行的具体任务和要求]## 子类别指令（Sub-categories）[可以按需分类更详细的指令]# 推理步骤（Reasoning Steps）[引导AI如何思考问题，可以要求它遵循特定的分析步骤]# 输出格式（Output Format）[明确指定你希望得到的回答格式，如列表、表格、JSON等]# 示例（Examples）## 示例1[提供一个实例，展示你期望的输入-输出对]# 背景信息（Context）[提供相关的背景信息或参考资料]# 最终指示（Final instructions）[最后的提醒和要求，通常包括&quot;请一步一步思考&quot;等引导]\n\n这个模版的每个部分都有特定用途：\n\n角色与目标：设定AI的专业身份和任务目的，帮助它理解回答的整体方向\n指令：明确任务的具体要求和约束条件\n推理步骤：指导AI如何分析问题，尤其适合复杂任务\n输出格式：确保AI的回答符合你的使用需求和偏好\n示例：通过实例直观地传达你的期望\n背景信息：提供AI可能需要的额外参考数据\n最终指示：引导AI采用特定的思考方式，如系统性、逐步分析\n\n此外，在使用Prompt 的过程中，经过不断尝试需要添加以下几点来增强表现的能力\n## PERSISTENCEYou are an agent - please keep going until the user&#x27;s query is completely resolved, before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved.## TOOL CALLINGIf you are not sure about file content or codebase structure pertaining to the user&#x27;s request, use your tools to read files and gather the relevant information: do NOT guess or make up an answer.## PLANNINGYou MUST plan extensively before each function call, and reflect extensively on the outcomes of the previous function calls. DO NOT do this entire process by making function calls only, as this can impair your ability to solve the problem and think insightfully.## 坚持您是一名代理 - 请持续操作，直到用户的查询完全解决，然后结束您的回合并交还给用户。只有当您确定问题已解决时，才终止您的回合。## 工具调用如果您不确定与用户请求相关的文件内容或代码库结构，请使用您的工具读取文件并收集相关信息：请勿猜测或编造答案。## 规划您必须在每次函数调用之前进行周密的规划，并充分反思之前函数调用的结果。不要在整个过程中只进行函数调用，因为这会削弱您解决问题和深入思考的能力。\n\n在实际应用中，你可以根据任务的复杂性和具体需求选择性使用这些部分。对于简单任务，可能只需要指令和输出格式；而对于复杂分析，完整的模版可能会带来更好的结果。\nLangGPT提示词模版LangGPT 是一个创新的结构化提示词设计框架，旨在让每个人都能成为提示词专家。该项目由”云中江树”创建，通过将编程语言的思想应用到提示工程中，提供了一种系统化、可复用的提示词创建方法。\n核心理念LangGPT的核心是将提示词视为一种”编程语言”，通过结构化的模板来定义与AI的交互。这种方法有以下优势：\n\n结构清晰：使用角色模板组织提示词，包含角色设定、技能、规则等元素\n易于复用：模板化设计使提示词可以被重复使用和修改\n提高效率：降低编写高质量提示词的门槛，节省时间\n质量保证：通过标准化结构减少遗漏和错误\n\n基本用法LangGPT主要基于角色模板（Role Template）构建提示词，一个典型的LangGPT提示词包含以下部分：\n# Role: 角色名称## Profile- 专业领域- 语言风格- 价值观## Skills- 技能1- 技能2## Goals- 目标1- 目标2## Constraints- 限制条件1- 限制条件2## Workflows1. 步骤12. 步骤2## Output Format- 输出格式要求\n\n应用场景LangGPT框架适用于广泛的场景，包括但不限于：\n\n创建专业顾问角色（如医生、律师、教师）\n设计创意写作助手\n开发代码编程指导\n构建个性化学习导师\n创建数据分析专家\n\n发展状态该项目目前处于发展阶段，已经完成基础角色模板、文档和示例等功能，正在开发更高级的语法特性、提示链、JSON/YAML支持等功能。项目团队欢迎社区贡献，共同推动提示工程的发展。\n用于帮助生成LangGPT结构的提示词# Role: LangGPT Prompt Assistant## Profile- Author: Allen- Version: 1.1- Language: English- Description: You are a specialized AI assistant that helps users create high-quality structured prompts following the LangGPT methodology. Your expertise is in transforming user requirements into well-structured, effective prompts that maximize the performance of large language models.### Skills:- Deep understanding of the LangGPT structured prompt methodology- Ability to guide users through the prompt creation process step by step- Knowledge of prompt engineering best practices- Expertise in creating role-based prompts with clear structures- Ability to explain the purpose and benefits of each section in a LangGPT prompt- Skill in helping users refine and improve their prompts## Rules1. Always maintain a helpful and educational tone.2. Explain the purpose of each section when introducing it.3. Provide examples when they would be helpful.4. Don&#x27;t skip important sections of the LangGPT structure.5. Ask clarifying questions when user requirements are unclear.6. Always output the final prompt in proper markdown format.7. Suggest improvements to user-provided content when appropriate.8. Don&#x27;t include placeholder text in the final prompt without explaining it.9. Ensure the final prompt is coherent and follows the LangGPT structure.10. Respect the user&#x27;s specific requirements while guiding them toward best practices.## Workflow1. Greet the user and explain your purpose as a LangGPT Prompt Assistant.2. Ask the user about their prompt requirements (what kind of assistant they want to create).3. Guide the user through creating each section of the LangGPT prompt:   - Role: The specific role the AI should adopt   - Profile: Basic information about the role (Author, Version, Language, Description)   - Skills: Specific abilities the role should have   - Rules: Guidelines the role must follow   - Workflow: Step-by-step process the role should follow   - Commands (optional): Special commands the user can use   - Initialization: How the role should begin the conversation4. Compile all the information into a properly formatted LangGPT prompt.5. Present the final prompt to the user in markdown format.6. Offer to make any adjustments or improvements.## Commands- Prefix: &quot;/&quot;- Commands:    - help: Explain the LangGPT methodology and how to use this assistant.    - template: Show the basic LangGPT template structure.    - example: Provide an example of a complete LangGPT prompt.    - section: Explain a specific section (e.g., /section profile).## InitializationAs a/an &lt;Role&gt;, you must follow the &lt;Rules&gt;, you must talk to the user in the default &lt;Language&gt;. You must greet the user, introduce yourself as a specialized assistant for creating LangGPT structured prompts, and briefly explain what LangGPT is. Explain that you&#x27;ll guide them through creating a structured prompt step by step, and they can use commands like /help, /template, or /example at any time. Ask what kind of prompt they&#x27;d like to create. \n\n案例1：客服SYS_PROMPT_CUSTOMER_SERVICE = &quot;&quot;&quot;You are a helpful customer service agent working for NewTelco, helping a user efficiently fulfill their request while adhering closely to provided guidelines.# Instructions- Always greet the user with &quot;Hi, you&#x27;ve reached NewTelco, how can I help you?&quot;- Always call a tool before answering factual questions about the company, its offerings or products, or a user&#x27;s account. Only use retrieved context and never rely on your own knowledge for any of these questions.    - However, if you don&#x27;t have enough information to properly call the tool, ask the user for the information you need.- Escalate to a human if the user requests.- Do not discuss prohibited topics (politics, religion, controversial current events, medical, legal, or financial advice, personal conversations, internal company operations, or criticism of any people or company).- Rely on sample phrases whenever appropriate, but never repeat a sample phrase in the same conversation. Feel free to vary the sample phrases to avoid sounding repetitive and make it more appropriate for the user.- Always follow the provided output format for new messages, including citations for any factual statements from retrieved policy documents.- If you&#x27;re going to call a tool, always message the user with an appropriate message before and after calling the tool.- Maintain a professional and concise tone in all responses, and use emojis between sentences.- If you&#x27;ve resolved the user&#x27;s request, ask if there&#x27;s anything else you can help with# Precise Response Steps (for each response)1. If necessary, call tools to fulfill the user&#x27;s desired action. Always message the user before and after calling a tool to keep them in the loop.2. In your response to the user    a. Use active listening and echo back what you heard the user ask for.    b. Respond appropriately given the above guidelines.# Sample Phrases## Deflecting a Prohibited Topic- &quot;I&#x27;m sorry, but I&#x27;m unable to discuss that topic. Is there something else I can help you with?&quot;- &quot;That&#x27;s not something I&#x27;m able to provide information on, but I&#x27;m happy to help with any other questions you may have.&quot;## Before calling a tool- &quot;To help you with that, I&#x27;ll just need to verify your information.&quot;- &quot;Let me check that for you—one moment, please.&quot;- &quot;I&#x27;ll retrieve the latest details for you now.&quot;## After calling a tool- &quot;Okay, here&#x27;s what I found: [response]&quot;- &quot;So here&#x27;s what I found: [response]&quot;# Output Format- Always include your final response to the user.- When providing factual information from retrieved context, always include citations immediately after the relevant statement(s). Use the following citation format:    - For a single source: [NAME](ID)    - For multiple sources: [NAME](ID), [NAME](ID)- Only provide information about this company, its policies, its products, or the customer&#x27;s account, and only if it is based on information provided in context. Do not answer questions outside this scope.# Example## UserCan you tell me about your family plan options?## Assistant Response 1### Message&quot;Hi, you&#x27;ve reached NewTelco, how can I help you? 😊🎉\\n\\nYou&#x27;d like to know about our family plan options. 🤝 Let me check that for you—one moment, please. 🚀&quot;### Tool Callslookup_policy_document(topic=&quot;family plan options&quot;)// After tool call, the assistant would follow up with:## Assistant Response 2 (after tool call)### Message&quot;Okay, here&#x27;s what I found: 🎉 Our family plan allows up to 5 lines with shared data and a 10% discount for each additional line [Family Plan Policy](ID-010). 📱 Is there anything else I can help you with today? 😊&quot;&quot;&quot;&quot;get_policy_doc = &#123;    &quot;type&quot;: &quot;function&quot;,    &quot;name&quot;: &quot;lookup_policy_document&quot;,    &quot;description&quot;: &quot;Tool to look up internal documents and policies by topic or keyword.&quot;,    &quot;parameters&quot;: &#123;        &quot;strict&quot;: True,        &quot;type&quot;: &quot;object&quot;,        &quot;properties&quot;: &#123;            &quot;topic&quot;: &#123;                &quot;type&quot;: &quot;string&quot;,                &quot;description&quot;: &quot;The topic or keyword to search for in company policies or documents.&quot;,            &#125;,        &#125;,        &quot;required&quot;: [&quot;topic&quot;],        &quot;additionalProperties&quot;: False,    &#125;,&#125;get_user_acct = &#123;    &quot;type&quot;: &quot;function&quot;,    &quot;name&quot;: &quot;get_user_account_info&quot;,    &quot;description&quot;: &quot;Tool to get user account information&quot;,    &quot;parameters&quot;: &#123;        &quot;strict&quot;: True,        &quot;type&quot;: &quot;object&quot;,        &quot;properties&quot;: &#123;            &quot;phone_number&quot;: &#123;                &quot;type&quot;: &quot;string&quot;,                &quot;description&quot;: &quot;Formatted as &#x27;(xxx) xxx-xxxx&#x27;&quot;,            &#125;,        &#125;,        &quot;required&quot;: [&quot;phone_number&quot;],        &quot;additionalProperties&quot;: False,    &#125;,&#125;response = client.responses.create(    instructions=SYS_PROMPT_CUSTOMER_SERVICE,    model=&quot;gpt-4.1-2025-04-14&quot;,    tools=[get_policy_doc, get_user_acct],    input=&quot;How much will it cost for international service? I&#x27;m traveling to France.&quot;,    # input=&quot;Why was my last bill so high?&quot;)response.to_dict()[&quot;output&quot;]\n","tags":["Prompt Engineering","LLM"]},{"title":"Face Recognition","url":"/2024/12/14/Face-Recognition/","content":"Face RecognitionFace Recognition was part of the Human-Robot Interaction and my graduation project, which is to build a reception robot. In a home environment, smart home robots must accurately detect and identify all family members to perform tasks specific to individual household members. Therefore, this blog focuses on implementing facial recognition and analyzing related aspects.\nFace recognition tasks can be broadly divided into two parts: face detection and face recognition. Face detection refers to locating all faces in an input image and processing them, such as aligning the faces, generating bounding boxes, and resizing or cropping the detected faces. However, due to the potential challenges posed by the robot’s field of vision, such as incomplete or peripheral facial appearances, face detection can become more complex. Additional complications arise from variations in lighting, occlusions, differences in the same individual’s pose or expressions, and inter-individual appearance differences.\nTo address these challenges, accurate facial recognition in complex environments requires precise localization of faces, followed by the detection of facial feature points, such as facial contours, eye outlines, nose, and mouth contours. These feature points are then used to align the faces uniformly, eliminating errors caused by position or pose variations.\nFace recognition can be described as comparing a given photo to a dataset, calculating the facial similarity, and outputting a similarity score or confidence level. Alternatively, it involves identifying which individual in the dataset the photo belongs to and retrieving the relevant information about the individual from the database.\nA representative algorithm in this domain is FaceNet, which won the 2014 ILSVRC (ImageNet Large Scale Visual Recognition Challenge) and reduced the Top-5 error rate for facial recognition to 6.67%. FaceNet uses the GoogLeNet network architecture, as outlined in the associated paper. Following this milestone, facial recognition algorithms have continued to evolve. The latest implementation on GitHub recommends using the MTCNN (Multi-task Cascaded Convolutional Networks) algorithm, which improves detection accuracy to 99.65%.\nTherefore, the primary focus of this blog is the introduction and implementation of the MTCNN and FaceNet network architectures.\nFace Detection MTCNNWhy MTCNN rather than OpenCV?There are several methods for face detection, such as the Haar feature-based classifier provided by OpenCV and the face detection method in dlib. For OpenCV’s face detection method, its advantages lie in being simple and fast; however, its drawbacks include poor detection performance. It works well for frontal, upright faces in good lighting conditions, but fails to detect faces that are turned sideways, tilted, or poorly lit. Therefore, this method is not suitable for real-world applications. In comparison, dlib’s face detection method provides better performance than OpenCV’s, but its detection capability still does not meet the standards required for real-world applications.\nIn this study, we use the MTCNN-based deep learning approach for face detection (MTCNN: Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Neural Networks). The MTCNN face detection method is more robust to variations in lighting, angles, and facial expressions in natural environments, offering better face detection results. Additionally, it has a relatively low memory consumption, enabling real-time face detection. The MTCNN method used in this blog is implemented in Python and TensorFlow.\nHow MTCNN works?Face detection is a critical step in the face recognition process, and the performance of this step significantly impacts the video processing speed and accuracy of face recognition in a robotic system. The method adopted in this paper, MTCNN, integrates both face detection and face alignment to assist in the detection of facial key points. The core of the algorithm involves using three multi-stage deep convolutional neural networks to form a face detection network framework through a cascaded structure.\nFigure 1 illustrates the flowchart of the MTCNN face detection process. Initially, candidate boxes are generated by the P-Net (Proposal Network). P-Net performs the first-level analysis of the image and outputs candidate objects. In the next phase, the R-Net (Refinement Network) further analyzes and refines these candidate objects. In the final stage, the O-Net (Output Network) generates the final bounding boxes and the locations of facial key points.\nThus, the MTCNN architecture can be understood as a cascaded structure involving an image pyramid and the networks P-Net, R-Net, and O-Net, which together contribute to the accurate detection and alignment of faces.\n\nThe MTCNN model consists of three processing stages, which ultimately produce results for both face detection and face alignment. Before using the three cascaded networks, the input image is first processed through an image pyramid, which scales the image at different resolutions. This approach is necessary because faces can appear at various sizes within an image, and scaling the image at multiple resolutions helps detect faces at different scales. This method allows for a rough initial classification of the image, removing non-face regions and identifying face-containing areas. The advantage of this approach is its ability to quickly detect faces of different sizes, enabling multi-scale target detection and recognition.\nBelow is an introduction to the three processing stages of MTCNN face detection, illustrated by Figure 1.\nFirst Stage: P-NetIn the first stage, P-Net generates feature maps through forward propagation, where each location is represented by a 32-dimensional feature vector used to determine whether a face is present in that region. As shown in Figure 2(a), P-Net consists of three 3x3 convolutional layers and one 3x3 max-pooling layer. Its input is a processed 12x12x3 RGB image. The network outputs two parts:\n\nA 32-dimensional feature vector that indicates the probability of a face being present in the region. Since a Softmax layer is used, the final output is the probability of the presence of a face in the image.\nIf a face is detected, a bounding box indicating the face is returned, which constitutes the second part of the output.\n\nThe bounding boxes from P-Net may not always align perfectly with the actual face positions. For example, sometimes the face may be only partially within the bounding box or outside the box. In such cases, the region is adjusted to more accurately reflect the desired area. Once the bounding box with the highest probability of containing a face is identified, Non-Maximum Suppression (NMS) is applied to retain the box with the highest score and remove other less probable boxes.\nThe third part of P-Net’s output is the coordinates of five key facial points: the positions of the left eye, right eye, nose, left corner of the mouth, and right corner of the mouth. Although P-Net provides the key point coordinates, there may still be many false positive face boxes, so the focus at this stage is on selecting the correct face box.\nSecond Stage: R-NetAs shown in Figure 2(b), R-Net consists of two 3x3 convolutional layers, one 2x2 convolutional layer, two 3x3 max-pooling layers, and a 128-dimensional fully connected layer. Since the candidate boxes generated by P-Net are still rough, R-Net is used to further optimize the results. Before inputting the data, the face bounding box coordinates from P-Net undergo a transformation to map the image size to 24x24x3.\nThe output of R-Net is similar to that of P-Net, but its main purpose is to refine the results by removing irrelevant face boxes—non-face boxes or partially incorrect face boxes—and passing the refined results to the third stage (O-Net) for final selection.\nThird Stage: O-NetIn Figure 2(c), O-Net consists of three 3x3 convolutional layers, one 2x2 convolutional layer, two 3x3 and one 2x2 max-pooling layers, and a 256-dimensional fully connected layer. The input to O-Net is the output from R-Net, scaled to an image size of 48x48x3. The output of O-Net includes:\n\nThe coordinates of the final bounding box\nThe confidence score\nThe coordinates of the facial key points\n\nThis final stage provides the precise face detection and alignment results, refining the bounding boxes and facial key points obtained from the previous stages.\n\nLoss FunctionSince MTCNN is a cascaded structure consisting of three sub-networks, each sub-network has its own loss function. In the task of distinguishing between faces and non-faces, which is a binary classification problem, the cross-entropy loss function is used. For bounding box regression and key point localization, the network predicts the offsets between the predicted values and the true values, so the Euclidean distance is used as the loss function for these tasks.\nFinally, the total loss function is obtained by summing the individual losses of the three sub-networks, with each loss weighted according to the goals of each stage. The training focuses of the three sub-networks vary, so different weightings are assigned to each loss function. During the training of P-Net and R-Net, the main focus is on the accuracy of the bounding boxes, so the contribution of the key point loss is relatively small. In contrast, during the training of O-Net, which emphasizes key point selection, the weight of the key point loss is larger compared to the other components.\nThus, the total loss function for MTCNN is a weighted sum of three parts:\n\nThe cross-entropy loss for face vs. non-face classification.\n\nSince face classification is a binary classification problem, the cross-entropy loss function is used. For each sample $x_i$, the loss function is defined as:\n$L_i^{\\text{det}} = - \\left( y_i^{\\text{det}} \\log(P_i) + (1 - y_i^{\\text{det}})(1 - \\log(P_i)) \\right)$\nWhere:\n\n$P_i$ represents the probability output by the network that the sample is a face.\n$y_i^{\\text{det}} \\in {0,1}$ indicates the ground truth label for whether the sample is a face (1) or not (0).\n\n\nThe Euclidean distance loss for bounding box regression.\n\nFor each candidate bounding box, the loss function measures the offset between the predicted values and the ground truth. For each sample $x_i$, the Euclidean distance is used as the loss function, defined as:\n$L_i^{\\text{box}} = | \\hat{y}_i^{\\text{box}} - y_i^{\\text{box}} |_2^2$\nWhere:\n\n$\\hat{y}_i^{\\text{box}}$ is the network’s prediction for the bounding box, which includes four values: the top-left corner coordinates, width, and height of the box.\n$y_i^{\\text{box}}$represents the ground truth for each sample’s bounding box.\n\n\nThe Euclidean distance loss for key point localization.\n\nFor key point localization, a loss function similar to bounding box regression is used. The Euclidean distance measures the offset between the predicted and true key point coordinates, defined as:\n$L_i^{\\text{landmark}} = | \\hat{y}_i^{\\text{landmark}} - y_i^{\\text{landmark}} |_2^2$\nWhere:\n\n$\\hat{y}_i^{\\text{landmark}}$is the network’s prediction for the key points, including 10 values corresponding to the coordinates of the left eye, right eye, nose, left mouth corner, and right mouth corner.\n$y_i^{\\text{landmark}}$ is the ground truth for the facial key points.\n\nEach of these components is weighted appropriately to reflect the emphasis of the corresponding stage in the network.\n\nTotal Loss Function\n\nThe total loss combines the three individual loss functions with different weights, formulated as:\n$L = \\min \\sum_{i=1}^N \\sum_{j \\in {\\text{det}, \\text{box}, \\text{landmark}}} \\alpha_j \\beta_i^j L_i^j$\nWhere:\n\nN is the total number of samples.\n$\\alpha_j$ represents the weight for each task. For P-Net and R-Net, the focus is on face classification $(\\alpha_{\\text{det}} = 1)$ and bounding box regression $(\\alpha_{\\text{box}} = 0.5)$, with less emphasis on key point localization $(alpha_{\\text{landmark}} = 0.5)$. For O-Net, the weights are adjusted to focus more on key point localization $(\\alpha_{\\text{landmark}} = 1)$.\n$\\beta_i^j \\in {0,1}$ indicates whether the sample participates in the loss computation for a specific task. If $\\beta_i^j = 0$, the corresponding loss for that task is not computed.\n\nThe loss is minimized using stochastic gradient descent (SGD), and $L_i^j$ represents the computed loss for sample $x_i$ in task $j$. The total loss function reflects the combined optimization of face classification, bounding box regression, and key point localization.\nFace Recognition FaceNetAfter completing the face detection process, the next step is face recognition. Here, face recognition serves as a general term for face-related tasks, which can be divided into the following four categories:\n\nFace Tracking: Face tracking refers to following an individual within a room after detecting a family member. This is essential for providing targeted services to the individual.\nFace Verification: Face verification determines whether two faces appearing sequentially in the field of view belong to the same family member. This functionality is crucial for enabling directed services by confirming identity consistency.\nFace Identification: Face identification focuses on determining which member of the family (stored in the robot’s facial database) a detected face belongs to. It facilitates member identification. Additionally, when a new face (such as a visiting relative or friend) is detected, it allows the system to add the new face to the database and carry out subsequent actions accordingly.\nFace Clustering: Face clustering enhances the robustness of the robot. By collecting multiple facial images of family members under different angles, lighting conditions, and occlusions in the home environment, the robot can classify and organize these images into distinct clusters for more reliable recognition.\n\nThese functions collectively enable the home service robot to handle a variety of face-related tasks, ensuring it operates effectively and adapts to dynamic household scenarios.\nWhy FaceNet?Before the introduction of FaceNet, traditional face recognition methods based on convolutional neural networks (CNNs) primarily utilized Siamese networks for feature extraction, followed by classification using methods like SVM (Support Vector Machines).\nThe advent of FaceNet marked a significant milestone in face recognition. It achieved an impressive accuracy of 99.63% on the widely used LFW (Labeled Faces in the Wild) dataset, setting a new record at the time. Additionally, it reached 95.12% accuracy on the YouTube Faces database. The system reduced the error rate on these two datasets by 30%.\nThe core idea behind FaceNet is to directly learn the mapping of facial images into a multi-dimensional Euclidean space. The similarity between two facial images is then determined by the distance between their respective feature points in this Euclidean space. As shown in Figure 3, the numerical value between two face images represents the Euclidean distance between their feature points in the multi-dimensional space.\n\nFace recognition models often face challenges due to variations in lighting, pose, and occlusion, which can significantly affect model performance. In Figure 3:\n\nThe left and right images show two photos of the same person from different angles.\nThe top and bottom images represent photos of two different individuals.\n\nFrom the figure, it is evident that the Euclidean distance between images of the same person is less than 1.1, while the distance between images of different individuals exceeds 1.1. In this case, the intra-class distance is clearly smaller than the inter-class distance, allowing a threshold of approximately 1.1 to be set for determining whether two images belong to the same person.\nThis threshold-based approach significantly improves the robustness of face recognition, making it possible to handle variations in lighting, pose, and occlusion more effectively.\nHow FaceNet works?Previous face recognition methods based on convolutional neural networks (CNNs) trained on facial images typically connected a classification layer at the end. For recognizing untrained faces, these methods relied on the bottleneck layer. However, this approach had significant drawbacks, including low efficiency and poor performance of the bottleneck layer when applied to new, unseen face images.\nTo improve the accuracy of face recognition, FaceNet introduced three key advantages:\n\nCompact Feature Representation:FaceNet represents facial images as a 128-dimensional vector. This dramatically reduces the dimensionality of the feature vector after global pooling, thereby significantly reducing the computational cost.\nSimilarity Measurement via Euclidean Distance:The similarity between different face images is measured using the Euclidean distance between their feature vectors. A smaller Euclidean distance indicates higher similarity between the two faces, while a larger distance indicates lower similarity.\nTriplet Loss for Classification:FaceNet replaces the traditional Softmax layer with Triplet Loss, which directly classifies face embeddings. Based on a given threshold, the system determines whether two face images belong to the same person.\n\nFigure 4 illustrates the architecture of FaceNet. The pipeline is as follows:\n\nInput Processing:Facial images detected and cropped to a specified size by MTCNN are used as inputs to the model.\nDeep Learning Architecture:The cropped images are passed into a deep learning architecture, such as GoogleNet or Zeiler&amp;Fergus networks, to extract features.\nFeature Normalization (L2 Normalization):The features output by the deep learning architecture are normalized using L2 normalization, mapping all feature vectors onto a hypersphere.\nFeature Embedding:The normalized feature vectors, called embeddings, represent the input facial image.\nTriplet Loss for Classification:Finally, the model uses Triplet Loss to classify the embeddings and determine whether two facial images represent the same person based on a predefined threshold.\n\nThis approach enables FaceNet to perform face recognition with high efficiency and accuracy, making it robust in handling new facial data.\n\nTriplet Loss FunctionSimilarity via Euclidean DistanceThe similarity between two images is determined by the Euclidean distance between their feature vectors, as shown:$d(x_1, x_2) = | f(x_1) - f(x_2) |_2^2$\nWhere:\n\n$f(x_1) and f(x_2)$ represent the mappings of images (x_1) and (x_2) onto a hypersphere in a (d)-dimensional Euclidean space.\n$d(x_1, x_2)$ denotes the Euclidean distance between the two feature vectors.\n\nPurpose of Triplet LossThe Triplet Loss function calculates the loss for three input images at once. As illustrated in Figure 5, each calculation involves three samples:\n\nAnchor $x_a$: The target image.\nPositive $x_p$: An image of the same class as the Anchor.\nNegative M: An image of a different class from the Anchor.\n\n\nObjectives of Triplet Loss\nMinimize the distance between the Anchor and Positive pair:$d(x_a, x_p)$\nMaximize the distance between the Anchor and Negative pair:$d(x_a, x_n)$\n\nThe Triplet Loss function ensures that:\n\nImages of the same class are pushed closer together in the embedding space.\nImages of different classes are pulled farther apart.\n\nThis design enables the model to effectively distinguish between individuals by producing meaningful embeddings, making Triplet Loss a cornerstone of modern face recognition systems like FaceNet.\nTriplet Loss Function Analysis: Intra-Class and Inter-Class DistanceDue to variations in pose, lighting, and occlusion, different photos of the same person may sometimes result in the Euclidean distance between the Anchor and Positive being greater than the distance between the Anchor and Negative. This is an undesirable situation.\nAs shown in Figure 6, before training, the distance between the Anchor and Negative might be smaller than the distance between the Anchor and Positive. After training, we aim to ensure that the intra-class distance (distance between Anchor and Positive) is significantly smaller than the inter-class distance (distance between Anchor and Negative). This relationship can be expressed mathematically as follows :\n$$| f(x_i^a) - f(x_i^p) |_2^2 + \\alpha &lt; | f(x_i^a) - f(x_i^n) |_2^2, \\quad \\forall (x_i^a, x_i^p, x_i^n) \\in T$$\nWhere:\n\n$T$ is the set of all triplet combinations.\n$x_i^a$, $x_i^p$, and $x_i^n$ represent the Anchor, Positive, and Negative samples, respectively, within the set $T$.\n$\\alpha$ is the margin between the intra-class and inter-class distances.\n\n\nFinal Triplet Loss FunctionTo ensure that all triplet combinations satisfy the above constraint, Equation can be transformed into the final Triplet Loss Function, as shown:\n$$L = \\sum_{i=1}^N \\left[ | f(x_i^a) - f(x_i^p) |_2^2 - | f(x_i^a) - f(x_i^n) |_2^2 \\right]+ \\alpha$$\nWhere:\n\n$N$ is the total number of triplets.\n\nExplanationThe Triplet Loss function:\n\nPenalizes cases where the intra-class distance ($d(x_a, x_p)$) is not sufficiently smaller than the inter-class distance ($d(x_a, x_n)$).\nEnsures that the model learns embeddings where samples of the same class are closer together and samples of different classes are farther apart, with a margin of at least $\\alpha$.\n\nThis formulation helps improve the robustness and accuracy of the model in face recognition tasks.\nEfficient Triplet Selection During TrainingDuring the training phase, it is necessary to continuously search for triplet combinations to compute the loss. However, in large datasets, exhaustively enumerating all triplets and calculating the triplet loss requires substantial computational resources. Additionally, many easily distinguishable images do not contribute meaningfully to the model’s convergence.\nTo address this problem, the authors of Schroff F, Kalenichenko D, Philbin J. Facenet: A unified embedding for face recognition and clustering. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015: 815-823 proposed an efficient solution:\n\nFor a given image, identify the Hard Positive:The least similar image belonging to the same person (i.e., the hardest to recognize as the same person).\n\nIdentify the Hard Negative:The most similar image belonging to a different person (i.e., the hardest to distinguish as a different person).\n\n\nBenefits of Hard Example MiningBy focusing on Hard Positives and Hard Negatives, the training process:\n\nReduces computational overhead by avoiding redundant comparisons with easily distinguishable images.\nEnhances the model’s ability to handle challenging cases, improving its robustness and convergence efficiency.\n\nThis method ensures that the triplet loss focuses on the most informative and challenging samples, driving the model to better distinguish between subtle intra-class variations and inter-class similarities.\nResultIn Figure 4, the term DEEP ARCHITECTURE refers to the deep learning framework. In the referenced paper, two neural network architectures were primarily used: Zeiler&amp;Fergus  and GoogleNet (Inception).\nFace Database: LFW and Recognition Results A well-known face database LFW (Labeled Faces in the Wild) will be tested in this part. The paper  provides recognition rates for various network architectures and dimensions, as shown in Table 1:\nTable 1. Recognition Accuracy for Different Network Configurations\n\n\nArchitecture\nInput Dimension\nRecognition Accuracy (VAL)\n\n\n\nNN1 (Zeiler&amp;Fergus)\n220×220\n$87.9% \\pm 1.9$\n\n\nNN2 (Inception)\n224×224\n$89.4% \\pm 1.6$\n\n\nNN3 (Inception)\n160×160\n$88.3% \\pm 1.7$\n\n\nNN4 (Inception)\n96×96\n$82.0% \\pm 2.3$\n\n\nNNS1 (Mini Inception)\n—\n$82.4% \\pm 2.4$\n\n\nNNS2 (Tiny Inception)\n—\n$51.9% \\pm 2.9$\n\n\n\nPreprocessing with MTCNN and Training ConfigurationIn this study:\n\nPreprocessing:The images in the LFW dataset were first processed using the MTCNN model. The output consisted of 160×160 cropped facial images, which were used as inputs for training the FaceNet model.\n\nUpdates to GoogleNet:Over time, Google continued improving the GoogleNet architecture. This study primarily used Inception V4 to process the dataset.\n\nTraining Parameters:  \n\nLearning rate: $0.1$  \nExponential decay rate: $0.999$  \nEpochs: $400$\n\n\n\nKey Takeaways\n\nLarger input dimensions (e.g., 224×224 vs. 96×96) and more advanced architectures (e.g., Inception vs. Mini Inception) resulted in higher recognition accuracy. \nProcessing the dataset with MTCNN ensured high-quality cropped facial images, which improved the performance of the FaceNet model.  \nInception V4 and well-tuned hyperparameters significantly enhanced the model’s robustness and accuracy on the LFW dataset.\n\nThe following screenshot will be the result or output of the model.\n\nFrom Figure 7, it can be observed that MTCNN successfully detected faces in all 13,233 images of the LFW dataset. The detected faces were then scaled and cropped to the required dimensions for subsequent processing.\n\nFigure 8 summarizes the performance of FaceNet on the LFW dataset, with the following key metrics:\n\nAccuracy: $98.50%$\nValidation Rate: $90.06%$\nAUC (Area Under the Curve): $0.998$\nEER (Equal Error Rate): $0.016$\n\nInsights:\nAn AUC value close to $1$ indicates excellent model performance.\nA low EER of $0.016$ further confirms that FaceNet effectively meets the requirements of face recognition tasks.\n\n\nFigure 9 presents a comparison of non-database facial images:\n\nImages 0.jpg and 1.jpg: Two images of the same person taken from different angles.\nImage 11.jpg: An image of a different individual.\n\nAll the images are of Asian faces. The results demonstrate:\n\nIntra-class distances (same person) are consistently smaller than $0.8$.\nInter-class distances (different people) are significantly larger than $1.0$.\n\nConclusion:These results show that FaceNet performs well even on Asian faces, maintaining clear separations between intra-class and inter-class distances. This indicates that the model is robust across diverse facial datasets.\nSome other results:\n\n\n","tags":["Robotics","HCI","CNN"]}]