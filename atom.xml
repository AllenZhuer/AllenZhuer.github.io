<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2025-05-05T13:41:38.813Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>RAG与LoRA</title>
    <link href="http://example.com/2025/05/05/RAG%E4%B8%8ELoRA/"/>
    <id>http://example.com/2025/05/05/RAG%E4%B8%8ELoRA/</id>
    <published>2025-05-05T13:33:21.000Z</published>
    <updated>2025-05-05T13:41:38.813Z</updated>
    
    <content type="html"><![CDATA[<h1 id="RAG-与-LoRA-技术详解"><a href="#RAG-与-LoRA-技术详解" class="headerlink" title="RAG 与 LoRA 技术详解"></a>RAG 与 LoRA 技术详解</h1><p>本文档总结了 RAG (Retrieval-Augmented Generation) 和 LoRA (Low-Rank Adaptation) 两种在大型语言模型 (LLM) 领域常用的技术。</p><h2 id="RAG-Retrieval-Augmented-Generation-检索增强生成"><a href="#RAG-Retrieval-Augmented-Generation-检索增强生成" class="headerlink" title="RAG (Retrieval-Augmented Generation) - 检索增强生成"></a>RAG (Retrieval-Augmented Generation) - 检索增强生成</h2><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>RAG 结合了<strong>信息检索</strong>和<strong>自然语言生成</strong>，在 LLM 生成响应前，先从外部知识库检索相关信息，并将这些信息作为上下文提供给 LLM，以生成更准确、更可靠的回答。</p><h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>缓解 LLM 面临的挑战：</p><ul><li><strong>知识静态性:</strong> 无法获取最新信息。</li><li><strong>幻觉:</strong> 编造错误信息。</li><li><strong>领域知识缺乏:</strong> 专业知识不足。</li><li><strong>无法访问私有/实时数据。</strong></li><li><strong>缺乏可解释性:</strong> 难以溯源信息来源。</li><li><strong>更新成本高:</strong> 重新训练 LLM 成本高昂。</li></ul><h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><ol><li> <strong>输入 (Input):</strong> 用户提问或提示。</li><li><strong>检索 (Retrieval):</strong><ul><li>  查询编码 (Query Encoding): 将输入转为向量。</li><li>  相似性搜索 (Similarity Search): 在向量数据库中搜索相似的文本块。</li><li>  获取相关文本 (Retrieve Relevant Chunks): 得到 Top-K 相关文本块。</li></ul></li><li><strong>增强 (Augmentation):</strong><ul><li>  构建上下文提示 (Contextual Prompt Construction): 将检索到的文本与原始查询组合成新提示。</li></ul></li><li><strong>生成 (Generation):</strong><ul><li>  LLM 推理 (LLM Inference): 将增强提示输入 LLM。</li><li>  生成响应 (Generate Response): LLM 基于内部知识和外部上下文生成回答。</li></ul></li></ol><h3 id="关键组成部分"><a href="#关键组成部分" class="headerlink" title="关键组成部分"></a>关键组成部分</h3><ul><li>  <strong>检索器 (Retriever):</strong> 编码器、知识库（向量数据库）、索引。</li><li>  <strong>生成器 (Generator):</strong> 大型语言模型 (LLM)。</li><li>  <strong>知识库构建流程 (Indexing Pipeline):</strong> 数据加载、文本切分、向量化、索引存储。</li></ul><h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><ul><li>  <strong>知识实时性:</strong> 可通过更新知识库获取最新信息。</li><li>  <strong>减少幻觉:</strong> 提供事实依据。</li><li>  <strong>提高准确性与相关性:</strong> 基于相关上下文生成。</li><li>  <strong>领域适应性:</strong> 易于适应特定领域。</li><li>  <strong>可解释性与可溯源性:</strong> 可追踪信息来源。</li><li>  <strong>成本效益:</strong> 更新知识库比重训 LLM 便宜。</li><li>  <strong>数据隐私:</strong> 私有数据可存于外部库。</li></ul><h3 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h3><ul><li>  <strong>检索质量:</strong> 检索效果是瓶颈。</li><li>  <strong>文本块切分策略:</strong> 影响上下文和精度。</li><li>  <strong>检索与生成的平衡。</strong></li><li>  <strong>处理冲突信息。</strong></li><li>  <strong>系统复杂性。</strong></li><li>  <strong>延迟:</strong> 检索增加响应时间。</li><li>  <strong>成本:</strong> 向量数据库和检索仍有成本。</li></ul><hr><h2 id="LoRA-Low-Rank-Adaptation-低秩适应"><a href="#LoRA-Low-Rank-Adaptation-低秩适应" class="headerlink" title="LoRA (Low-Rank Adaptation) - 低秩适应"></a>LoRA (Low-Rank Adaptation) - 低秩适应</h2><h3 id="背景与动机"><a href="#背景与动机" class="headerlink" title="背景与动机"></a>背景与动机</h3><p>解决大型模型<strong>完全微调 (Full Fine-tuning)</strong> 带来的问题：</p><ul><li>计算资源消耗大 (显存、算力)。</li><li>存储成本高 (每个任务一个完整副本)。</li><li>训练时间长。</li></ul><p>LoRA 是一种<strong>参数高效微调 (Parameter-Efficient Fine-Tuning, PEFT)</strong> 技术。</p><h3 id="核心思想-1"><a href="#核心思想-1" class="headerlink" title="核心思想"></a>核心思想</h3><p>假设模型权重在适应新任务时的<strong>变化量 ($\Delta W$)</strong> 是<strong>低秩 (Low-Rank)</strong> 的。因此，可以通过学习两个小的低秩矩阵 $A$ 和 $B$ (其中 $\Delta W = BA$) 来表示这个变化量，而无需更新全部原始权重 $W_0$。</p><h3 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h3><ul><li>  冻结原始模型权重 $W_0$。</li><li>  在选定的层（如注意力 QKV 矩阵）旁边添加低秩适配器矩阵 $A$ ($r \times k$) 和 $B$ ($d \times r$)，其中 $r$ 是秩 ($r \ll \min(d, k)$)。</li><li>  只训练 $A$ 和 $B$ 的参数。</li><li>  前向传播时计算：$h = W_0 x + \alpha B A x$。</li></ul><h3 id="关键组成部分与超参数"><a href="#关键组成部分与超参数" class="headerlink" title="关键组成部分与超参数"></a>关键组成部分与超参数</h3><ul><li>  <strong>适配器矩阵 A 和 B:</strong> 新增的可训练参数。</li><li>  <strong>秩 (Rank, r):</strong> 控制适配器大小的关键超参数 (如 4, 8, 16, …)。</li><li>  <strong>缩放因子 ($\alpha$):</strong> 控制适配器影响程度。</li><li>  <strong>目标模块 (Target Modules):</strong> 选择应用 LoRA 的层。</li></ul><h3 id="优势-1"><a href="#优势-1" class="headerlink" title="优势"></a>优势</h3><ul><li>  <strong>极高参数效率:</strong> 显著降低显存需求。</li><li>  <strong>减少存储:</strong> 只需存储小型适配器权重。</li><li>  <strong>可能更快的训练:</strong> 低显存允许更大批量。</li><li>  <strong>无额外推理延迟:</strong> 训练后可将 $BA$ 合并回 $W_0$ ($W = W_0 + BA$)，推理时无额外计算。</li><li>  <strong>可媲美的性能:</strong> 效果接近完全微调。</li><li>  <strong>便捷的任务切换:</strong> 易于加载不同适配器。</li></ul><h3 id="挑战-1"><a href="#挑战-1" class="headerlink" title="挑战"></a>挑战</h3><ul><li>  <strong>超参数选择:</strong> $r$, $\alpha$, 目标模块需要调优。</li><li>  <strong>并非万能:</strong> 对某些任务可能不如完全微调。</li><li>  <strong>合并操作:</strong> 需要额外步骤来消除推理延迟。</li></ul><h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><ul><li>  LLM 下游任务微调。</li><li>  模型风格、领域知识适应。</li><li>  文生图模型（Stable Diffusion）的风格/人物学习。</li><li>  高效实现对齐技术（如 SFT）。 </li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;RAG-与-LoRA-技术详解&quot;&gt;&lt;a href=&quot;#RAG-与-LoRA-技术详解&quot; class=&quot;headerlink&quot; title=&quot;RAG 与 LoRA 技术详解&quot;&gt;&lt;/a&gt;RAG 与 LoRA 技术详解&lt;/h1&gt;&lt;p&gt;本文档总结了 RAG (Retrie</summary>
      
    
    
    
    
    <category term="RAG" scheme="http://example.com/tags/RAG/"/>
    
    <category term="LoRA" scheme="http://example.com/tags/LoRA/"/>
    
  </entry>
  
  <entry>
    <title>提示词工程 Prompt Engineering</title>
    <link href="http://example.com/2025/05/05/%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B-Prompt-Engineering/"/>
    <id>http://example.com/2025/05/05/%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B-Prompt-Engineering/</id>
    <published>2025-05-05T13:03:59.000Z</published>
    <updated>2025-05-06T01:58:44.878Z</updated>
    
    <content type="html"><![CDATA[<h1 id="提示词工程（Prompt-Engineering）学习笔记"><a href="#提示词工程（Prompt-Engineering）学习笔记" class="headerlink" title="提示词工程（Prompt Engineering）学习笔记"></a>提示词工程（Prompt Engineering）学习笔记</h1><hr><p><strong>目录</strong></p><ul><li><a href="#%E5%BC%95%E8%A8%80">引言</a></li><li><a href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%BE%88%E9%87%8D%E8%A6%81">为什么提示工程很重要？</a></li><li><a href="#%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E7%9A%84%E6%A0%B8%E5%BF%83%E5%8E%9F%E5%88%99">提示工程的核心原则</a></li><li><a href="#llm%E5%8F%82%E6%95%B0%E4%BB%8B%E7%BB%8D">LLM参数介绍</a></li><li><a href="#%E9%AB%98%E7%BA%A7%E6%8F%90%E7%A4%BA%E6%8A%80%E5%B7%A7">高级提示技巧</a><ul><li><a href="#%E6%80%9D%E7%BB%B4%E9%93%BEchain-of-thought%E5%BC%95%E5%AF%BC-ai-%E9%80%90%E6%AD%A5%E6%80%9D%E8%80%83%E9%97%AE%E9%A2%98">思维链（Chain of Thought）</a></li><li><a href="#%E8%A7%92%E8%89%B2%E6%89%AE%E6%BC%94role-playing%E8%B5%8B%E4%BA%88-ai-%E7%89%B9%E5%AE%9A%E8%BA%AB%E4%BB%BD%E5%92%8C%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86">角色扮演（Role Playing）</a></li><li><a href="#%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8B%86%E8%A7%A3prompt-decomposition%E5%B0%86%E5%A4%8D%E6%9D%82%E4%BB%BB%E5%8A%A1%E5%88%86%E8%A7%A3%E4%B8%BA%E7%AE%80%E5%8D%95%E6%AD%A5%E9%AA%A4">提示词拆解（Prompt Decomposition）</a></li><li><a href="#%E7%BA%A6%E6%9D%9F%E6%8C%87%E4%BB%A4constraint-instructions%E8%AE%BE%E5%AE%9A%E5%9B%9E%E7%AD%94%E7%9A%84%E8%BE%B9%E7%95%8C%E5%92%8C%E9%99%90%E5%88%B6">约束指令（Constraint Instructions）</a></li><li><a href="#%E5%8F%8D%E4%BE%8B%E6%95%99%E5%AD%A6learning-from-mistakes%E9%80%9A%E8%BF%87%E5%8F%8D%E9%9D%A2%E4%BE%8B%E5%AD%90%E6%8C%87%E5%AF%BC-ai">反例教学（Learning from Mistakes）</a></li><li><a href="#%E5%B7%A5%E5%85%B7%E8%B0%83%E7%94%A8tool-calling%E5%B0%86%E5%A4%96%E9%83%A8%E5%B7%A5%E5%85%B7%E9%9B%86%E6%88%90%E5%88%B0%E6%8F%90%E7%A4%BA%E4%B8%AD">工具调用（Tool Calling）</a></li></ul></li><li><a href="#%E8%A7%84%E8%8C%83%E6%A0%BC%E5%BC%8F">规范格式</a></li><li><a href="#%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%B5%81%E7%A8%8B-openai41-workflow">提示词流程（OpenAI4.1 Workflow）</a></li><li><a href="#%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%A8%A1%E7%89%88">提示词模版</a></li><li><a href="#langgpt%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%A8%A1%E7%89%88">LangGPT提示词模版</a></li><li><a href="#%E6%A1%88%E4%BE%8B1%E5%AE%A2%E6%9C%8D">案例1：客服</a></li></ul><hr><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>嗨，这是我学习提示工程（Prompt Engineering）后整理的笔记，希望能帮助刚接触这个领域的朋友。</p><blockquote><p>本文涵盖提示工程的理论基础与实践技巧，教你如何用最佳方法与大语言模型（LLM）交互和构建应用。</p></blockquote><p><strong>什么是提示工程？</strong></p><p>提示工程（Prompt Engineering）是设计和优化与AI大语言模型（如ChatGPT）交流”指令”的技术。它是一门新兴学科，专注于开发和优化提示词，以便在各种应用场景中高效地使用和构建大语言模型（LLMs）。</p><p>掌握这项技能不仅能让我们理解LLM的能力和局限，更能帮助我们设计出更安全、更强大的AI应用。</p><h3 id="为什么提示工程很重要？"><a href="#为什么提示工程很重要？" class="headerlink" title="为什么提示工程很重要？"></a>为什么提示工程很重要？</h3><ul><li><strong>提高效率</strong>：良好的提示可以节省时间，减少来回沟通的次数</li><li><strong>提升质量</strong>：精心设计的提示能获得更准确、更有用的回答</li><li><strong>拓展能力</strong>：通过高级提示技巧，可以突破 AI 表面上的限制</li><li><strong>个性化体验</strong>：根据特定需求定制 AI 行为和输出风格</li></ul><h3 id="提示工程的核心原则"><a href="#提示工程的核心原则" class="headerlink" title="提示工程的核心原则"></a>提示工程的核心原则</h3><ol><li><strong>明确性</strong>：提供清晰、无歧义的指示</li><li><strong>具体性</strong>：详细说明您需要的内容、格式和标准</li><li><strong>上下文提供</strong>：给予充分的背景信息</li><li><strong>示例展示</strong>：通过示例说明您期望的输出</li><li><strong>角色设定</strong>：为 AI 分配特定角色或专业背景</li></ol><p>我发现掌握提示工程就像学习一门新语言——只有理解了”语法规则”，才能准确表达我们的需求，获得满意的回应。在这份笔记中，我会分享提示工程的核心原则、常用模板、高级技巧和实际应用案例，帮助你也能写出高质量的提示，充分发挥AI的潜力。</p><p>补充：<br>我在学习过程中，所有示例都是用OpenAI的Playground或者Claude 3.5 Sonnet模型。这些提示方法也适用于其他类似能力的模型，但不同模型的响应可能会有所差异。</p><h2 id="LLM参数介绍"><a href="#LLM参数介绍" class="headerlink" title="LLM参数介绍"></a>LLM参数介绍</h2><p>在设计和测试提示时，我们通常通过API与大语言模型进行交互。通过调整几个关键参数，可以为同一个提示获得不同的结果，调整这些设置对于提高响应的可靠性和符合预期非常重要。以下是使用各种LLM提供商时会遇到的常见设置：</p><ol><li><p>Temperature（温度）<br> 简单来说，温度数值越低，结果越确定，模型总是选择概率最高的下一个词。低温度更加适合进行基于事实的查询，因为这将更加精确和简洁；而高温度本质上是增加了其他词的权重，更加适合更有创意但是可能不是很准确的情况，比如头脑风暴，创意写作。</p></li><li><p>Top P（核采样）<br> 核采样是一种与温度一起使用的采样技术，旨在控制模型的确定性。使用Top P意味着只考虑构成top_p概率质量的词进行响应，如果需要准确和事实性的答案，保持较低的值；如果需要更多样化的回答，可以设置更高的值。</p></li></ol><p>  一般建议是调整Temperature或Top P，但不要同时调整两者。</p><ol start="3"><li><p>Max Length（最大长度）<br> 通过调整最大长度可以管理模型生成的词元（token）数量。指定最大长度有助于防止冗长或不相关的回答，并控制成本。</p></li><li><p>Stop Sequences（停止序列）<br> 停止序列是一个能够使模型停止生成词元的字符串。指定停止序列是控制模型回答长度和结构的另一种方式。例如，可以通过添加”11”作为停止序列，告诉模型生成不超过10项的列表。</p></li><li><p>Frequency Penalty（频率惩罚）<br> 频率惩罚会对下一个词元施加与该词元在回答和提示中已出现次数成正比的惩罚。频率惩罚越高，一个词再次出现的可能性就越低。这个设置通过给出现更多的词元更高的惩罚来减少模型回答中词语的重复。</p></li><li><p>Presence Penalty（存在惩罚）<br> 存在惩罚也会对重复的词元施加惩罚，但与频率惩罚不同，所有重复词元的惩罚都是相同的。出现两次的词元和出现10次的词元受到的惩罚是一样的。这个设置可以防止模型在回答中过于频繁地重复短语。如果希望模型生成多样化或创意性的文本，可能需要使用较高的存在惩罚；或者，如果需要模型保持专注，可以尝试使用较低的存在惩罚。</p></li></ol><p>  与温度和top_p类似，一般建议是调整频率或存在惩罚，但不要同时调整两者。</p><p>让我们看一下具体的代码实例，下面是一个使用 OpenAI API 的 Python 代码示例，展示了如何在实际调用中设置这些参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_completion</span>(<span class="params">params, messages</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; GET completion from openai api&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    response = openai.chat.completions.create(</span><br><span class="line">        model = params[<span class="string">&#x27;model&#x27;</span>],              <span class="comment"># 选择使用的模型，如 gpt-3.5-turbo</span></span><br><span class="line">        messages = messages,                  <span class="comment"># 消息列表，包含对话历史</span></span><br><span class="line">        temperature = params[<span class="string">&#x27;temperature&#x27;</span>],  <span class="comment"># 温度参数，控制创造性</span></span><br><span class="line">        max_tokens = params[<span class="string">&#x27;max_tokens&#x27;</span>],    <span class="comment"># 最大生成长度</span></span><br><span class="line">        top_p = params[<span class="string">&#x27;top_p&#x27;</span>],              <span class="comment"># 核采样参数</span></span><br><span class="line">        frequency_penalty = params[<span class="string">&#x27;frequency_penalty&#x27;</span>],  <span class="comment"># 频率惩罚参数</span></span><br><span class="line">        presence_penalty = params[<span class="string">&#x27;presence_penalty&#x27;</span>],    <span class="comment"># 存在惩罚参数</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> response</span><br></pre></td></tr></table></figure><p>代码解析：</p><ul><li>这个函数封装了与 OpenAI API 的交互，接收参数字典和消息列表</li><li>所有我们前面讨论的参数都在这里体现：<code>temperature</code>、<code>max_tokens</code>、<code>top_p</code>、<code>frequency_penalty</code> 和 <code>presence_penalty</code></li><li>参数存储在字典中，方便灵活调整不同场景下的配置</li><li>函数返回完整的响应对象，包含生成的文本和其他元数据</li></ul><p>这种设计使我们可以方便地为不同任务创建不同的参数配置。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 适合事实性查询的参数配置</span></span><br><span class="line">fact_params = &#123;</span><br><span class="line">    <span class="string">&#x27;model&#x27;</span>: <span class="string">&#x27;gpt-3.5-turbo&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;temperature&#x27;</span>: <span class="number">0.1</span>,  <span class="comment"># 低温度，更确定性的回答</span></span><br><span class="line">    <span class="string">&#x27;max_tokens&#x27;</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="string">&#x27;top_p&#x27;</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">&#x27;frequency_penalty&#x27;</span>: <span class="number">0.0</span>,</span><br><span class="line">    <span class="string">&#x27;presence_penalty&#x27;</span>: <span class="number">0.0</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 适合创意写作的参数配置</span></span><br><span class="line">creative_params = &#123;</span><br><span class="line">    <span class="string">&#x27;model&#x27;</span>: <span class="string">&#x27;gpt-3.5-turbo&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;temperature&#x27;</span>: <span class="number">0.8</span>,  <span class="comment"># 高温度，更多样化的回答</span></span><br><span class="line">    <span class="string">&#x27;max_tokens&#x27;</span>: <span class="number">1000</span>,  <span class="comment"># 允许更长的回答</span></span><br><span class="line">    <span class="string">&#x27;top_p&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&#x27;frequency_penalty&#x27;</span>: <span class="number">0.5</span>,  <span class="comment"># 减少重复</span></span><br><span class="line">    <span class="string">&#x27;presence_penalty&#x27;</span>: <span class="number">0.5</span>    <span class="comment"># 鼓励新颖内容</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过这种方式，我们可以根据任务需求轻松切换不同的参数配置，获得最适合的模型输出。</p><h2 id="高级提示技巧"><a href="#高级提示技巧" class="headerlink" title="高级提示技巧"></a>高级提示技巧</h2><h3 id="思维链（Chain-of-Thought）：引导-AI-逐步思考问题"><a href="#思维链（Chain-of-Thought）：引导-AI-逐步思考问题" class="headerlink" title="思维链（Chain of Thought）：引导 AI 逐步思考问题"></a><strong>思维链（Chain of Thought）</strong>：引导 AI 逐步思考问题</h3><p>思维链是一种强大的提示技巧，通过引导AI展示其推理过程，而不仅仅是直接给出答案。简单来说，就是让AI”一步步思考”，像人类解决问题那样展示完整的思考路径。<br>思维链之所以有效，是因为：</p><ol><li>**减少”跳跃性思维”**：当AI直接给出答案时，往往会跳过中间推理步骤，可能导致错误</li><li><strong>提高复杂推理能力</strong>：特别适合数学问题、逻辑推理和多步骤分析任务</li><li><strong>增强可解释性</strong>：让我们看到AI是如何得出结论的，增强透明度</li><li><strong>降低幻觉风险</strong>：逐步推理过程中更容易发现逻辑矛盾或事实错误<br>实现思维链主要有两种方式：</li><li><strong>零样本思维链</strong>：直接在提示中加入”让我们一步一步思考”这样的指令：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">问题：如果一件T恤原价80元，现在打7折，同时我有一张15元优惠券，最终需要支付多少钱？</span><br><span class="line">让我们一步一步思考。</span><br></pre></td></tr></table></figure></li><li><strong>少样本思维链</strong>：通过举例展示你期望的思考过程：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">问题：小明有5个苹果，小红有3个苹果，他们一共有多少个苹果？</span><br><span class="line">思考：小明有5个苹果，小红有3个苹果。要算出总数，我需要将两人的苹果数相加：5 + 3 = 8。所以他们一共有8个苹果。</span><br><span class="line"></span><br><span class="line">问题：如果一件T恤原价80元，现在打7折，同时我有一张15元优惠券，最终需要支付多少钱？</span><br><span class="line">思考：</span><br></pre></td></tr></table></figure></li></ol><p>同时使用思维链还可以要求AI进行：</p><ol><li>自我评估：让AI评价自己的思考过程，比如请解答这个问题，然后评估你的解答是否正确：’问题’</li><li>多路径探索：鼓励AI考虑多种解决方案，比如请用两种不同的方法解决这个问题，并比较它们：’问题’</li><li>特定领域的思维链：针对特定领域设计思考框架，比如在Marketing领域，可以要求AI按照5C和4P得框架进行思考：’公司’<br>但是思维链也具有局限性，比如：</li></ol><ul><li>不是所有问题都需要思维链，简单问题可能适得其反</li><li>可能会产生更长的回答，消耗更多token</li><li>对于极其复杂的问题，可能需要结合其他技巧（如问题拆解）</li></ul><h3 id="角色扮演（Role-Playing）：赋予-AI-特定身份和专业知识"><a href="#角色扮演（Role-Playing）：赋予-AI-特定身份和专业知识" class="headerlink" title="角色扮演（Role Playing）：赋予 AI 特定身份和专业知识"></a><strong>角色扮演（Role Playing）</strong>：赋予 AI 特定身份和专业知识</h3><p>角色扮演是一种非常有效的提示技巧，通过给AI分配特定的专业身份或角色，引导它从特定的专业视角回答问题。这种方法能够激活模型中与特定领域相关的知识，产生更专业、更深入的回答。<br>角色扮演之所以有效，有几个关键原因：</p><ol><li><strong>激活领域知识</strong>：AI模型在训练时学习了不同专业人士的写作和思考方式，指定角色能够激活这些特定知识</li><li><strong>提供思考框架</strong>：不同专业有不同的分析框架和方法论，角色设定为AI提供了特定的思考路径</li><li><strong>增强回答深度</strong>：专业角色通常会考虑更多领域内的细节和边界情况</li><li><strong>改变语言风格</strong>：不同角色会使用不同的术语、表达方式和沟通风格<br>实现有效的角色扮演需要注意以下几点：</li><li><strong>选择具体角色</strong>：越具体越好，”医生”不如”心脏科专家”；”程序员”不如”有10年经验的Python后端工程师”</li><li><strong>提供角色背景</strong>：补充角色的资历、专长和经验，例如：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">你是一位有20年经验的iOS开发工程师，曾参与多个百万用户级应用的开发，精通Swift和Objective-C，对性能优化和用户体验设计有独到见解。</span><br></pre></td></tr></table></figure></li><li><strong>设定回答风格</strong>：指定角色应该用什么风格或方式回答：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">请用简明专业的语言回答，适当使用技术术语，但确保非专业人士也能理解核心概念。</span><br></pre></td></tr></table></figure></li><li><strong>多角色对比</strong>：让AI从多个角色视角分析同一问题：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">请从投资者、用户和开发者三个视角分析这个产品创意的优缺点。</span><br></pre></td></tr></table></figure></li></ol><p>角色扮演模版</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[角色]: 你是一名[专业领域]专家</span><br><span class="line">[任务]: 我需要你帮我[具体任务]</span><br><span class="line">[格式]: 请以[特定格式]回答</span><br><span class="line">[限制/要求]: 回答需要[额外要求]</span><br></pre></td></tr></table></figure><p>专家咨询模板：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我想请教你关于[主题]的问题。请以[专业领域]专家的身份，考虑[关键因素]，并提供[具体需求]的建议。</span><br></pre></td></tr></table></figure><h3 id="提示词拆解（Prompt-Decomposition）：将复杂任务分解为简单步骤"><a href="#提示词拆解（Prompt-Decomposition）：将复杂任务分解为简单步骤" class="headerlink" title="提示词拆解（Prompt Decomposition）：将复杂任务分解为简单步骤"></a><strong>提示词拆解（Prompt Decomposition）</strong>：将复杂任务分解为简单步骤</h3><p>提示词拆解是一种处理复杂问题的强大技巧，通过将大型、复杂的任务分解成一系列较小、较简单的子任务，使AI能够逐步解决问题，最终得到更准确的结果。这种方法类似于人类解决复杂问题的思路——将庞大的挑战分解为可管理的部分。</p><h4 id="为什么提示词拆解有效？"><a href="#为什么提示词拆解有效？" class="headerlink" title="为什么提示词拆解有效？"></a>为什么提示词拆解有效？</h4><p>提示词拆解之所以有效，有几个关键原因：</p><ol><li><strong>减轻认知负担</strong>：即使对于先进的AI模型，一次处理大量信息也会导致遗漏或错误。分解后，每个子任务都更加聚焦。</li><li><strong>提高精确度</strong>：当任务被分解时，每个子步骤的错误率降低，整体结果更可靠。</li><li><strong>便于调试</strong>：如果最终结果有问题，更容易识别是哪个子步骤出了错。</li><li><strong>更好地利用上下文窗口</strong>：避免因模型上下文限制而丢失关键信息。</li><li><strong>结构化思考</strong>：促使AI按照逻辑顺序处理信息，减少遗漏或重复。</li></ol><h4 id="如何实现有效的提示词拆解？"><a href="#如何实现有效的提示词拆解？" class="headerlink" title="如何实现有效的提示词拆解？"></a>如何实现有效的提示词拆解？</h4><p>实现提示词拆解主要有两种方法：</p><ol><li><p><strong>单次提示中的任务分解</strong>：<br>在一个提示中明确列出解决问题的步骤，要求AI按顺序执行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">请帮我分析这家公司的财务状况。请按以下步骤进行：</span><br><span class="line">1. 首先分析收入增长趋势</span><br><span class="line">2. 然后评估利润率变化</span><br><span class="line">3. 接着分析债务状况</span><br><span class="line">4. 最后给出整体财务健康评估</span><br></pre></td></tr></table></figure></li><li><p><strong>多轮交互的任务分解</strong>：<br>将复杂任务拆分为多次独立的AI交互，每次只解决一个子问题：</p><ul><li>第一轮：分析收入增长</li><li>第二轮：将第一轮结果提供给AI，要求分析利润率</li><li>第三轮：结合前两轮结果，分析债务状况</li><li>最后一轮：整合所有分析，得出综合评估</li></ul></li></ol><h4 id="高级提示词拆解策略"><a href="#高级提示词拆解策略" class="headerlink" title="高级提示词拆解策略"></a>高级提示词拆解策略</h4><ol><li><p><strong>问题分类拆解</strong>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在回答这个问题前，请先确定这是一个事实性问题还是意见性问题。</span><br><span class="line">如果是事实性问题，请提供准确信息和来源。</span><br><span class="line">如果是意见性问题，请分析不同立场并给出平衡的观点。</span><br></pre></td></tr></table></figure></li><li><p><strong>复杂决策拆解</strong>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">帮我决定是否应该投资这家初创公司。请按以下步骤进行：</span><br><span class="line">1. 列出投资的所有潜在好处（至少3点）</span><br><span class="line">2. 列出所有可能的风险（至少3点）</span><br><span class="line">3. 分析市场状况和竞争格局</span><br><span class="line">4. 评估创始团队的优势和劣势</span><br><span class="line">5. 综合以上因素，提出最终建议</span><br></pre></td></tr></table></figure></li><li><p><strong>MECE原则应用</strong>（Mutually Exclusive, Collectively Exhaustive，相互独立，完全穷尽）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">请对这个商业问题进行MECE分析。首先按以下类别分解问题：</span><br><span class="line">1. 市场因素</span><br><span class="line">2. 竞争因素</span><br><span class="line">3. 内部运营因素</span><br><span class="line">4. 财务因素</span><br><span class="line"></span><br><span class="line">然后在每个类别下，确定相关的子因素，确保没有遗漏和重叠。</span><br></pre></td></tr></table></figure></li></ol><p>我发现提示词拆解特别适合以下场景：</p><ol><li><strong>复杂分析任务</strong>：市场研究、竞争分析、文献综述</li><li><strong>多步骤创意过程</strong>：产品设计、内容策划、营销活动</li><li><strong>综合评估问题</strong>：人才评估、项目评审、投资决策</li><li><strong>教学和引导</strong>：引导学生逐步学习复杂概念</li></ol><h3 id="约束指令（Constraint-Instructions）：设定回答的边界和限制"><a href="#约束指令（Constraint-Instructions）：设定回答的边界和限制" class="headerlink" title="约束指令（Constraint Instructions）：设定回答的边界和限制"></a><strong>约束指令（Constraint Instructions）</strong>：设定回答的边界和限制</h3><p>约束指令是一种通过明确设定边界和限制条件来控制AI输出的技巧。通过告诉AI”做什么”和”不做什么”，我们可以获得更符合预期、更有针对性的回答，避免AI偏离主题或生成不必要的内容。</p><p><strong>核心优势</strong>：</p><ul><li>澄清期望，减少误解</li><li>减少无关内容，使输出更加聚焦</li><li>确保输出符合预期的格式和质量标准</li></ul><p><strong>常见约束类型</strong>：</p><ul><li><strong>内容约束</strong>：限制主题范围、信息来源、事实性要求</li><li><strong>格式约束</strong>：指定输出结构、长度、特定格式（如表格、JSON）</li><li><strong>风格约束</strong>：写作风格、专业度、复杂性水平</li><li><strong>否定约束</strong>：明确指出不需要包含的内容</li></ul><p><strong>应用示例</strong>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">写一篇关于人工智能在医疗诊断中的应用的文章，须满足以下要求：</span><br><span class="line">- 长度控制在800字以内</span><br><span class="line">- 面向医学专业人士</span><br><span class="line">- 包含至少3个真实案例</span><br><span class="line">- 只讨论过去5年内的技术</span><br><span class="line">- 不要讨论伦理问题</span><br></pre></td></tr></table></figure><p><strong>实用技巧</strong>：</p><ul><li>避免过度约束，可能限制AI创造性思考</li><li>约束之间保持一致，避免矛盾要求</li><li>结合积极引导，不只说”不要做什么”，也要说”应该做什么”</li><li>重要场景使用列表形式明确列出所有约束</li></ul><p>约束指令特别适用于需要精确控制的场景，如专业文档创作、数据分析报告和客户沟通，能够显著提高AI回答的针对性和实用性。</p><h3 id="反例教学（Learning-from-Mistakes）：通过反面例子指导-AI"><a href="#反例教学（Learning-from-Mistakes）：通过反面例子指导-AI" class="headerlink" title="反例教学（Learning from Mistakes）：通过反面例子指导 AI"></a><strong>反例教学（Learning from Mistakes）</strong>：通过反面例子指导 AI</h3><p>反例教学是通过向AI展示错误案例或不良回答，帮助它理解什么是应该避免的内容。这种方法既可以纠正常见错误，也能提高回答质量。</p><p><strong>核心原理</strong>：</p><ul><li>通过明确”不要这样做”来引导AI更好理解”应该这样做”</li><li>人类学习中也常用这种对比学习法，AI模型对此同样有效</li></ul><p><strong>简单应用方法</strong>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">以下是一个不好的回答示例：</span><br><span class="line">[插入反例]</span><br><span class="line"></span><br><span class="line">这个回答有以下问题：</span><br><span class="line">1. [问题1]</span><br><span class="line">2. [问题2]</span><br><span class="line"></span><br><span class="line">请避免上述问题，提供一个更好的回答。</span><br></pre></td></tr></table></figure><p><strong>实际例子</strong>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">问题：如何写一篇好的商业提案？</span><br><span class="line"></span><br><span class="line">不好的回答：</span><br><span class="line">&quot;商业提案应该漂亮、专业，能抓住人的眼球。使用高大上的词汇会显得你很专业。&quot;</span><br><span class="line"></span><br><span class="line">这个回答的问题：</span><br><span class="line">- 过于笼统，缺乏具体建议</span><br><span class="line">- 强调表面而非内容</span><br><span class="line">- 误导读者使用华丽词藻而非清晰表达</span><br><span class="line"></span><br><span class="line">请提供一个更有帮助的回答，包含具体的结构建议和内容要点。</span><br></pre></td></tr></table></figure><p>反例教学尤其适用于需要高质量输出的场景，如内容创作、代码编写和专业建议。与其他技巧结合使用效果更佳。</p><h3 id="工具调用（Tool-Calling）：将外部工具集成到提示中"><a href="#工具调用（Tool-Calling）：将外部工具集成到提示中" class="headerlink" title="工具调用（Tool Calling）：将外部工具集成到提示中"></a><strong>工具调用（Tool Calling）</strong>：将外部工具集成到提示中</h3><p>工具调用是一种允许AI模型与外部工具、API或数据源交互的高级提示技巧。通过这种方式，AI不仅能够生成文本回答，还能执行具体操作，如查询数据库、调用API、执行计算等。</p><p><strong>核心概念</strong>：</p><ul><li><strong>Function Calling</strong>：使AI能够识别何时需要调用预定义函数，并以正确格式提供所需参数</li><li>**MCP(Multimodal Conversational Processor)**：多模态处理框架，能协调AI与多种工具的交互</li></ul><p><strong>Function Call示例</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义函数接口</span></span><br><span class="line">functions = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;get_weather&quot;</span>,</span><br><span class="line">        <span class="string">&quot;description&quot;</span>: <span class="string">&quot;获取指定城市的天气信息&quot;</span>,</span><br><span class="line">        <span class="string">&quot;parameters&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;type&quot;</span>: <span class="string">&quot;object&quot;</span>,</span><br><span class="line">            <span class="string">&quot;properties&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;city&quot;</span>: &#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>, <span class="string">&quot;description&quot;</span>: <span class="string">&quot;城市名称&quot;</span>&#125;,</span><br><span class="line">                <span class="string">&quot;date&quot;</span>: &#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>, <span class="string">&quot;description&quot;</span>: <span class="string">&quot;日期，格式为YYYY-MM-DD&quot;</span>&#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">&quot;required&quot;</span>: [<span class="string">&quot;city&quot;</span>]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在提示中调用</span></span><br><span class="line">response = model.chat.completions.create(</span><br><span class="line">    model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>,</span><br><span class="line">    messages=[</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;北京明天天气怎么样？&quot;</span>&#125;</span><br><span class="line">    ],</span><br><span class="line">    functions=functions,</span><br><span class="line">    function_call=<span class="string">&quot;auto&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><strong>MCP工作流程</strong>：</p><ol><li><strong>理解请求</strong>：AI分析用户输入，确定需要调用哪些工具</li><li><strong>工具选择</strong>：从可用工具集中选择最合适的工具</li><li><strong>参数提取</strong>：从用户输入中提取调用工具所需的参数</li><li><strong>执行调用</strong>：以正确格式调用工具API</li><li><strong>结果整合</strong>：将工具返回的结果整合到最终回答中</li></ol><p><strong>主要优势</strong>：</p><ul><li>突破纯文本生成的限制，实现真实世界交互</li><li>提供最新数据和实时信息</li><li>执行复杂计算或查询而不受模型知识限制</li><li>创建实用的、能够执行操作的AI助手</li></ul><p><strong>RAGAS</strong>是一个用于衡量RAG系统性能的项目，它衡量了RAG系统在信息检索、内容生成和交互质量方面的表现。 <a class="link"   href="https://github.com/explodinggradients/ragas" >https://github.com/explodinggradients/ragas<i class="fas fa-external-link-alt"></i></a> </p><p>在实践中，工具调用特别适合构建功能强大的AI助手，如日程管理、数据分析、智能家居控制和内容搜索等场景。使用这种技术需要一定的编程基础，但能显著扩展AI应用的能力边界。</p><h3 id="规范格式"><a href="#规范格式" class="headerlink" title="规范格式"></a><strong>规范格式</strong></h3><ol><li>使用Markdown语法</li><li>使用JSON格式</li><li>使用XML标签<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;examples&gt;</span><br><span class="line">&lt;example1 type=&quot;Abbreviate&quot;&gt;</span><br><span class="line">&lt;input&gt;San Francisco&lt;/input&gt;</span><br><span class="line">&lt;output&gt;- SF&lt;/output&gt;</span><br><span class="line">&lt;/example1&gt;</span><br><span class="line">&lt;/examples&gt;</span><br></pre></td></tr></table></figure></li></ol><p>##提示词流程 （OpenAI4.1 Workflow）<br>以下是我们推荐的开发和调试提示词的工作流程：</p><ol><li><strong>从高级指导开始</strong>：首先创建一个整体的”响应规则”或”指令”部分，提供高层次的指导和要点。</li><li><strong>添加特定行为修改</strong>：如果你想改变更具体的行为，添加一个部分来为该类别指定更多细节，比如”举例”。</li><li><strong>指定工作流程步骤</strong>：如果有特定步骤希望模型在其工作流程中遵循，添加一个有序列表并指示模型遵循这些步骤。</li><li><strong>解决效果不佳的问题</strong>：如果行为仍然不符合预期：<ul><li>检查是否存在冲突、未充分说明或错误的指令和示例。如果有冲突的指令，大语言模型通常会遵循提示词末尾附近的指令。</li><li>添加展示期望行为的例子；确保在你的规则中也引用了示例中展示的任何重要行为。</li></ul></li><li><strong>避免不必要的强调</strong>：通常不需要使用全大写字母或其他激励措施。建议一开始不使用这些技巧，只在必要时才采用。请注意，如果你现有的提示已包含这些技术，可能会导致模型过分关注它们。</li></ol><h2 id="提示词模版"><a href="#提示词模版" class="headerlink" title="提示词模版"></a>提示词模版</h2><p>结合以上的内容，接下来将是一个通用的提示词模版结构，适用于大多数复杂任务。这个框架可以根据需要进行调整，但包含这些元素通常会得到更好的结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 角色与目标（Role and Objective）</span><br><span class="line">[指定AI应该扮演的角色，以及需要完成的总体目标]</span><br><span class="line"></span><br><span class="line"># 指令（Instructions）</span><br><span class="line">[详细说明需要AI执行的具体任务和要求]</span><br><span class="line"></span><br><span class="line">## 子类别指令（Sub-categories）</span><br><span class="line">[可以按需分类更详细的指令]</span><br><span class="line"></span><br><span class="line"># 推理步骤（Reasoning Steps）</span><br><span class="line">[引导AI如何思考问题，可以要求它遵循特定的分析步骤]</span><br><span class="line"></span><br><span class="line"># 输出格式（Output Format）</span><br><span class="line">[明确指定你希望得到的回答格式，如列表、表格、JSON等]</span><br><span class="line"></span><br><span class="line"># 示例（Examples）</span><br><span class="line">## 示例1</span><br><span class="line">[提供一个实例，展示你期望的输入-输出对]</span><br><span class="line"></span><br><span class="line"># 背景信息（Context）</span><br><span class="line">[提供相关的背景信息或参考资料]</span><br><span class="line"></span><br><span class="line"># 最终指示（Final instructions）</span><br><span class="line">[最后的提醒和要求，通常包括&quot;请一步一步思考&quot;等引导]</span><br></pre></td></tr></table></figure><p>这个模版的每个部分都有特定用途：</p><ul><li><strong>角色与目标</strong>：设定AI的专业身份和任务目的，帮助它理解回答的整体方向</li><li><strong>指令</strong>：明确任务的具体要求和约束条件</li><li><strong>推理步骤</strong>：指导AI如何分析问题，尤其适合复杂任务</li><li><strong>输出格式</strong>：确保AI的回答符合你的使用需求和偏好</li><li><strong>示例</strong>：通过实例直观地传达你的期望</li><li><strong>背景信息</strong>：提供AI可能需要的额外参考数据</li><li><strong>最终指示</strong>：引导AI采用特定的思考方式，如系统性、逐步分析</li></ul><p>此外，在使用Prompt 的过程中，经过不断尝试需要添加以下几点来增强表现的能力</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">## PERSISTENCE</span><br><span class="line">You are an agent - please keep going until the user&#x27;s query is completely </span><br><span class="line">resolved, before ending your turn and yielding back to the user. Only </span><br><span class="line">terminate your turn when you are sure that the problem is solved.</span><br><span class="line"></span><br><span class="line">## TOOL CALLING</span><br><span class="line">If you are not sure about file content or codebase structure pertaining to </span><br><span class="line">the user&#x27;s request, use your tools to read files and gather the relevant </span><br><span class="line">information: do NOT guess or make up an answer.</span><br><span class="line"></span><br><span class="line">## PLANNING</span><br><span class="line">You MUST plan extensively before each function call, and reflect </span><br><span class="line">extensively on the outcomes of the previous function calls. DO NOT do this </span><br><span class="line">entire process by making function calls only, as this can impair your </span><br><span class="line">ability to solve the problem and think insightfully.</span><br><span class="line"></span><br><span class="line">## 坚持</span><br><span class="line">您是一名代理 - 请持续操作，直到用户的查询完全解决，然后结束您的回合并交还给用户。只有当您确定问题已解决时，才终止您的回合。</span><br><span class="line"></span><br><span class="line">## 工具调用</span><br><span class="line">如果您不确定与用户请求相关的文件内容或代码库结构，请使用您的工具读取文件并收集相关信息：请勿猜测或编造答案。</span><br><span class="line"></span><br><span class="line">## 规划</span><br><span class="line">您必须在每次函数调用之前进行周密的规划，并充分反思之前函数调用的结果。不要在整个过程中只进行函数调用，因为这会削弱您解决问题和深入思考的能力。</span><br></pre></td></tr></table></figure><p>在实际应用中，你可以根据任务的复杂性和具体需求选择性使用这些部分。对于简单任务，可能只需要指令和输出格式；而对于复杂分析，完整的模版可能会带来更好的结果。</p><h2 id="LangGPT提示词模版"><a href="#LangGPT提示词模版" class="headerlink" title="LangGPT提示词模版"></a>LangGPT提示词模版</h2><p><a href="%EF%BC%88https://arxiv.org/abs/2402.16929%EF%BC%89">LangGPT</a> 是一个创新的结构化提示词设计框架，旨在让每个人都能成为提示词专家。该项目由”云中江树”创建，通过将编程语言的思想应用到提示工程中，提供了一种系统化、可复用的提示词创建方法。</p><h3 id="核心理念"><a href="#核心理念" class="headerlink" title="核心理念"></a>核心理念</h3><p>LangGPT的核心是将提示词视为一种”编程语言”，通过结构化的模板来定义与AI的交互。这种方法有以下优势：</p><ol><li><strong>结构清晰</strong>：使用角色模板组织提示词，包含角色设定、技能、规则等元素</li><li><strong>易于复用</strong>：模板化设计使提示词可以被重复使用和修改</li><li><strong>提高效率</strong>：降低编写高质量提示词的门槛，节省时间</li><li><strong>质量保证</strong>：通过标准化结构减少遗漏和错误</li></ol><h3 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h3><p>LangGPT主要基于角色模板（Role Template）构建提示词，一个典型的LangGPT提示词包含以下部分：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># Role: 角色名称</span></span><br><span class="line"></span><br><span class="line"><span class="section">## Profile</span></span><br><span class="line"><span class="bullet">-</span> 专业领域</span><br><span class="line"><span class="bullet">-</span> 语言风格</span><br><span class="line"><span class="bullet">-</span> 价值观</span><br><span class="line"></span><br><span class="line"><span class="section">## Skills</span></span><br><span class="line"><span class="bullet">-</span> 技能1</span><br><span class="line"><span class="bullet">-</span> 技能2</span><br><span class="line"></span><br><span class="line"><span class="section">## Goals</span></span><br><span class="line"><span class="bullet">-</span> 目标1</span><br><span class="line"><span class="bullet">-</span> 目标2</span><br><span class="line"></span><br><span class="line"><span class="section">## Constraints</span></span><br><span class="line"><span class="bullet">-</span> 限制条件1</span><br><span class="line"><span class="bullet">-</span> 限制条件2</span><br><span class="line"></span><br><span class="line"><span class="section">## Workflows</span></span><br><span class="line"><span class="bullet">1.</span> 步骤1</span><br><span class="line"><span class="bullet">2.</span> 步骤2</span><br><span class="line"></span><br><span class="line"><span class="section">## Output Format</span></span><br><span class="line"><span class="bullet">-</span> 输出格式要求</span><br></pre></td></tr></table></figure><h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><p>LangGPT框架适用于广泛的场景，包括但不限于：</p><ul><li>创建专业顾问角色（如医生、律师、教师）</li><li>设计创意写作助手</li><li>开发代码编程指导</li><li>构建个性化学习导师</li><li>创建数据分析专家</li></ul><h3 id="发展状态"><a href="#发展状态" class="headerlink" title="发展状态"></a>发展状态</h3><p>该项目目前处于发展阶段，已经完成基础角色模板、文档和示例等功能，正在开发更高级的语法特性、提示链、JSON/YAML支持等功能。项目团队欢迎社区贡献，共同推动提示工程的发展。</p><h3 id="用于帮助生成LangGPT结构的提示词"><a href="#用于帮助生成LangGPT结构的提示词" class="headerlink" title="用于帮助生成LangGPT结构的提示词"></a>用于帮助生成LangGPT结构的提示词</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"># Role: LangGPT Prompt Assistant</span><br><span class="line"></span><br><span class="line">## Profile</span><br><span class="line"></span><br><span class="line">- Author: Allen</span><br><span class="line">- Version: 1.1</span><br><span class="line">- Language: English</span><br><span class="line">- Description: You are a specialized AI assistant that helps users create high-quality structured prompts following the LangGPT methodology. Your expertise is in transforming user requirements into well-structured, effective prompts that maximize the performance of large language models.</span><br><span class="line"></span><br><span class="line">### Skills:</span><br><span class="line">- Deep understanding of the LangGPT structured prompt methodology</span><br><span class="line">- Ability to guide users through the prompt creation process step by step</span><br><span class="line">- Knowledge of prompt engineering best practices</span><br><span class="line">- Expertise in creating role-based prompts with clear structures</span><br><span class="line">- Ability to explain the purpose and benefits of each section in a LangGPT prompt</span><br><span class="line">- Skill in helping users refine and improve their prompts</span><br><span class="line"></span><br><span class="line">## Rules</span><br><span class="line">1. Always maintain a helpful and educational tone.</span><br><span class="line">2. Explain the purpose of each section when introducing it.</span><br><span class="line">3. Provide examples when they would be helpful.</span><br><span class="line">4. Don&#x27;t skip important sections of the LangGPT structure.</span><br><span class="line">5. Ask clarifying questions when user requirements are unclear.</span><br><span class="line">6. Always output the final prompt in proper markdown format.</span><br><span class="line">7. Suggest improvements to user-provided content when appropriate.</span><br><span class="line">8. Don&#x27;t include placeholder text in the final prompt without explaining it.</span><br><span class="line">9. Ensure the final prompt is coherent and follows the LangGPT structure.</span><br><span class="line">10. Respect the user&#x27;s specific requirements while guiding them toward best practices.</span><br><span class="line"></span><br><span class="line">## Workflow</span><br><span class="line">1. Greet the user and explain your purpose as a LangGPT Prompt Assistant.</span><br><span class="line">2. Ask the user about their prompt requirements (what kind of assistant they want to create).</span><br><span class="line">3. Guide the user through creating each section of the LangGPT prompt:</span><br><span class="line">   - Role: The specific role the AI should adopt</span><br><span class="line">   - Profile: Basic information about the role (Author, Version, Language, Description)</span><br><span class="line">   - Skills: Specific abilities the role should have</span><br><span class="line">   - Rules: Guidelines the role must follow</span><br><span class="line">   - Workflow: Step-by-step process the role should follow</span><br><span class="line">   - Commands (optional): Special commands the user can use</span><br><span class="line">   - Initialization: How the role should begin the conversation</span><br><span class="line">4. Compile all the information into a properly formatted LangGPT prompt.</span><br><span class="line">5. Present the final prompt to the user in markdown format.</span><br><span class="line">6. Offer to make any adjustments or improvements.</span><br><span class="line"></span><br><span class="line">## Commands</span><br><span class="line">- Prefix: &quot;/&quot;</span><br><span class="line">- Commands:</span><br><span class="line">    - help: Explain the LangGPT methodology and how to use this assistant.</span><br><span class="line">    - template: Show the basic LangGPT template structure.</span><br><span class="line">    - example: Provide an example of a complete LangGPT prompt.</span><br><span class="line">    - section: Explain a specific section (e.g., /section profile).</span><br><span class="line"></span><br><span class="line">## Initialization</span><br><span class="line">As a/an &lt;Role&gt;, you must follow the &lt;Rules&gt;, you must talk to the user in the default &lt;Language&gt;. You must greet the user, introduce yourself as a specialized assistant for creating LangGPT structured prompts, and briefly explain what LangGPT is. Explain that you&#x27;ll guide them through creating a structured prompt step by step, and they can use commands like /help, /template, or /example at any time. Ask what kind of prompt they&#x27;d like to create. </span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="案例1：客服"><a href="#案例1：客服" class="headerlink" title="案例1：客服"></a>案例1：客服</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">SYS_PROMPT_CUSTOMER_SERVICE = &quot;&quot;&quot;You are a helpful customer service agent working for NewTelco, helping a user efficiently fulfill their request while adhering closely to provided guidelines.</span><br><span class="line"></span><br><span class="line"># Instructions</span><br><span class="line">- Always greet the user with &quot;Hi, you&#x27;ve reached NewTelco, how can I help you?&quot;</span><br><span class="line">- Always call a tool before answering factual questions about the company, its offerings or products, or a user&#x27;s account. Only use retrieved context and never rely on your own knowledge for any of these questions.</span><br><span class="line">    - However, if you don&#x27;t have enough information to properly call the tool, ask the user for the information you need.</span><br><span class="line">- Escalate to a human if the user requests.</span><br><span class="line">- Do not discuss prohibited topics (politics, religion, controversial current events, medical, legal, or financial advice, personal conversations, internal company operations, or criticism of any people or company).</span><br><span class="line">- Rely on sample phrases whenever appropriate, but never repeat a sample phrase in the same conversation. Feel free to vary the sample phrases to avoid sounding repetitive and make it more appropriate for the user.</span><br><span class="line">- Always follow the provided output format for new messages, including citations for any factual statements from retrieved policy documents.</span><br><span class="line">- If you&#x27;re going to call a tool, always message the user with an appropriate message before and after calling the tool.</span><br><span class="line">- Maintain a professional and concise tone in all responses, and use emojis between sentences.</span><br><span class="line">- If you&#x27;ve resolved the user&#x27;s request, ask if there&#x27;s anything else you can help with</span><br><span class="line"></span><br><span class="line"># Precise Response Steps (for each response)</span><br><span class="line">1. If necessary, call tools to fulfill the user&#x27;s desired action. Always message the user before and after calling a tool to keep them in the loop.</span><br><span class="line">2. In your response to the user</span><br><span class="line">    a. Use active listening and echo back what you heard the user ask for.</span><br><span class="line">    b. Respond appropriately given the above guidelines.</span><br><span class="line"></span><br><span class="line"># Sample Phrases</span><br><span class="line">## Deflecting a Prohibited Topic</span><br><span class="line">- &quot;I&#x27;m sorry, but I&#x27;m unable to discuss that topic. Is there something else I can help you with?&quot;</span><br><span class="line">- &quot;That&#x27;s not something I&#x27;m able to provide information on, but I&#x27;m happy to help with any other questions you may have.&quot;</span><br><span class="line"></span><br><span class="line">## Before calling a tool</span><br><span class="line">- &quot;To help you with that, I&#x27;ll just need to verify your information.&quot;</span><br><span class="line">- &quot;Let me check that for you—one moment, please.&quot;</span><br><span class="line">- &quot;I&#x27;ll retrieve the latest details for you now.&quot;</span><br><span class="line"></span><br><span class="line">## After calling a tool</span><br><span class="line">- &quot;Okay, here&#x27;s what I found: [response]&quot;</span><br><span class="line">- &quot;So here&#x27;s what I found: [response]&quot;</span><br><span class="line"></span><br><span class="line"># Output Format</span><br><span class="line">- Always include your final response to the user.</span><br><span class="line">- When providing factual information from retrieved context, always include citations immediately after the relevant statement(s). Use the following citation format:</span><br><span class="line">    - For a single source: [NAME](ID)</span><br><span class="line">    - For multiple sources: [NAME](ID), [NAME](ID)</span><br><span class="line">- Only provide information about this company, its policies, its products, or the customer&#x27;s account, and only if it is based on information provided in context. Do not answer questions outside this scope.</span><br><span class="line"></span><br><span class="line"># Example</span><br><span class="line">## User</span><br><span class="line">Can you tell me about your family plan options?</span><br><span class="line"></span><br><span class="line">## Assistant Response 1</span><br><span class="line">### Message</span><br><span class="line">&quot;Hi, you&#x27;ve reached NewTelco, how can I help you? 😊🎉\n\nYou&#x27;d like to know about our family plan options. 🤝 Let me check that for you—one moment, please. 🚀&quot;</span><br><span class="line"></span><br><span class="line">### Tool Calls</span><br><span class="line">lookup_policy_document(topic=&quot;family plan options&quot;)</span><br><span class="line"></span><br><span class="line">// After tool call, the assistant would follow up with:</span><br><span class="line"></span><br><span class="line">## Assistant Response 2 (after tool call)</span><br><span class="line">### Message</span><br><span class="line">&quot;Okay, here&#x27;s what I found: 🎉 Our family plan allows up to 5 lines with shared data and a 10% discount for each additional line [Family Plan Policy](ID-010). 📱 Is there anything else I can help you with today? 😊&quot;</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">get_policy_doc = &#123;</span><br><span class="line">    &quot;type&quot;: &quot;function&quot;,</span><br><span class="line">    &quot;name&quot;: &quot;lookup_policy_document&quot;,</span><br><span class="line">    &quot;description&quot;: &quot;Tool to look up internal documents and policies by topic or keyword.&quot;,</span><br><span class="line">    &quot;parameters&quot;: &#123;</span><br><span class="line">        &quot;strict&quot;: True,</span><br><span class="line">        &quot;type&quot;: &quot;object&quot;,</span><br><span class="line">        &quot;properties&quot;: &#123;</span><br><span class="line">            &quot;topic&quot;: &#123;</span><br><span class="line">                &quot;type&quot;: &quot;string&quot;,</span><br><span class="line">                &quot;description&quot;: &quot;The topic or keyword to search for in company policies or documents.&quot;,</span><br><span class="line">            &#125;,</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;required&quot;: [&quot;topic&quot;],</span><br><span class="line">        &quot;additionalProperties&quot;: False,</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">get_user_acct = &#123;</span><br><span class="line">    &quot;type&quot;: &quot;function&quot;,</span><br><span class="line">    &quot;name&quot;: &quot;get_user_account_info&quot;,</span><br><span class="line">    &quot;description&quot;: &quot;Tool to get user account information&quot;,</span><br><span class="line">    &quot;parameters&quot;: &#123;</span><br><span class="line">        &quot;strict&quot;: True,</span><br><span class="line">        &quot;type&quot;: &quot;object&quot;,</span><br><span class="line">        &quot;properties&quot;: &#123;</span><br><span class="line">            &quot;phone_number&quot;: &#123;</span><br><span class="line">                &quot;type&quot;: &quot;string&quot;,</span><br><span class="line">                &quot;description&quot;: &quot;Formatted as &#x27;(xxx) xxx-xxxx&#x27;&quot;,</span><br><span class="line">            &#125;,</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;required&quot;: [&quot;phone_number&quot;],</span><br><span class="line">        &quot;additionalProperties&quot;: False,</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = client.responses.create(</span><br><span class="line">    instructions=SYS_PROMPT_CUSTOMER_SERVICE,</span><br><span class="line">    model=&quot;gpt-4.1-2025-04-14&quot;,</span><br><span class="line">    tools=[get_policy_doc, get_user_acct],</span><br><span class="line">    input=&quot;How much will it cost for international service? I&#x27;m traveling to France.&quot;,</span><br><span class="line">    # input=&quot;Why was my last bill so high?&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">response.to_dict()[&quot;output&quot;]</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;提示词工程（Prompt-Engineering）学习笔记&quot;&gt;&lt;a href=&quot;#提示词工程（Prompt-Engineering）学习笔记&quot; class=&quot;headerlink&quot; title=&quot;提示词工程（Prompt Engineering）学习笔记&quot;&gt;&lt;/a</summary>
      
    
    
    
    
    <category term="Prompt Engineering" scheme="http://example.com/tags/Prompt-Engineering/"/>
    
    <category term="LLM" scheme="http://example.com/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>Face Recognition</title>
    <link href="http://example.com/2024/12/14/Face-Recognition/"/>
    <id>http://example.com/2024/12/14/Face-Recognition/</id>
    <published>2024-12-14T08:24:37.000Z</published>
    <updated>2024-12-15T14:19:10.001Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Face-Recognition"><a href="#Face-Recognition" class="headerlink" title="Face Recognition"></a>Face Recognition</h1><p>Face Recognition was part of the Human-Robot Interaction and my graduation project, which is to build a reception robot. In a home environment, smart home robots must accurately detect and identify all family members to perform tasks specific to individual household members. Therefore, this blog focuses on implementing facial recognition and analyzing related aspects.</p><p>Face recognition tasks can be broadly divided into two parts: <strong>face detection</strong> and <strong>face recognition</strong>. Face detection refers to locating all faces in an input image and processing them, such as aligning the faces, generating bounding boxes, and resizing or cropping the detected faces. However, due to the potential challenges posed by the robot’s field of vision, such as incomplete or peripheral facial appearances, face detection can become more complex. Additional complications arise from variations in lighting, occlusions, differences in the same individual’s pose or expressions, and inter-individual appearance differences.</p><p>To address these challenges, accurate facial recognition in complex environments requires precise localization of faces, followed by the detection of facial feature points, such as facial contours, eye outlines, nose, and mouth contours. These feature points are then used to align the faces uniformly, eliminating errors caused by position or pose variations.</p><p><strong>Face recognition</strong> can be described as comparing a given photo to a dataset, calculating the facial similarity, and outputting a similarity score or confidence level. Alternatively, it involves identifying which individual in the dataset the photo belongs to and retrieving the relevant information about the individual from the database.</p><p>A representative algorithm in this domain is <strong>FaceNet</strong>, which won the 2014 ILSVRC (ImageNet Large Scale Visual Recognition Challenge) and reduced the Top-5 error rate for facial recognition to 6.67%. FaceNet uses the GoogLeNet network architecture, as outlined in the associated paper. Following this milestone, facial recognition algorithms have continued to evolve. The latest implementation on GitHub recommends using the <strong>MTCNN</strong> (Multi-task Cascaded Convolutional Networks) algorithm, which improves detection accuracy to 99.65%.</p><p>Therefore, the primary focus of this blog is the introduction and implementation of the MTCNN and FaceNet network architectures.</p><h1 id="Face-Detection-MTCNN"><a href="#Face-Detection-MTCNN" class="headerlink" title="Face Detection MTCNN"></a>Face Detection MTCNN</h1><h2 id="Why-MTCNN-rather-than-OpenCV"><a href="#Why-MTCNN-rather-than-OpenCV" class="headerlink" title="Why MTCNN rather than OpenCV?"></a>Why MTCNN rather than OpenCV?</h2><p>There are several methods for face detection, such as the Haar feature-based classifier provided by OpenCV and the face detection method in dlib. For OpenCV’s face detection method, its advantages lie in being simple and fast; however, its drawbacks include poor detection performance. It works well for frontal, upright faces in good lighting conditions, but fails to detect faces that are turned sideways, tilted, or poorly lit. Therefore, this method is not suitable for real-world applications. In comparison, dlib’s face detection method provides better performance than OpenCV’s, but its detection capability still does not meet the standards required for real-world applications.</p><p>In this study, we use the MTCNN-based deep learning approach for face detection (MTCNN: Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Neural Networks). The MTCNN face detection method is more robust to variations in lighting, angles, and facial expressions in natural environments, offering better face detection results. Additionally, it has a relatively low memory consumption, enabling real-time face detection. The MTCNN method used in this blog is implemented in Python and TensorFlow.</p><h2 id="How-MTCNN-works"><a href="#How-MTCNN-works" class="headerlink" title="How MTCNN works?"></a>How MTCNN works?</h2><p>Face detection is a critical step in the face recognition process, and the performance of this step significantly impacts the video processing speed and accuracy of face recognition in a robotic system. The method adopted in this paper, <strong>MTCNN</strong>, integrates both face detection and face alignment to assist in the detection of facial key points. The core of the algorithm involves using three multi-stage deep convolutional neural networks to form a face detection network framework through a cascaded structure.</p><p>Figure 1 illustrates the flowchart of the MTCNN face detection process. Initially, candidate boxes are generated by the <strong>P-Net</strong> (Proposal Network). P-Net performs the first-level analysis of the image and outputs candidate objects. In the next phase, the <strong>R-Net</strong> (Refinement Network) further analyzes and refines these candidate objects. In the final stage, the <strong>O-Net</strong> (Output Network) generates the final bounding boxes and the locations of facial key points.</p><p>Thus, the MTCNN architecture can be understood as a cascaded structure involving an image pyramid and the networks P-Net, R-Net, and O-Net, which together contribute to the accurate detection and alignment of faces.</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.1ovhlgrglp.webp"                      alt="Figure 1: MTCNN face detection process framework"                ></p><p>The MTCNN model consists of three processing stages, which ultimately produce results for both face detection and face alignment. Before using the three cascaded networks, the input image is first processed through an image pyramid, which scales the image at different resolutions. This approach is necessary because faces can appear at various sizes within an image, and scaling the image at multiple resolutions helps detect faces at different scales. This method allows for a rough initial classification of the image, removing non-face regions and identifying face-containing areas. The advantage of this approach is its ability to quickly detect faces of different sizes, enabling multi-scale target detection and recognition.</p><p>Below is an introduction to the three processing stages of MTCNN face detection, illustrated by Figure 1.</p><h3 id="First-Stage-P-Net"><a href="#First-Stage-P-Net" class="headerlink" title="First Stage: P-Net"></a>First Stage: P-Net</h3><p>In the first stage, <strong>P-Net</strong> generates feature maps through forward propagation, where each location is represented by a 32-dimensional feature vector used to determine whether a face is present in that region. As shown in Figure 2(a), <strong>P-Net</strong> consists of three 3x3 convolutional layers and one 3x3 max-pooling layer. Its input is a processed 12x12x3 RGB image. The network outputs two parts:</p><ol><li>A 32-dimensional feature vector that indicates the probability of a face being present in the region. Since a <strong>Softmax</strong> layer is used, the final output is the probability of the presence of a face in the image.</li><li>If a face is detected, a bounding box indicating the face is returned, which constitutes the second part of the output.</li></ol><p>The bounding boxes from P-Net may not always align perfectly with the actual face positions. For example, sometimes the face may be only partially within the bounding box or outside the box. In such cases, the region is adjusted to more accurately reflect the desired area. Once the bounding box with the highest probability of containing a face is identified, <strong>Non-Maximum Suppression (NMS)</strong> is applied to retain the box with the highest score and remove other less probable boxes.</p><p>The third part of P-Net’s output is the coordinates of five key facial points: the positions of the left eye, right eye, nose, left corner of the mouth, and right corner of the mouth. Although P-Net provides the key point coordinates, there may still be many false positive face boxes, so the focus at this stage is on selecting the correct face box.</p><h3 id="Second-Stage-R-Net"><a href="#Second-Stage-R-Net" class="headerlink" title="Second Stage: R-Net"></a>Second Stage: R-Net</h3><p>As shown in Figure 2(b), <strong>R-Net</strong> consists of two 3x3 convolutional layers, one 2x2 convolutional layer, two 3x3 max-pooling layers, and a 128-dimensional fully connected layer. Since the candidate boxes generated by P-Net are still rough, <strong>R-Net</strong> is used to further optimize the results. Before inputting the data, the face bounding box coordinates from P-Net undergo a transformation to map the image size to 24x24x3.</p><p>The output of <strong>R-Net</strong> is similar to that of P-Net, but its main purpose is to refine the results by removing irrelevant face boxes—non-face boxes or partially incorrect face boxes—and passing the refined results to the third stage (O-Net) for final selection.</p><h3 id="Third-Stage-O-Net"><a href="#Third-Stage-O-Net" class="headerlink" title="Third Stage: O-Net"></a>Third Stage: O-Net</h3><p>In Figure 2(c), <strong>O-Net</strong> consists of three 3x3 convolutional layers, one 2x2 convolutional layer, two 3x3 and one 2x2 max-pooling layers, and a 256-dimensional fully connected layer. The input to <strong>O-Net</strong> is the output from <strong>R-Net</strong>, scaled to an image size of 48x48x3. The output of <strong>O-Net</strong> includes:</p><ul><li>The coordinates of the final bounding box</li><li>The confidence score</li><li>The coordinates of the facial key points</li></ul><p>This final stage provides the precise face detection and alignment results, refining the bounding boxes and facial key points obtained from the previous stages.</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.8hgj7xmscv.webp"                      alt="Figure 2: MTCNN subnetwork framework"                ></p><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>Since MTCNN is a cascaded structure consisting of three sub-networks, each sub-network has its own loss function. In the task of distinguishing between faces and non-faces, which is a binary classification problem, the <strong>cross-entropy loss</strong> function is used. For bounding box regression and key point localization, the network predicts the offsets between the predicted values and the true values, so the <strong>Euclidean distance</strong> is used as the loss function for these tasks.</p><p>Finally, the total loss function is obtained by summing the individual losses of the three sub-networks, with each loss weighted according to the goals of each stage. The training focuses of the three sub-networks vary, so different weightings are assigned to each loss function. During the training of <strong>P-Net</strong> and <strong>R-Net</strong>, the main focus is on the accuracy of the bounding boxes, so the contribution of the key point loss is relatively small. In contrast, during the training of <strong>O-Net</strong>, which emphasizes key point selection, the weight of the key point loss is larger compared to the other components.</p><p>Thus, the total loss function for MTCNN is a weighted sum of three parts:</p><ol><li>The <strong>cross-entropy loss</strong> for face vs. non-face classification.</li></ol><p>Since face classification is a binary classification problem, the <strong>cross-entropy loss function</strong> is used. For each sample $x_i$, the loss function is defined as:</p><p>$L_i^{\text{det}} = - \left( y_i^{\text{det}} \log(P_i) + (1 - y_i^{\text{det}})(1 - \log(P_i)) \right)$</p><p>Where:</p><ul><li>$P_i$ represents the probability output by the network that the sample is a face.</li><li>$y_i^{\text{det}} \in {0,1}$ indicates the ground truth label for whether the sample is a face (1) or not (0).</li></ul><ol start="2"><li>The <strong>Euclidean distance loss</strong> for bounding box regression.</li></ol><p>For each candidate bounding box, the loss function measures the offset between the predicted values and the ground truth. For each sample $x_i$, the <strong>Euclidean distance</strong> is used as the loss function, defined as:</p><p>$L_i^{\text{box}} = | \hat{y}_i^{\text{box}} - y_i^{\text{box}} |_2^2$</p><p>Where:</p><ul><li>$\hat{y}_i^{\text{box}}$ is the network’s prediction for the bounding box, which includes four values: the top-left corner coordinates, width, and height of the box.</li><li>$y_i^{\text{box}}$represents the ground truth for each sample’s bounding box.</li></ul><ol start="3"><li>The <strong>Euclidean distance loss</strong> for key point localization.</li></ol><p>For key point localization, a loss function similar to bounding box regression is used. The <strong>Euclidean distance</strong> measures the offset between the predicted and true key point coordinates, defined as:</p><p>$L_i^{\text{landmark}} = | \hat{y}_i^{\text{landmark}} - y_i^{\text{landmark}} |_2^2$</p><p>Where:</p><ul><li>$\hat{y}_i^{\text{landmark}}$is the network’s prediction for the key points, including 10 values corresponding to the coordinates of the left eye, right eye, nose, left mouth corner, and right mouth corner.</li><li>$y_i^{\text{landmark}}$ is the ground truth for the facial key points.</li></ul><p>Each of these components is weighted appropriately to reflect the emphasis of the corresponding stage in the network.</p><ol start="4"><li>Total Loss Function</li></ol><p>The total loss combines the three individual loss functions with different weights, formulated as:</p><p>$L = \min \sum_{i=1}^N \sum_{j \in {\text{det}, \text{box}, \text{landmark}}} \alpha_j \beta_i^j L_i^j$</p><p>Where:</p><ul><li>N is the total number of samples.</li><li>$\alpha_j$ represents the weight for each task. For <strong>P-Net</strong> and <strong>R-Net</strong>, the focus is on face classification $(\alpha_{\text{det}} = 1)$ and bounding box regression $(\alpha_{\text{box}} = 0.5)$, with less emphasis on key point localization $(alpha_{\text{landmark}} = 0.5)$. For <strong>O-Net</strong>, the weights are adjusted to focus more on key point localization $(\alpha_{\text{landmark}} = 1)$.</li><li>$\beta_i^j \in {0,1}$ indicates whether the sample participates in the loss computation for a specific task. If $\beta_i^j = 0$, the corresponding loss for that task is not computed.</li></ul><p>The loss is minimized using <strong>stochastic gradient descent (SGD)</strong>, and $L_i^j$ represents the computed loss for sample $x_i$ in task $j$. The total loss function reflects the combined optimization of face classification, bounding box regression, and key point localization.</p><h1 id="Face-Recognition-FaceNet"><a href="#Face-Recognition-FaceNet" class="headerlink" title="Face Recognition FaceNet"></a>Face Recognition FaceNet</h1><p>After completing the face detection process, the next step is <strong>face recognition</strong>. Here, face recognition serves as a general term for face-related tasks, which can be divided into the following four categories:</p><ol><li><strong>Face Tracking</strong>: Face tracking refers to following an individual within a room after detecting a family member. This is essential for providing targeted services to the individual.</li><li><strong>Face Verification</strong>: Face verification determines whether two faces appearing sequentially in the field of view belong to the same family member. This functionality is crucial for enabling directed services by confirming identity consistency.</li><li><strong>Face Identification</strong>: Face identification focuses on determining which member of the family (stored in the robot’s facial database) a detected face belongs to. It facilitates member identification. Additionally, when a new face (such as a visiting relative or friend) is detected, it allows the system to add the new face to the database and carry out subsequent actions accordingly.</li><li><strong>Face Clustering</strong>: Face clustering enhances the robustness of the robot. By collecting multiple facial images of family members under different angles, lighting conditions, and occlusions in the home environment, the robot can classify and organize these images into distinct clusters for more reliable recognition.</li></ol><p>These functions collectively enable the home service robot to handle a variety of face-related tasks, ensuring it operates effectively and adapts to dynamic household scenarios.</p><h2 id="Why-FaceNet"><a href="#Why-FaceNet" class="headerlink" title="Why FaceNet?"></a>Why FaceNet?</h2><p>Before the introduction of <strong>FaceNet</strong>, traditional face recognition methods based on convolutional neural networks (CNNs) primarily utilized <strong>Siamese networks</strong> for feature extraction, followed by classification using methods like SVM (Support Vector Machines).</p><p>The advent of <strong>FaceNet</strong> marked a significant milestone in face recognition. It achieved an impressive accuracy of <strong>99.63%</strong> on the widely used LFW (Labeled Faces in the Wild) dataset, setting a new record at the time. Additionally, it reached <strong>95.12%</strong> accuracy on the YouTube Faces database. The system reduced the error rate on these two datasets by 30%.</p><p>The core idea behind FaceNet is to directly learn the mapping of facial images into a multi-dimensional Euclidean space. The similarity between two facial images is then determined by the distance between their respective feature points in this Euclidean space. As shown in Figure 3, the numerical value between two face images represents the Euclidean distance between their feature points in the multi-dimensional space.</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.7zqhjdm1te.webp"                      alt="Figure 3: FaceNet Model result"                ></p><p>Face recognition models often face challenges due to variations in <strong>lighting</strong>, <strong>pose</strong>, and <strong>occlusion</strong>, which can significantly affect model performance. In Figure 3:</p><ul><li>The left and right images show two photos of the same person from different angles.</li><li>The top and bottom images represent photos of two different individuals.</li></ul><p>From the figure, it is evident that the Euclidean distance between images of the same person is less than <strong>1.1</strong>, while the distance between images of different individuals exceeds <strong>1.1</strong>. In this case, the intra-class distance is clearly smaller than the inter-class distance, allowing a threshold of approximately <strong>1.1</strong> to be set for determining whether two images belong to the same person.</p><p>This threshold-based approach significantly improves the robustness of face recognition, making it possible to handle variations in lighting, pose, and occlusion more effectively.</p><h2 id="How-FaceNet-works"><a href="#How-FaceNet-works" class="headerlink" title="How FaceNet works?"></a>How FaceNet works?</h2><p>Previous face recognition methods based on convolutional neural networks (CNNs) trained on facial images typically connected a <strong>classification layer</strong> at the end. For recognizing untrained faces, these methods relied on the bottleneck layer. However, this approach had significant drawbacks, including <strong>low efficiency</strong> and poor performance of the bottleneck layer when applied to new, unseen face images.</p><p>To improve the accuracy of face recognition, <strong>FaceNet</strong> introduced three key advantages:</p><ol><li><strong>Compact Feature Representation</strong>:<br>FaceNet represents facial images as a 128-dimensional vector. This dramatically reduces the dimensionality of the feature vector after global pooling, thereby significantly reducing the computational cost.</li><li><strong>Similarity Measurement via Euclidean Distance</strong>:<br>The similarity between different face images is measured using the <strong>Euclidean distance</strong> between their feature vectors. A smaller Euclidean distance indicates higher similarity between the two faces, while a larger distance indicates lower similarity.</li><li><strong>Triplet Loss for Classification</strong>:<br>FaceNet replaces the traditional <strong>Softmax layer</strong> with <strong>Triplet Loss</strong>, which directly classifies face embeddings. Based on a given threshold, the system determines whether two face images belong to the same person.</li></ol><p>Figure 4 illustrates the architecture of FaceNet. The pipeline is as follows:</p><ol><li><strong>Input Processing</strong>:<br>Facial images detected and cropped to a specified size by <strong>MTCNN</strong> are used as inputs to the model.</li><li><strong>Deep Learning Architecture</strong>:<br>The cropped images are passed into a deep learning architecture, such as <strong>GoogleNet</strong> or <strong>Zeiler&amp;Fergus networks</strong>, to extract features.</li><li><strong>Feature Normalization (L2 Normalization)</strong>:<br>The features output by the deep learning architecture are normalized using <strong>L2 normalization</strong>, mapping all feature vectors onto a hypersphere.</li><li><strong>Feature Embedding</strong>:<br>The normalized feature vectors, called <strong>embeddings</strong>, represent the input facial image.</li><li><strong>Triplet Loss for Classification</strong>:<br>Finally, the model uses <strong>Triplet Loss</strong> to classify the embeddings and determine whether two facial images represent the same person based on a predefined threshold.</li></ol><p>This approach enables FaceNet to perform face recognition with high efficiency and accuracy, making it robust in handling new facial data.</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.3d4uiorykf.webp"                      alt="Figure 4: FaceNet framework"                ></p><h3 id="Triplet-Loss-Function"><a href="#Triplet-Loss-Function" class="headerlink" title="Triplet Loss Function"></a>Triplet Loss Function</h3><h4 id="Similarity-via-Euclidean-Distance"><a href="#Similarity-via-Euclidean-Distance" class="headerlink" title="Similarity via Euclidean Distance"></a>Similarity via Euclidean Distance</h4><p>The similarity between two images is determined by the <strong>Euclidean distance</strong> between their feature vectors, as shown:<br>$d(x_1, x_2) = | f(x_1) - f(x_2) |_2^2$</p><p>Where:</p><ul><li>$f(x_1) and f(x_2)$ represent the mappings of images (x_1) and (x_2) onto a hypersphere in a (d)-dimensional Euclidean space.</li><li>$d(x_1, x_2)$ denotes the Euclidean distance between the two feature vectors.</li></ul><h4 id="Purpose-of-Triplet-Loss"><a href="#Purpose-of-Triplet-Loss" class="headerlink" title="Purpose of Triplet Loss"></a>Purpose of Triplet Loss</h4><p>The <strong>Triplet Loss</strong> function calculates the loss for three input images at once. As illustrated in Figure 5, each calculation involves three samples:</p><ol><li><strong>Anchor $x_a$</strong>: The target image.</li><li><strong>Positive $x_p$</strong>: An image of the same class as the Anchor.</li><li><strong>Negative M</strong>: An image of a different class from the Anchor.</li></ol><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.ic6cwkrxd.webp"                      alt="Figure 5: Triplet Loss example"                ></p><h4 id="Objectives-of-Triplet-Loss"><a href="#Objectives-of-Triplet-Loss" class="headerlink" title="Objectives of Triplet Loss"></a>Objectives of Triplet Loss</h4><ol><li><strong>Minimize</strong> the distance between the Anchor and Positive pair:<br>$d(x_a, x_p)$</li><li><strong>Maximize</strong> the distance between the Anchor and Negative pair:<br>$d(x_a, x_n)$</li></ol><p>The Triplet Loss function ensures that:</p><ul><li>Images of the same class are pushed closer together in the embedding space.</li><li>Images of different classes are pulled farther apart.</li></ul><p>This design enables the model to effectively distinguish between individuals by producing meaningful embeddings, making Triplet Loss a cornerstone of modern face recognition systems like <strong>FaceNet</strong>.</p><h4 id="Triplet-Loss-Function-Analysis-Intra-Class-and-Inter-Class-Distance"><a href="#Triplet-Loss-Function-Analysis-Intra-Class-and-Inter-Class-Distance" class="headerlink" title="Triplet Loss Function Analysis: Intra-Class and Inter-Class Distance"></a>Triplet Loss Function Analysis: Intra-Class and Inter-Class Distance</h4><p>Due to variations in <strong>pose</strong>, <strong>lighting</strong>, and <strong>occlusion</strong>, different photos of the same person may sometimes result in the Euclidean distance between the Anchor and Positive being greater than the distance between the Anchor and Negative. This is an undesirable situation.</p><p>As shown in Figure 6, before training, the distance between the Anchor and Negative might be smaller than the distance between the Anchor and Positive. After training, we aim to ensure that the <strong>intra-class distance</strong> (distance between Anchor and Positive) is significantly smaller than the <strong>inter-class distance</strong> (distance between Anchor and Negative). This relationship can be expressed mathematically as follows :</p><p>$$<br>| f(x_i^a) - f(x_i^p) |_2^2 + \alpha &lt; | f(x_i^a) - f(x_i^n) |_2^2, \quad \forall (x_i^a, x_i^p, x_i^n) \in T<br>$$</p><p>Where:</p><ul><li>$T$ is the set of all triplet combinations.</li><li>$x_i^a$, $x_i^p$, and $x_i^n$ represent the <strong>Anchor</strong>, <strong>Positive</strong>, and <strong>Negative</strong> samples, respectively, within the set $T$.</li><li>$\alpha$ is the margin between the intra-class and inter-class distances.</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.2rv6weq0ck.webp"                      alt="Figure 6: Triplet Loss Function"                ></p><h4 id="Final-Triplet-Loss-Function"><a href="#Final-Triplet-Loss-Function" class="headerlink" title="Final Triplet Loss Function"></a>Final Triplet Loss Function</h4><p>To ensure that all triplet combinations satisfy the above constraint, Equation can be transformed into the final <strong>Triplet Loss Function</strong>, as shown:</p><p>$$<br>L = \sum_{i=1}^N \left[ | f(x_i^a) - f(x_i^p) |_2^2 - | f(x_i^a) - f(x_i^n) |_2^2 \right]+ \alpha<br>$$</p><p>Where:</p><ul><li>$N$ is the total number of triplets.</li></ul><h4 id="Explanation"><a href="#Explanation" class="headerlink" title="Explanation"></a>Explanation</h4><p>The Triplet Loss function:</p><ol><li>Penalizes cases where the <strong>intra-class distance</strong> ($d(x_a, x_p)$) is not sufficiently smaller than the <strong>inter-class distance</strong> ($d(x_a, x_n)$).</li><li>Ensures that the model learns embeddings where samples of the same class are closer together and samples of different classes are farther apart, with a margin of at least $\alpha$.</li></ol><p>This formulation helps improve the robustness and accuracy of the model in face recognition tasks.</p><h4 id="Efficient-Triplet-Selection-During-Training"><a href="#Efficient-Triplet-Selection-During-Training" class="headerlink" title="Efficient Triplet Selection During Training"></a>Efficient Triplet Selection During Training</h4><p>During the training phase, it is necessary to continuously search for triplet combinations to compute the loss. However, in large datasets, <strong>exhaustively enumerating all triplets</strong> and calculating the triplet loss requires substantial computational resources. Additionally, many easily distinguishable images do not contribute meaningfully to the model’s convergence.</p><p>To address this problem, the authors of <a href="https://arxiv.org/abs/1503.03832">Schroff F, Kalenichenko D, Philbin J. <em>Facenet: A unified embedding for face recognition and clustering.</em> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015: 815-823</a> proposed an efficient solution:</p><ol><li><p>For a given image, identify the <strong>Hard Positive</strong>:<br>The least similar image belonging to the same person (i.e., the hardest to recognize as the same person).</p></li><li><p>Identify the <strong>Hard Negative</strong>:<br>The most similar image belonging to a different person (i.e., the hardest to distinguish as a different person).</p></li></ol><h4 id="Benefits-of-Hard-Example-Mining"><a href="#Benefits-of-Hard-Example-Mining" class="headerlink" title="Benefits of Hard Example Mining"></a>Benefits of Hard Example Mining</h4><p>By focusing on <strong>Hard Positives</strong> and <strong>Hard Negatives</strong>, the training process:</p><ul><li>Reduces computational overhead by avoiding redundant comparisons with easily distinguishable images.</li><li>Enhances the model’s ability to handle challenging cases, improving its robustness and convergence efficiency.</li></ul><p>This method ensures that the triplet loss focuses on the most informative and challenging samples, driving the model to better distinguish between subtle intra-class variations and inter-class similarities.</p><h1 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h1><p>In <strong>Figure 4</strong>, the term <strong>DEEP ARCHITECTURE</strong> refers to the deep learning framework. In the referenced paper, two neural network architectures were primarily used: <strong>Zeiler&amp;Fergus</strong>  and <strong>GoogleNet (Inception)</strong>.</p><h2 id="Face-Database-LFW-and-Recognition-Results"><a href="#Face-Database-LFW-and-Recognition-Results" class="headerlink" title="Face Database: LFW and Recognition Results"></a>Face Database: LFW and Recognition Results</h2><p> A well-known face database <strong>LFW (Labeled Faces in the Wild)</strong> will be tested in this part. The paper  provides recognition rates for various network architectures and dimensions, as shown in <strong>Table 1</strong>:</p><h3 id="Table-1-Recognition-Accuracy-for-Different-Network-Configurations"><a href="#Table-1-Recognition-Accuracy-for-Different-Network-Configurations" class="headerlink" title="Table 1. Recognition Accuracy for Different Network Configurations"></a>Table 1. Recognition Accuracy for Different Network Configurations</h3><table><thead><tr><th><strong>Architecture</strong></th><th><strong>Input Dimension</strong></th><th><strong>Recognition Accuracy (VAL)</strong></th></tr></thead><tbody><tr><td>NN1 (Zeiler&amp;Fergus)</td><td>220×220</td><td>$87.9% \pm 1.9$</td></tr><tr><td>NN2 (Inception)</td><td>224×224</td><td>$89.4% \pm 1.6$</td></tr><tr><td>NN3 (Inception)</td><td>160×160</td><td>$88.3% \pm 1.7$</td></tr><tr><td>NN4 (Inception)</td><td>96×96</td><td>$82.0% \pm 2.3$</td></tr><tr><td>NNS1 (Mini Inception)</td><td>—</td><td>$82.4% \pm 2.4$</td></tr><tr><td>NNS2 (Tiny Inception)</td><td>—</td><td>$51.9% \pm 2.9$</td></tr></tbody></table><hr><h3 id="Preprocessing-with-MTCNN-and-Training-Configuration"><a href="#Preprocessing-with-MTCNN-and-Training-Configuration" class="headerlink" title="Preprocessing with MTCNN and Training Configuration"></a>Preprocessing with MTCNN and Training Configuration</h3><p>In this study:</p><ol><li><p><strong>Preprocessing</strong>:<br>The images in the <strong>LFW dataset</strong> were first processed using the <strong>MTCNN model</strong>. The output consisted of 160×160 cropped facial images, which were used as inputs for training the <strong>FaceNet model</strong>.</p></li><li><p><strong>Updates to GoogleNet</strong>:<br>Over time, Google continued improving the GoogleNet architecture. This study primarily used <strong>Inception V4</strong> to process the dataset.</p></li><li><p><strong>Training Parameters</strong>:  </p><ul><li>Learning rate: $0.1$  </li><li>Exponential decay rate: $0.999$  </li><li>Epochs: $400$</li></ul></li></ol><p>Key Takeaways</p><ul><li>Larger input dimensions (e.g., 224×224 vs. 96×96) and more advanced architectures (e.g., Inception vs. Mini Inception) resulted in higher recognition accuracy. </li><li>Processing the dataset with <strong>MTCNN</strong> ensured high-quality cropped facial images, which improved the performance of the FaceNet model.  </li><li><strong>Inception V4</strong> and well-tuned hyperparameters significantly enhanced the model’s robustness and accuracy on the LFW dataset.</li></ul><p>The following screenshot will be the result or output of the model.</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.3nrobv6rdw.webp"                      alt="Figure 7: MTCNN for face detection on LFW"                ></p><p>From <strong>Figure 7</strong>, it can be observed that <strong>MTCNN</strong> successfully detected faces in all 13,233 images of the <strong>LFW dataset</strong>. The detected faces were then scaled and cropped to the required dimensions for subsequent processing.</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.7ax7ze2rf1.webp"                      alt="Figure 8: Facenet training results map on LFW dataset"                ></p><p><strong>Figure 8</strong> summarizes the performance of <strong>FaceNet</strong> on the LFW dataset, with the following key metrics:</p><ul><li><strong>Accuracy</strong>: $98.50%$</li><li><strong>Validation Rate</strong>: $90.06%$</li><li><strong>AUC (Area Under the Curve)</strong>: $0.998$</li><li><strong>EER (Equal Error Rate)</strong>: $0.016$</li></ul><h3 id="Insights"><a href="#Insights" class="headerlink" title="Insights:"></a>Insights:</h3><ul><li>An <strong>AUC</strong> value close to $1$ indicates excellent model performance.</li><li>A low <strong>EER</strong> of $0.016$ further confirms that FaceNet effectively meets the requirements of face recognition tasks.</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.wim3sl29q.webp"                      alt="Figure 9: The European -style distance matrix between three pictures"                ></p><p><strong>Figure 9</strong> presents a comparison of non-database facial images:</p><ul><li><strong>Images 0.jpg and 1.jpg</strong>: Two images of the same person taken from different angles.</li><li><strong>Image 11.jpg</strong>: An image of a different individual.</li></ul><p>All the images are of <strong>Asian faces</strong>. The results demonstrate:</p><ul><li><strong>Intra-class distances</strong> (same person) are consistently smaller than $0.8$.</li><li><strong>Inter-class distances</strong> (different people) are significantly larger than $1.0$.</li></ul><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion:"></a>Conclusion:</h3><p>These results show that FaceNet performs well even on <strong>Asian faces</strong>, maintaining clear separations between intra-class and inter-class distances. This indicates that the model is robust across diverse facial datasets.</p><p>Some other results:</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.5fkn84dj1a.webp"                      alt="image"                ></p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.9rjgfnx42u.webp"                      alt="image"                ></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Face-Recognition&quot;&gt;&lt;a href=&quot;#Face-Recognition&quot; class=&quot;headerlink&quot; title=&quot;Face Recognition&quot;&gt;&lt;/a&gt;Face Recognition&lt;/h1&gt;&lt;p&gt;Face Recogniti</summary>
      
    
    
    
    
    <category term="Robotics" scheme="http://example.com/tags/Robotics/"/>
    
    <category term="HCI" scheme="http://example.com/tags/HCI/"/>
    
    <category term="CNN" scheme="http://example.com/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>My first cv project, from ID card to face recognition</title>
    <link href="http://example.com/2024/12/12/My-first-university-project-ID-card-number-extraction/"/>
    <id>http://example.com/2024/12/12/My-first-university-project-ID-card-number-extraction/</id>
    <published>2024-12-12T03:29:16.000Z</published>
    <updated>2024-12-15T11:32:16.426Z</updated>
    
    <content type="html"><![CDATA[<h1 id="My-first-cv-project-Extract-ID-card-number"><a href="#My-first-cv-project-Extract-ID-card-number" class="headerlink" title="My first cv project: Extract ID card number"></a>My first cv project: Extract ID card number</h1><p>My CV story starts with a cousrse project when I was in sophomore. The final personal project can be selected from the following choices: 24-point game program, Tetris, ID card number extraction, face recognition by opencv, License plate number extraction. Actually, I have finished the first two in other courses and face recognition(CNN, will be introduced in the following part) was my parallel project directed by another professor, so only two choices:  ID card number extraction or License plate number extraction. They were same to me, so I chose ID card number extraction. </p><p>Let’s start!</p><h2 id="Development-Process"><a href="#Development-Process" class="headerlink" title="Development Process"></a>Development Process</h2><h3 id="Step-1-Development-Environment"><a href="#Step-1-Development-Environment" class="headerlink" title="Step 1: Development Environment"></a>Step 1: Development Environment</h3><p>The project uses:</p><ul><li><strong>OpenCV</strong>: A library for working with images.</li><li><strong>NumPy</strong>: For numerical operations like creating matrices.</li><li><strong>Pytesseract</strong>: A library to perform OCR (Optical Character Recognition) to extract text from images.</li></ul><h3 id="Step-2-Loading-the-Image"><a href="#Step-2-Loading-the-Image" class="headerlink" title="Step 2: Loading the Image"></a>Step 2: Loading the Image</h3><p><code>image = cv2.imread(&#39;test1.jpg&#39;)</code></p><p>test1.jpg will be the ID card, which is from the Internet.</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.m4lbulzo.webp"                      alt="image"                ></p><h3 id="Step-3-Finding-the-Area-with-the-ID-Number-Region-of-Interest"><a href="#Step-3-Finding-the-Area-with-the-ID-Number-Region-of-Interest" class="headerlink" title="Step 3: Finding the Area with the ID Number (Region of Interest)"></a>Step 3: Finding the Area with the ID Number (Region of Interest)</h3><p><code>height, width = image.shape[:2] roi = image[int(0.75 * height):height, int(0.1 * width):int(0.9 * width)]</code> The program crops the bottom part of the image, where the ID number is expected. It assumes the ID number is located in a specific area (e.g., the bottom center of the image). </p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.8ojr0exryr.webp"                      alt="image"                ></p><h3 id="Step-4-Cleaning-the-Image"><a href="#Step-4-Cleaning-the-Image" class="headerlink" title="Step 4: Cleaning the Image"></a>Step 4: Cleaning the Image</h3><p>Before extracting text, the program prepares the image to make it easier for Tesseract to read.</p><ol><li><strong>Convert to Grayscale</strong>: Removes color, leaving only shades of gray.</li></ol><p><code>gray_image = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)</code></p><ol start="2"><li><strong>Blur the Image</strong>: Reduces noise by smoothing the image.</li></ol><p><code>blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)</code></p><ol start="3"><li><strong>Thresholding</strong>: Converts the image into pure black and white.</li></ol><p><code>_, thresholded_image = cv2.threshold(blurred_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)</code></p><ol start="4"><li><strong>Morphological Cleaning</strong>: Removes small imperfections (like dots or specks) in the image.</li></ol><p><code>kernel = np.ones((3, 3), np.uint8) cleaned_image = cv2.morphologyEx(thresholded_image, cv2.MORPH_CLOSE, kernel)</code></p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.4ckxsvk4lh.webp"                      alt="image"                ></p><h3 id="Step-5-Extracting-the-Text"><a href="#Step-5-Extracting-the-Text" class="headerlink" title="Step 5: Extracting the Text"></a>Step 5: Extracting the Text</h3><p><code>extracted_text = pytesseract.image_to_string(cleaned_image, config=custom_config)</code></p><p><strong>Tesseract OCR</strong> reads the cleaned image and tries to find text. And The <code>config</code> setting tells Tesseract to assume the text is in a single block and to use a specific engine mode.</p><h3 id="Step-6-Extracting-Only-Numbers"><a href="#Step-6-Extracting-Only-Numbers" class="headerlink" title="Step 6: Extracting Only Numbers"></a>Step 6: Extracting Only Numbers</h3><p><code>id_number = &#39;&#39;.join(filter(str.isdigit, extracted_text))</code></p><p>If the extracted text is too long, it tries to trim it to 18 digits (the typical length of a Chinese ID number).</p><h3 id="Step-7-Displaying-Results"><a href="#Step-7-Displaying-Results" class="headerlink" title="Step 7: Displaying Results"></a>Step 7: <strong>Displaying Results</strong></h3><p>Finally, the program displays the extracted number. If the result doesn’t look correct, it gives a warning. It also shows the intermediate images (like the cropped region and cleaned image) for debugging purposes.</p><p>The result will be “Extracted ID Number: 310109199107141011” which is the same as the ID number.</p><p>All done!</p><p>If the ID number can be extracted, and the <a class="link"   href="https://allenzhuer.github.io/2024/12/14/Face-Recognition/" >face recognition<i class="fas fa-external-link-alt"></i></a> can be used too, then a KYC(know your customers) system can be realized. </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;My-first-cv-project-Extract-ID-card-number&quot;&gt;&lt;a href=&quot;#My-first-cv-project-Extract-ID-card-number&quot; class=&quot;headerlink&quot; title=&quot;My first</summary>
      
    
    
    
    
    <category term="Robotics" scheme="http://example.com/tags/Robotics/"/>
    
    <category term="opencv" scheme="http://example.com/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>The road to Navigation</title>
    <link href="http://example.com/2024/12/12/The-road-to-Navigation/"/>
    <id>http://example.com/2024/12/12/The-road-to-Navigation/</id>
    <published>2024-12-12T03:28:35.000Z</published>
    <updated>2024-12-20T02:03:34.355Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Global-path-planning-algorithm"><a href="#Global-path-planning-algorithm" class="headerlink" title="Global path planning algorithm"></a>Global path planning algorithm</h1><h2 id="Dijkstra"><a href="#Dijkstra" class="headerlink" title="Dijkstra"></a>Dijkstra</h2><h3 id="What-is-Dijkstra’s-Algorithm"><a href="#What-is-Dijkstra’s-Algorithm" class="headerlink" title="What is Dijkstra’s Algorithm?"></a><strong>What is Dijkstra’s Algorithm?</strong></h3><p>Dijkstra’s algorithm is fundamental in robotics, especially for path planning, where robots need to find the shortest path from one point to another.</p><p>Dijkstra’s algorithm finds the shortest path between nodes in a graph. In robotics, nodes represent positions, and edges (connections between nodes) represent possible paths with associated costs (distances, time, or effort).</p><h2 id="A"><a href="#A" class="headerlink" title="A*"></a>A*</h2><h1 id="Local-path-planning-algorithm"><a href="#Local-path-planning-algorithm" class="headerlink" title="Local path planning algorithm"></a>Local path planning algorithm</h1><h2 id="DWA"><a href="#DWA" class="headerlink" title="DWA"></a>DWA</h2><h2 id="TEB"><a href="#TEB" class="headerlink" title="TEB"></a>TEB</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Global-path-planning-algorithm&quot;&gt;&lt;a href=&quot;#Global-path-planning-algorithm&quot; class=&quot;headerlink&quot; title=&quot;Global path planning algorithm&quot;&gt;</summary>
      
    
    
    
    
    <category term="Robotics" scheme="http://example.com/tags/Robotics/"/>
    
    <category term="Path Planning" scheme="http://example.com/tags/Path-Planning/"/>
    
    <category term="Navigation" scheme="http://example.com/tags/Navigation/"/>
    
  </entry>
  
  <entry>
    <title>Bridging Technology and Everyday Life, Building a Home Environment Reception Robot</title>
    <link href="http://example.com/2024/11/25/Graduation-project-Mobile-robot-interaction-design-in-the-home-environment/"/>
    <id>http://example.com/2024/11/25/Graduation-project-Mobile-robot-interaction-design-in-the-home-environment/</id>
    <published>2024-11-25T02:29:14.000Z</published>
    <updated>2024-12-15T14:47:12.896Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>In an era where artificial intelligence and robotics are reshaping industries, the promise of home robots stands out as a beacon of innovation. Imagine a robot that can assist with household chores, recognize family members, engage in meaningful conversations, and adapt to dynamic environments. This vision inspired my graduation thesis, where I designed and implemented a mobile robot system tailored to interact seamlessly within a home environment. </p><p>A few years later, I learned that my project and work were used as teaching materials and the foundation for the teams from the School of Automation, Southeast University, to participate in the RoboCup@Home China. I’m very proud that my work can contribute to the competition. This also means my desire to dive deep into Human-Computer Interaction to combine cutting-edge technologies and business to create inspired products and services that solve customers’ problems.</p><blockquote><p>This is a list of the current desired technical abilities that the tests in RoboCup@Home will focus on.</p><ul><li>Navigation in dynamic environments </li><li>Fast and easy calibration and setup: The ultimate goal is to have a robot up and running out of the box</li><li>Object recognition </li><li>Object manipulation </li><li>Detection and Recognition of Humans </li><li>Natural human-robot interaction </li><li>Speech recognition </li><li>Gesture recognition </li><li>Robot applications RoboCup@Home is aiming for applications of robots in daily life.</li></ul></blockquote><p>This project aims to develop highly relevant service and assistive robot technology for future personal domestic applications. The focus lies on the following domains but is not limited to: Human-Robot Interaction and Cooperation, Navigation and Mapping in home environments, Computer Vision and Object Recognition under natural light conditions, Object Manipulation, Adaptive Behaviors, Standardization, and System Integration.</p><h1 id="Core-Functions"><a href="#Core-Functions" class="headerlink" title="Core Functions"></a>Core Functions</h1><h2 id="Scenario-Welcoming-Friends-at-Home-with-the-Robot-Assistant"><a href="#Scenario-Welcoming-Friends-at-Home-with-the-Robot-Assistant" class="headerlink" title="Scenario: Welcoming Friends at Home with the Robot Assistant"></a>Scenario: Welcoming Friends at Home with the Robot Assistant</h2><p>It’s a quiet afternoon at home, and you’ve invited a couple of friends over to relax and catch up. Just as you hear the doorbell ring, your robot assistant springs to life, ready to help create a seamless and welcoming experience.</p><h3 id="Step-1-Greeting-the-Guests"><a href="#Step-1-Greeting-the-Guests" class="headerlink" title="Step 1:  Greeting the Guests"></a>Step 1:  Greeting the Guests</h3><p>As your first friend steps into the house, the robot assistant moves forward to greet them. Its camera focuses on their face, and it says warmly:<br><em>“Hello, welcome! May I know your name?”</em><br>Your friend replies, “I’m Alex.” The robot repeats their name to confirm and then adds:<br><em>“It’s great to meet you, Alex. May I ask, what’s your favorite drink?”</em><br>Alex responds with a smile, “I love Cola.” The robot confirms their choice and saves their details, associating the name and drink preference with their face.</p><p>When your second friend arrives, the robot repeats the same process, efficiently recognizing and recording their name and drink preference as well.</p><p>If one of your friends is a frequent visitor, the robot has already  recognized them instantly then will greet them with personalized messages like:<br><em>“Welcome back, Alex! Would you like the usual Cola today?”</em> </p><h3 id="Step-2-Guiding-the-Guests-to-the-Living-Room"><a href="#Step-2-Guiding-the-Guests-to-the-Living-Room" class="headerlink" title="Step 2: Guiding the Guests to the Living Room"></a>Step 2: Guiding the Guests to the Living Room</h3><p>Once both friends are inside, the robot gestures and says:<br><em>“Please follow me, and I’ll guide you to your seats.”</em><br>It smoothly navigates through the house, leading your friends to the living room. It identifies the unoccupied seats and gestures politely:<br><em>“Here are two comfortable seats for you. Please, have a seat.”</em></p><h3 id="Step-3-Personalized-Drink-Delivery"><a href="#Step-3-Personalized-Drink-Delivery" class="headerlink" title="Step 3: Personalized Drink Delivery"></a>Step 3: Personalized Drink Delivery</h3><p>After your friends are seated, the robot turns to you for a moment and confirms:<br><em>“Would you like me to prepare the drinks for Alex and Jamie?”</em><br>You nod, and the robot replies, <em>“Understood. Please relax while I handle it.”</em></p><p>The robot moves to the kitchen, identifies the correct drinks, and carefully retrieves them. It returns to the living room and delivers the drinks to each guest by name:<br><em>“Alex, here’s your Cola. Jamie, here’s your favorite soda. Enjoy!”</em></p><p>Even if the robot can’t physically pick up the drinks, it uses a clear gesture or light indicator to direct your guests to their beverages, ensuring a thoughtful touch.</p><h3 id="Step-4-Creating-a-Relaxed-Atmosphere"><a href="#Step-4-Creating-a-Relaxed-Atmosphere" class="headerlink" title="Step 4: Creating a Relaxed Atmosphere"></a>Step 4: <strong>Creating a Relaxed Atmosphere</strong></h3><p>As the conversation flows, the robot subtly monitors the interaction, staying on standby in case anyone needs assistance. Its graphical user interface, displayed on a nearby screen, shows a friendly message such as:<br><em>“Let me know if you need anything else!”</em></p><p>If your friends move around, the robot adjusts and remains responsive, making sure they feel at home.</p><h2 id="Product-Function-and-Modules"><a href="#Product-Function-and-Modules" class="headerlink" title="Product Function and Modules"></a>Product Function and Modules</h2><p>The product’s core functionalities include the following five aspects:</p><ol><li><strong>Visitor Identification Based on Visual Recognition</strong>: Assuming there are two visitors, each arrives and pauses in front of the robot upon entry. The robot’s camera detects and captures their faces, automatically recognizes them, and saves the visitors’ facial images.</li><li><strong>Voice-Based Greeting Function</strong>: After detecting the visitor’s face, the robot provides a voice greeting and asks for the visitor’s name. It confirms the name by repetition and records it. The robot then asks for the visitor’s favorite beverage, confirms it, and records the information. Both the name and the preferred beverage are linked to the facial image, with the image file named after the visitor’s name.</li><li><strong>Seating Guidance Function</strong>: The robot directs the guest to an unoccupied seat. It issues a voice prompt, “Please follow me,” and leads the guest to the available spot, arranging for them to sit.</li><li><strong>Beverage Delivery Function</strong>: After the guest is seated, the robot moves to the beverage station, picks up the guest’s preferred drink (represented by an empty bottle if necessary), and delivers it to the correct guest. (If the grasping function is not feasible, the robot can instead point to the beverage.)</li><li><strong>Graphical User Interface Control</strong>: A graphical user interface is implemented to control the entire process.</li></ol><h4 id="Additional-Potential-Enhancements"><a href="#Additional-Potential-Enhancements" class="headerlink" title="Additional Potential Enhancements:"></a>Additional Potential Enhancements:</h4><ul><li>Adding object recognition to enable functionalities like object grasping and delivery.</li></ul><p>To meet the basic requirements, the product requires at least five functional modules: a camera detection module, a microphone acquisition module, a path planning module, a trajectory optimization module, and an arms control module. The correspondence between functions and modules is shown in <strong>Table 1</strong>.</p><h3 id="Table-1-Functions-and-Modules"><a href="#Table-1-Functions-and-Modules" class="headerlink" title="Table 1:  Functions and Modules"></a>Table 1:  Functions and Modules</h3><table><thead><tr><th>Function</th><th>Camera Detection Module</th><th>Microphone Acquisition Module</th><th>Path Planning Module</th><th>Trajectory Optimization Module</th><th>Arm Control Module</th></tr></thead><tbody><tr><td>Visitor identification based on face recognition</td><td>✓</td><td></td><td></td><td></td><td></td></tr><tr><td>Voice-based greeting function</td><td></td><td>✓</td><td></td><td></td><td></td></tr><tr><td>Seating guidance function</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td></td></tr><tr><td>Beverage delivery and identification</td><td>✓</td><td></td><td></td><td>✓</td><td>✓</td></tr><tr><td>Graphical interface control</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr></tbody></table><h1 id="Design-and-Methodology"><a href="#Design-and-Methodology" class="headerlink" title="Design and Methodology"></a>Design and Methodology</h1><h2 id="System-Overview"><a href="#System-Overview" class="headerlink" title="System Overview"></a>System Overview</h2><p>The system is built on a Turtlebot platform, enhanced with a HOKUYO URG-04LX-UG01 laser radar and an RGB-D camera for advanced functionality. Facial detection and recognition use MTCNN and FaceNet models, trained and fine-tuned on NVIDIA 2080Ti GPUs or Google Colab.</p><h4 id="Development-Environment"><a href="#Development-Environment" class="headerlink" title="Development Environment"></a><strong>Development Environment</strong></h4><ul><li><strong>OS</strong>: Ubuntu 16.04</li><li><strong>Frameworks</strong>: ROS for Turtlebot development, CUDA 9.0, and TensorFlow 1.7 for model training.</li></ul><h4 id="Final-Deployment"><a href="#Final-Deployment" class="headerlink" title="Final Deployment"></a><strong>Final Deployment</strong></h4><p>The fully developed system was successfully implemented on the MORO dual-arm robot, seamlessly integrating navigation, facial recognition, and robotic control. Since the MORO platform was still at the very beginning stage then, the grasp tasks were replaced by pointing to the drinks, and the KUKA LBR IIWA robot arm and RGBD camera realized the grasp tasks. </p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.8ojr4t0mou.webp"                      alt="MORO II robot"                ></p><h2 id="Visitor-identification"><a href="#Visitor-identification" class="headerlink" title="Visitor identification"></a>Visitor identification</h2><p>The visitor identification task module primarily uses an <strong>RGB-D camera</strong> to actively capture images for face detection and recognition. The goal is to identify whether a detected face is new or already stored in the system database and transition seamlessly to the inquiry task once a face is recognized.</p><h4 id="Process-Flow"><a href="#Process-Flow" class="headerlink" title="Process Flow"></a><strong>Process Flow</strong></h4><ol><li><p><strong>Initialization</strong>:</p><ul><li>The robot begins in a stationary position at the door.</li><li>The <strong>camera thread</strong> is activated to continuously capture frames.</li></ul></li><li><p><strong>Face Detection</strong>:</p><ul><li>Each frame is analyzed for face detection with a time limit of <strong>3 seconds</strong> per detection attempt.</li><li>If no face is detected within 3 seconds, the robot performs <strong>small rotational movements</strong> to expand its field of view and continues detecting.</li></ul></li><li><p><strong>Recognition and Database Check</strong>:</p><ul><li>If a face is detected:<ul><li>The system marks this recognition task as successful.</li><li>It checks the database for the detected face:<ul><li>If the face is new, the calculated facial feature data is extracted and stored in the database.</li></ul></li></ul></li></ul></li><li><p><strong>Failure Handling</strong>:</p><ul><li><p>If no face is detected after </p><p>3 seconds of rotation</p><p>, the system:</p><ul><li>Confirms whether any face was found during the current session.</li><li>If no face was found:<ul><li>All threads are stopped, and the robot pauses operations for 3 seconds before restarting the recognition task.</li></ul></li><li>If a face <strong>was found</strong>, the recognition task concludes, and the system transitions to the <strong>inquiry task</strong>.</li></ul></li></ul><p>(The chart is genrated by ChatGPT.)</p></li></ol><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/Face_recognition.2kryc1wfy6.svg"                      alt="Visitor identification"                ></p><p>Face Recognition will be introduced in a new blog for the learning path in <a class="link"   href="https://allenzhuer.github.io/2024/12/14/Face-Recognition/" >Face Recognition<i class="fas fa-external-link-alt"></i></a>. </p><h2 id="Voice-based-greeting-function"><a href="#Voice-based-greeting-function" class="headerlink" title="Voice-based greeting function"></a>Voice-based greeting function</h2><p>The IFLYTEK voice transcription realizes a voice-based greeting function to obtain voice information, which will be stored in the database with face information and personal information. (I have not learned NLP then, so have to turn to open-source tech or tools for help.)</p><p>The inquiry task module leverages a <strong>microphone array</strong>, an <strong>RGB-D camera</strong>, and a <strong>speech output module</strong> to interact with household members and collect their preferences. For example, the robot might ask, <em>“What’s your favorite drink?”</em> Upon receiving a response, it converts the speech to text, detects keywords (e.g., Sprite, Coke, milk tea), and stores the data for future use.</p><p><strong>Process Flow</strong></p><ol><li><strong>Module Initialization</strong>:<ul><li>Start the <strong>RGB-D camera thread</strong> and <strong>microphone array thread</strong>.</li><li>Rotate the camera to search for faces.</li></ul></li><li><strong>Face Detection</strong>:<ul><li>Once a single face is detected, the camera aligns to keep the face centered.</li></ul></li><li><strong>Interactive Questioning</strong>:<ul><li>The robot asks: <em>“What’s your name?”</em> Processes and saves the name.</li><li>Then asks: <em>“What’s your favorite drink?”</em> Transcribes the response and extracts key preferences.</li></ul></li><li><strong>Data Storage</strong>:<ul><li>The system links the detected face with the name and preferences.</li><li>Packages the data into a structured record in the database.</li></ul></li><li><strong>Repeating the Process</strong>:<ul><li>After one member’s information is collected, the system checks for other undetected faces.</li><li>If faces remain, the camera rotates to locate another person and repeats the inquiry process.</li></ul></li><li><strong>Task Completion</strong>:<ul><li>When all faces have been detected and information collected, the module concludes the inquiry task and transitions to the <strong>guidance task</strong>.</li></ul></li></ol><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.6wqrjom36d.webp"                      alt="Voice-based greeting function"                > </p><p>(The chart is genrated by ChatGPT.)</p><h2 id="Guidance-function"><a href="#Guidance-function" class="headerlink" title="Guidance function"></a>Guidance function</h2><p>The primary functionalities required for this task include <strong>Marker search and recognition</strong>, <strong>motion planning</strong>, and <strong>obstacle avoidance</strong>. Each function works in harmony to ensure the robot can execute tasks efficiently and safely. The markers are used for turtlebot, and it can be replaced by pre-programming in Moro robot system to guide to the pre-defined places. </p><p>You can visit the blog regrading to the Motion planning and Obstacle avoidance to learn the specific algorithms.  <em><strong>TODO: Motion planning blog links.</strong></em></p><h4 id="Process-flow"><a href="#Process-flow" class="headerlink" title="Process flow"></a><strong>Process flow</strong></h4><ol><li>The robot initializes the task by scanning for the first marker to determine its starting point.</li><li>Upon detecting a marker:<ul><li>The marker’s information is decoded to determine the next action.</li><li>The robot plans its route to the specified target area.</li></ul></li><li>During navigation:<ul><li>Obstacles are detected and avoided using real-time LiDAR data.</li><li>The robot updates its path dynamically to ensure smooth movement.</li></ul></li><li>If the robot reaches the final marker, it concludes the task or transitions to a new task as per the marker’s instructions.</li></ol><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.60ua4ckstu.webp"                      alt="Guidance function"                ></p><h2 id="Grasp-Function"><a href="#Grasp-Function" class="headerlink" title="Grasp Function"></a>Grasp Function</h2><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.3yei67cre4.webp"                      alt="image"                ></p><h1 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h1><h2 id="Face-Recognition"><a href="#Face-Recognition" class="headerlink" title="Face Recognition"></a>Face Recognition</h2><p><strong>Face Recognition with LFW Dataset and MTCNN</strong></p><ol><li><p><strong>MTCNN Training on LFW Dataset</strong></p><ul><li>The LFW dataset was used to train MTCNN for face detection.</li><li>The dataset was divided into 80% for training and 20% for testing and validation, along with a custom-built face dataset.</li><li>The final detection accuracy was 96.4%.</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.5fkn84dj1a.webp"                      alt="MTCNN Examples"                ></p></li><li><p><strong>FaceNet Training with Processed Images</strong></p><ul><li>All images in the LFW dataset were resized to 160×160 pixels using MTCNN preprocessing.</li><li>The processed images were then fed into the FaceNet model for training.</li><li>The trained FaceNet model achieved a recognition accuracy of 98.5%.</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.9rjgfnx42u.webp"                      alt="FaceNet examples"                ></p></li><li><p><strong>Challenges in Recognition Accuracy</strong></p><ul><li>Misrecognitions occurred in two primary scenarios:<br>a. When an image contained two or more faces, the target face might not be selected.<br>b. Poor lighting, occlusions, or unfavorable angles reduced the confidence score of the target face compared to others.</li></ul></li><li><p><strong>Example of MTCNN Misrecognition</strong></p><ul><li>In one example, the male face had a confidence score of 0.78, while the female face had a score of 0.85.</li><li>The target was the male face, but due to occlusion, MTCNN detected and cropped the female face instead.</li><li>As a result, the male face was excluded.</li></ul><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.5tr2yznm48.webp"                      alt="MTCNN Misrecognition example"                ></p></li></ol><p><strong>Conclusion:</strong><br>MTCNN and FaceNet demonstrate high accuracy for face detection and recognition. However, challenges like multiple faces, occlusions, and lighting variations can lead to misrecognitions, indicating areas for further improvement.</p><h2 id="Voice-Greating-Function"><a href="#Voice-Greating-Function" class="headerlink" title="Voice Greating Function"></a>Voice Greating Function</h2><p><strong>Inquiry Module</strong></p><p>The inquiry module primarily utilizes the <strong>iFlytek speech-to-text API</strong>. This API allows for the transcription of a local <code>.wav</code> audio file into text. For example, a local file named <code>input.wav</code> can be transcribed into text such as, <em>“我是猪八戒，我喜欢喝可乐”</em> (translated: “I am Zhu Bajie, and I like drinking cola”). An example of this process is shown in the following Figures.</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.6wqs9vobth.webp"                      alt="image"                ></p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.1ovhmvvw4a.webp"                      alt="image"                ></p><p>The two images above illustrate the process of recording audio for family members and saving it locally as the <code>input.wav</code> audio file. The workflow is as follows:</p><ol><li><strong>Audio Recording</strong>: Family members’ voices are recorded and saved as a <code>.wav</code> file (e.g., <code>input.wav</code>).</li><li><strong>Speech Transcription</strong>: The audio file is transcribed into text using the iFlytek API.</li><li><strong>Keyword Extraction</strong>: Keywords, such as names and preferences, are extracted from the transcribed text and saved locally in the <code>mine.txt</code> file.</li><li><strong>Image Naming</strong>: Collected facial images are linked to the corresponding names from the transcribed data and renamed accordingly.</li></ol><p>This integrated approach ensures that each family member’s face and preferences are organized systematically for further use.</p><h2 id="The-whole-process"><a href="#The-whole-process" class="headerlink" title="The whole process"></a>The whole process</h2><p>To evaluate the overall effectiveness of integrating all components, the following images illustrate the testing process for the entire task workflow:</p><ol><li><p><strong>Face Detection and Recognition</strong>:<br>As shown in <strong>Figure (a)</strong>, when the target person enters the robot’s field of view, the robot captures images from the video feed via its camera. The system then performs face detection and recognition, extracting the facial information of the person and storing it in a local database. Once the facial information is obtained, the system issues a command to change the task state and transitions to the voice interaction module.</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.2rv6xrwxl3.webp"                      alt="Figure a"                ></p></li><li><p><strong>Voice Interaction</strong>:<br>In <strong>Figure (b)</strong>, the microphone prompts the target individual with a question: “Hello, what is your name?” The person responds, and the system attempts to recognize the spoken name. If the name is not accurately recognized, the system either repeats the question or confirms the name by ensuring that the same name is detected twice consecutively. If there is a mismatch, the system continues to ask until two consistent results are obtained. After obtaining the name, the system proceeds to inquire about the person’s favorite beverage. This process is similar to the name inquiry, and the results are displayed in <strong>Figure (c)</strong>.</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.1ap1w0tddq.webp"                      alt="Figure b"                ></p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.1ap1w0tddq.webp"                      alt="Figure c"                ></p></li><li><p><strong>Saving Information to the Database</strong>:<br>Once all information is collected, the system saves it in the database. As shown in <strong>Figure (f)</strong>, the database includes details such as the member’s ID, name, age, preferences, and facial data. The collected facial images are named according to the person’s name and saved locally.</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.2yyet7ldqq.webp"                      alt="Figre f"                ></p></li><li><p><strong>Proposing Personalized Tasks</strong>:<br>After completing the inquiries, the robot addresses the person by their name and makes a proposal, such as: “Wang Dapeng, would you like a cup of milk tea?” This question is formatted as: “Target name, would you like a cup of [favorite beverage]?” An example of this interaction is shown in <strong>Figure (d)</strong></p></li></ol><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.73u05bhlfx.webp"                      alt="Figure d"                ></p><ol start="5"><li><p><strong>Re-encountering a Recognized Member</strong>:<br>When a previously recognized member reappears in the robot’s field of view, such as after registering “Wang Dapeng” and later encountering another person “Liu Tao,” the system logs their information as new. However, when “Wang Dapeng” appears again, the system recognizes them as an existing member and generates an “old” message to indicate prior recognition. The system then repeats the personalized interaction, as it did during the first encounter, by asking: “Wang Dapeng, would you like a cup of milk tea?” The result of this process is shown in <strong>Figure (e)</strong>.</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://allenzhuer.github.io/picx-images-hosting/image.67xipv8ldf.webp"                      alt="Figure e"                ></p></li></ol><h1 id="Improvements"><a href="#Improvements" class="headerlink" title="Improvements"></a>Improvements</h1><p>As described above, this paper presents the design of a simple home service robot system that performs several tasks. However, there are still many limitations for home service robots, which can be summarized as follows:</p><ol><li><strong>Communication Delays</strong>:<br>The communication delay between the face recognition and voice recognition modules is relatively long. In the future, the adoption of <strong>5G transmission</strong> could significantly improve the speed of data transfer, especially when dealing with large datasets.</li><li><strong>Misidentification in Multi-Face Scenarios</strong>:<br>When multiple faces appear in the robot’s field of view, the system may misidentify the target face. Future improvements in algorithms, such as the adoption of <strong>3D face recognition</strong>, could greatly enhance the accuracy of identification.</li><li><strong>Limited System Functionality</strong>:<br>The system designed in this study is relatively simple and performs only basic, single-purpose tasks. It is not yet suitable for complex home environments. In the future, home service robots will likely become <strong>smarter and more versatile</strong>, capable of completing a wide range of tasks quickly and efficiently.</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;In an era where artificial int</summary>
      
    
    
    
    
    <category term="Robotics" scheme="http://example.com/tags/Robotics/"/>
    
    <category term="HCI" scheme="http://example.com/tags/HCI/"/>
    
  </entry>
  
  <entry>
    <title>How to Analyze DeFi Projects</title>
    <link href="http://example.com/2023/09/12/How-to-Analyze-DeFi-Projects/"/>
    <id>http://example.com/2023/09/12/How-to-Analyze-DeFi-Projects/</id>
    <published>2023-09-12T02:45:18.000Z</published>
    <updated>2023-10-27T05:02:04.262Z</updated>
    
    <content type="html"><![CDATA[<h1 id="How-to-Analyze-DeFi-Projects"><a href="#How-to-Analyze-DeFi-Projects" class="headerlink" title="How to Analyze DeFi Projects"></a>How to Analyze DeFi Projects</h1><p><strong>Introduction:</strong></p><ul><li>This article focuses on explaining the thought process and not on commenting on or analyzing any specific project.</li><li>Before diving into a deep analysis, understanding the fundamentals of a DeFi project is extremely important.</li><li>Tracking popular metrics of DeFi protocols can help investors assess the health of the project and evaluate whether it is undervalued or overvalued.</li><li>By understanding token economics, investors can make wiser decisions about which tokens to buy, sell, and hold.</li><li>The technology underpinning DeFi projects is a key factor in determining its feasibility and effectiveness.</li><li>The team’s pursuit of security is of paramount importance.</li><li>Investors need to be able to identify projects that align with the use cases they believe in and evaluate the team behind them.</li><li>The strength of a project’s community and the extent to which it is adopted by users are also important indicators of success.</li></ul><h2 id="Introduction-to-DeFi"><a href="#Introduction-to-DeFi" class="headerlink" title="Introduction to DeFi"></a>Introduction to DeFi</h2><p>The explosive growth of Decentralized Finance (DeFi) has fundamentally transformed the financial field, offering a variety of innovative products and services. As a result, individuals can now access unprecedented financial services and products without having to go through intermediaries like banks or traditional financial institutions.</p><p>DeFi has made finance more accessible, allowing anyone with an internet connection to participate in activities like lending, trading, and earning interest on their digital assets. However, as more and more projects enter the market, understanding how to effectively analyze DeFi projects has become crucial. This means that investors should conduct thorough due diligence on a project before acquiring its tokens.</p><h2 id="Understanding-the-Basics-of-DeFi"><a href="#Understanding-the-Basics-of-DeFi" class="headerlink" title="Understanding the Basics of DeFi"></a>Understanding the Basics of DeFi</h2><p>Before diving into a deep analysis, understanding the fundamentals of a DeFi project is essential. DeFi refers to financial applications built on blockchain networks, aiming to offer open, permissionless, and trustless alternatives to traditional financial services. These projects usually provide services like lending, decentralized exchanges, and liquidity mining.</p><p>Considering the open and permissionless nature of this technology, various individuals and teams are building applications that cover many use cases, such as giving users unprecedented control over their personal data and complete ownership of their digital assets. For those new to the DeFi field, they should approach these projects with caution and prudence.</p><h2 id="Why-is-Due-Diligence-Risk-Management-in-DeFi-Important"><a href="#Why-is-Due-Diligence-Risk-Management-in-DeFi-Important" class="headerlink" title="Why is Due Diligence (Risk Management) in DeFi Important?"></a>Why is Due Diligence (Risk Management) in DeFi Important?</h2><p>Due to the unique nature and risks of the rapidly evolving field of DeFi, conducting due diligence before investing or purchasing tokens is extremely important. DeFi investments often come with risks such as potential scams, hacking attacks, and the loss of part or all of the investor’s principal. By thoroughly evaluating a project’s fundamentals, team, technology, and security measures, one can avoid potential scams or bad projects. By conducting due diligence, investors can make decisions, reduce risks, and identify investment opportunities that align with their investment goals and risk tolerance. Moreover, it will help build the necessary knowledge and understanding foundation to invest in the complex and rapidly growing field of DeFi.</p><p>This requires individuals involved in the DeFi field to consider the following factors when engaging with new projects. </p><h2 id="Fundamentals-of-the-Project"><a href="#Fundamentals-of-the-Project" class="headerlink" title="Fundamentals of the Project"></a>Fundamentals of the Project</h2><p>Understanding the core aspects of a project, such as its purpose, utility, value proposition, and roadmap, is key. Assessing the market size and growth potential of the project is also essential. Reading the whitepaper can provide insights into the project’s objectives, use cases, and technology. The track record of team members reveals their ability to build and scale the product. Finally, the roadmap can be used to judge whether the project is proceeding as planned. It can serve as a supplement to other research, determining the likelihood of the project reaching its milestone goals.</p><h2 id="Popular-metrics-of-DeFi-Protocols"><a href="#Popular-metrics-of-DeFi-Protocols" class="headerlink" title="Popular metrics of DeFi Protocols"></a>Popular metrics of DeFi Protocols</h2><p>There are several popular indicators that can provide valuable references to understand the performance and potential of a DeFi project. These include the token’s Total Value Locked (TVL), 24-hour trading volume, inflation rate, price, and market cap.</p><h3 id="Total-Value-Locked-TVL"><a href="#Total-Value-Locked-TVL" class="headerlink" title="Total Value Locked (TVL)"></a>Total Value Locked (TVL)</h3><p>Total Value Locked (TVL) is the total amount of funds locked in a DeFi protocol. You can view TVL as the total amount deposited by liquidity providers into a protocol. For example, as of July 2023, Uniswap’s TVL exceeds 3.8 billion USD, meaning users have deposited more than $3.8 billion into Uniswap. You can review TVL to assess the overall interest in a specific DeFi protocol or use it to compare which DeFi protocols have “locked” more value. Attracting substantial TVL is often a sign of a strong project.</p><h3 id="Market-Cap"><a href="#Market-Cap" class="headerlink" title="Market Cap"></a>Market Cap</h3><p>The market cap reflects the total value of circulating tokens in a DeFi protocol. It is calculated by multiplying the current price of the token by the number of tokens in circulation. The market cap provides an overall assessment of the protocol’s size and prominence. However, it must be considered in conjunction with other factors such as the utility of the protocol and competition.</p><h3 id="24-hour-Trading-Volume"><a href="#24-hour-Trading-Volume" class="headerlink" title="24-hour Trading Volume"></a>24-hour Trading Volume</h3><p>The 24-hour trading volume measures the total value of transactions executed in a specific DeFi protocol over a 24-hour span. A high 24-hour trading volume indicates that the project is more active and has better liquidity, reflecting an active ecosystem and increased market interest. Conversely, a low 24-hour trading volume shows less buying and selling of the protocol’s tokens, and lower liquidity. Low 24-hour trading volume may signify a lack of interest or participation from traders and investors.</p><h3 id="Inflation-Rate"><a href="#Inflation-Rate" class="headerlink" title="Inflation Rate"></a>Inflation Rate</h3><p>Another important indicator is the inflation rate, referring to the rate at which new tokens are generated within the DeFi protocol. A lower inflation rate might be favorable, as it indicates a more controllable token supply, possibly increasing in value over time, such as ETH transitioning to a deflationary state after moving to POS, and BTC halving, where the new BTC output will be halved. However, assessing the overall token economy and allocation model to ensure a balance between incentivizing participants and maintaining token scarcity is crucial. This topic will be further explored in this article.</p><h3 id="Market-Cap-to-Total-Value-Locked-Ratio-TVL-Ratio"><a href="#Market-Cap-to-Total-Value-Locked-Ratio-TVL-Ratio" class="headerlink" title="Market Cap to Total Value Locked Ratio (TVL Ratio)"></a>Market Cap to Total Value Locked Ratio (TVL Ratio)</h3><p>The Market Cap to TVL ratio provides investors with information about the relationship between a DeFi project’s valuation and demand. For example, if a protocol’s token market cap is low but its TVL is high, it may mean that the token is undervalued. If a token’s TVL ratio is below 1, it may be more attractive, suitable for potential investors.</p><h3 id="Number-of-Addresses"><a href="#Number-of-Addresses" class="headerlink" title="Number of Addresses"></a>Number of Addresses</h3><p>The number of wallet addresses is an indicator that measures the number of DeFi protocol participants. Similar to the user numbers of traditional Web2 applications, it is used to gauge user engagement. An increase in address numbers may be a good sign of growing popularity and adoption. It should be noted that this metric might be manipulated, as in the case of airdrops where an investor might have multiple addresses. Investors should use this metric in conjunction with other indicators.</p><h2 id="Tokenomics"><a href="#Tokenomics" class="headerlink" title="Tokenomics"></a>Tokenomics</h2><p>Tokenomics is a term that originates from the fusion of “token” and “economics.” It is a key concept in the cryptocurrency domain, referring to the economic principles and incentive systems for managing the creation, distribution, and value of tokens within a specific blockchain system.</p><p>Here are some aspects that tokenomics considers:</p><ul><li><p><strong>Token Supply and Issuance</strong>: This involves the total number of existing or to-be-issued tokens. Some tokens have a fixed supply, while others may have a mechanism to mint new tokens. Issuance mechanisms can significantly influence the price and overall economy of the tokens. Fixed supply may lead to scarcity and possibly increase the price but might limit the system’s flexibility. Mintable tokens offer greater flexibility but could lead to inflation if mismanaged. For example, MATIC originally had a fixed supply cap of 10 billion, while the POL white paper allows for a specific rate of annual increase beyond the initial supply of 10 billion (all converted from MATIC).</p></li><li><p><strong>Token Distribution</strong>: How tokens are distributed among users can greatly impact the health and sustainability of the token economy. Fair and reasonable distribution helps maintain a healthy and active user base. Additionally, tokens may be distributed according to a vesting schedule, meaning they can be used within specific periods. This can encourage long-term holding and contribute to the token’s stability.</p></li><li><p><strong>Token Utility</strong>: Tokens usually serve specific functions within their ecosystem. They can provide services, voting rights, or other benefits to users. The utility of a token is a key factor in determining its value. Tokens that provide services, voting rights, or other benefits are more likely to retain value than those without clear use cases.</p></li><li><p><strong>Mining and Staking Incentives</strong>: Many blockchain projects reward users for contributing to network security and stability with newly minted tokens, a practice known as mining or staking. Participation in incentive measures like staking rewards can encourage user engagement. However, this must be balanced to avoid vicious inflation or other unintended consequences.</p></li><li><p><strong>Token Burning</strong>: Some projects may choose to periodically “burn” tokens, permanently removing them from circulation to reduce supply and potentially increase the value of the remaining tokens.</p></li><li><p><strong>Governance</strong>: Many tokens also serve as governance tokens, granting holders voting rights over the project’s future decisions.</p></li><li><p><strong>Economic Policies</strong>: Tokenomics involves implementing economic policies at the protocol level, influencing how tokens are issued, distributed, and managed.</p></li><li><p><strong>Security</strong>: The security of tokens and their underlying blockchain technology is crucial. This includes the security of smart contracts managing the tokens and the overall security of the blockchain.</p></li></ul><p>Tokenomics is vital as it shapes the dynamics of digital assets and projects. By understanding tokenomics, investors can make more informed decisions about buying and holding particular tokens. It can also help project developers design more sustainable and successful token economies. Equally important, remember that the above-mentioned aspects are just things to consider; the “best” token economy may vary depending on the specific goals of the project. For instance, tokens aimed at incentivizing participation in decentralized applications may have different requirements compared to tokens intended to act as stablecoins.</p><h2 id="Assessing-DeFi-Technology"><a href="#Assessing-DeFi-Technology" class="headerlink" title="Assessing DeFi Technology"></a>Assessing DeFi Technology</h2><p>The technology that underpins DeFi (Decentralized Finance) projects is a critical factor in determining their feasibility and effectiveness. One of the key aspects to consider is the blockchain platform on which the project is built. For example, Ethereum is a popular choice, providing a robust and mature infrastructure for DeFi applications. Meanwhile, with the development of Layer 2 solutions, more and more projects are seeking collaboration with OP and ARB. Other Layer 1 blockchain have also emerged, offering promising options for scalable DeFi applications. Investors seeking to acquire DeFi tokens should weigh the pros and cons of the underlying blockchain to make an informed decision.</p><p>Another crucial aspect is the assessment of the smart contracts that drive the DeFi projects. Smart contracts are self-executing contracts on the blockchain, with the terms of the agreement directly written into code. Although audits may not necessarily guarantee security, it is vital for DeFi projects to regularly audit their smart contracts to reduce potential vulnerabilities and protect user funds. Reviewing the smart contract audit history of a project can give insights into the team’s efforts to protect users.</p><p>Interoperability is another key aspect of DeFi tokens, ensuring seamless communication and interaction between different platforms and protocols. It allows tokens to be easily transferred, traded, and used across multiple decentralized applications (DApps) and blockchain networks. Investors should check the interoperability of DeFi tokens before deciding to invest.</p><h2 id="Security"><a href="#Security" class="headerlink" title="Security"></a>Security</h2><p>Given the frequent occurrence of security vulnerabilities in the DeFi (Decentralized Finance) sector, assessing a project’s security measures is crucial. Regular audits by reputable third-party companies, which indicate that the project is actively identifying potential vulnerabilities to ensure the security of smart contracts, are a positive sign. Considering the history of DeFi projects being targeted by hackers and the subsequent panic that ensues, robust security measures are a necessity.</p><p>In addition to audits, incentives similar to bounty hunters exist, such as bug bounty programs, which serve as another indicator of the project’s serious commitment to security. In a bug bounty program, the organization sets specific standards, guidelines, and rewards for individuals who discover and responsibly disclose valid security vulnerabilities. When bug finders identify vulnerabilities, they can submit a detailed report to the organization, outlining the issue, its impact, and the steps to reproduce the flaw. The organization then verifies the reported vulnerability, and if valid, offers a bounty or predetermined monetary reward to the bug-finder. This proactive approach to security encourages a community of security researchers to help improve the overall safety and resilience of the DeFi project.</p><h2 id="Team"><a href="#Team" class="headerlink" title="Team"></a>Team</h2><p>The following are common points to consider in team assessment for stock investments, which may be enlightening for evaluating teams within DeFi (Decentralized Finance) projects:</p><ul><li><strong>Assessing the Skills, Experience, and Qualifications of Team Members:</strong> Look for signs of success in similar enterprises or industries. The collective ability of the team should align with the company’s mission and goals.</li><li><strong>Researching the Team’s Compensation Structure:</strong> This includes salary, bonuses, and stock options (or token allocation in the context of DeFi). Stock options can inflate compensation figures and affect shareholder value. High compensation that doesn’t align with the company’s performance might be a warning signal.</li><li><strong>Observing Insider Buying and Stock (or Token) Repurchases:</strong> If management is purchasing company stock (or tokens), it might indicate their confidence in the company’s future. Conversely, if management is selling stock (or tokens), it may signal potential problems.</li><li><strong>Paying Attention to How Management Communicates with Shareholders and the Public:</strong> Transparency, regular updates, and a clear vision for the company’s future are all signs of strong leadership. Questions can be asked through social media, community channels, and AMAs (Ask Me Anything sessions).</li><li><strong>Company Policies and Mission Statements:</strong> These can provide deep insights into management’s values and strategic direction. Look for evidence of ethical practices and a focus on long-term profitability.</li><li><strong>Being Alert to Potential Red Flags:</strong> Such as past fraudulent activities or ongoing legal issues.</li></ul><p>In the context of DeFi projects, these traditional considerations take on new dimensions, given the decentralized and often pseudonymous nature of blockchain projects. Still, careful analysis of the team’s background, alignment with project goals, compensation structure, and communication strategy can provide valuable insights into the project’s credibility, alignment with stakeholders’ interests, and long-term viability.</p><h2 id="Community-and-Social-Media"><a href="#Community-and-Social-Media" class="headerlink" title="Community and Social Media"></a>Community and Social Media</h2><p>The strength of a project’s community and user adoption is a key indicator of its potential success. These metrics provide valuable insights into the project’s viability, future growth prospects, and overall market acceptance.</p><ul><li>**Emotions and Engagement: ** Emotion is an essential factor in trading and also a reflection of the strength of a project’s community. Engaging with the project’s community is vital to deeply understanding their sentiment, active participation, and overall enthusiasm. This interaction allows you to assess the levels of support, engagement, and trust within the community.</li><li>**Social Media Platforms: ** Platforms like Twitter, Telegram, Discord, and even WeChat provide abundant information about the project community. Investors should monitor discussions, comments, and activity levels on these platforms, looking for signs of positive sentiment, active discussions, and growing follower numbers.</li><li><strong>Positive Sentiment:</strong> Positive sentiment indicates community trust and enthusiasm for the project. It’s often reflected in the tone and content of discussions within social media channels.</li><li><strong>Community Activities:</strong> Participation in community forums, AMAs (Ask Me Anything sessions), and active community channels often indicates a thriving community that is engaged and supportive of the project.</li><li><strong>Warning Signs:</strong> On the flip side, investors should be cautious about manipulated sentiment, paid influencers, or unnatural spikes in activity that could be signs of artificial hype or misleading marketing strategies.</li><li><strong>Importance of Community in DeFi:</strong> In the world of decentralized finance (DeFi), community plays an even more significant role. Decisions related to the development and direction of a project often involve community voting and active participation from the stakeholders. A strong community can foster innovation, support the project through challenges, and contribute to the decentralized ethos that underpins these projects.</li></ul><p>Evaluating the community and social media presence of a DeFi project is a multifaceted task. It requires a nuanced understanding of various platforms, the ability to discern genuine sentiment from manipulation, and an appreciation for the role of the community in driving project success. Combined with other assessment factors like technology, security, and team evaluation, community analysis offers a holistic view of a project’s potential and can guide wise investment decisions.</p><h2 id="VC-investment"><a href="#VC-investment" class="headerlink" title="VC investment"></a>VC investment</h2><p>Venture Capital (VC) investment in a crypto project can have substantial impacts, both positive and negative. Here’s an analysis of how VC investment may affect a crypto or decentralized finance (DeFi) project:</p><h3 id="Positive-Impacts"><a href="#Positive-Impacts" class="headerlink" title="Positive Impacts"></a>Positive Impacts</h3><ul><li><p><strong>Increased Capital</strong>: VC funding provides the necessary capital to scale the project, develop technology, hire talent, and execute the roadmap. It can accelerate growth and enable the project to reach milestones more quickly.</p></li><li><p><strong>Network and Expertise</strong>: VCs often come with vast industry networks and strategic insights. These connections can help in forming partnerships, gaining market insights, and providing mentorship to the team.</p></li><li><p><strong>Trust</strong>: Backing from renowned VCs can bring trust to the project. It signals trust and often attracts further institutional and retail interest.</p></li><li><p><strong>Marketing and Community Building</strong>: VC investment can facilitate more substantial marketing efforts and community engagement, which can result in better visibility and adoption.</p></li></ul><h3 id="Negative-Impacts"><a href="#Negative-Impacts" class="headerlink" title="Negative Impacts"></a>Negative Impacts</h3><ul><li><p><strong>Control and Decision-making</strong>: Depending on the terms, VC investment may come with conditions that affect control and decision-making within the project. VCs might seek seats on the board or have a say in major decisions, potentially leading to conflicts with the original vision of the founders.</p></li><li><p><strong>Token Distribution and Economics</strong>: If not managed carefully, the token distribution to VCs might affect the token economics, potentially leading to an oversupply or concentration of tokens with a few entities. This can impact price stability and community dynamics.</p></li><li><p><strong>Exit Pressure</strong>: VC funds usually seek returns on their investment within a defined period. This can lead to pressures on the project to deliver financial results, sometimes at the expense of long-term innovation or community interests.</p></li><li><p><strong>Potential Misalignment with Decentralization Ethos</strong>: In some cases, VC involvement might be seen as contrary to the decentralized ethos of crypto projects, especially if there’s significant control exerted by the VCs. This could lead to community backlash or a shift away from the project’s original decentralized mission.</p></li></ul><p>VC investment in a crypto project is a multifaceted aspect that carries both opportunities and challenges. While it provides much-needed capital and potential strategic advantages, it also introduces complexities related to control, token economics, and alignment with the project’s vision and values.</p><p>Projects considering VC investment must carefully assess and negotiate the terms to ensure alignment with their long-term goals, community interests, and decentralized principles if they pertain to the specific project. Transparency with the community and stakeholders about the nature and terms of the investment can also be vital in maintaining trust and alignment.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Analyzing DeFi projects is a critical process in making informed investment decisions in the rapidly expanding field of decentralized finance. While the factors outlined above are essential for making wise choices, what’s even more vital is that investments should never exceed what one can afford to lose. Investors should proceed with caution, not only fully understanding the technical foundation, security measures, team competence, community engagement, and social media impact of the project but also scrutinizing potential risks and market volatility. VC investment may bring new opportunities and challenges, and this too requires careful assessment. Ultimately, investing in DeFi or any crypto projects should be based on comprehensive research and understanding, coupled with clear investment strategies and risk management plans. While pursuing returns, maintaining caution and responsibility is of paramount importance.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;How-to-Analyze-DeFi-Projects&quot;&gt;&lt;a href=&quot;#How-to-Analyze-DeFi-Projects&quot; class=&quot;headerlink&quot; title=&quot;How to Analyze DeFi Projects&quot;&gt;&lt;/a&gt;Ho</summary>
      
    
    
    
    
    <category term="DeFi" scheme="http://example.com/tags/DeFi/"/>
    
    <category term="Investment" scheme="http://example.com/tags/Investment/"/>
    
  </entry>
  
  <entry>
    <title>THE LEAN PRODUCT PROCESS</title>
    <link href="http://example.com/2022/09/12/THE-LEAN-PRODUCT-PROCESS/"/>
    <id>http://example.com/2022/09/12/THE-LEAN-PRODUCT-PROCESS/</id>
    <published>2022-09-12T01:01:08.000Z</published>
    <updated>2022-09-12T02:54:50.390Z</updated>
    
    <content type="html"><![CDATA[<h1 id="THE-LEAN-PRODUCT-PROCESS"><a href="#THE-LEAN-PRODUCT-PROCESS" class="headerlink" title="THE LEAN PRODUCT PROCESS"></a>THE LEAN PRODUCT PROCESS</h1><p><em>The Lean Product Playbook</em> written by Dan Olsen shows a detailed guide on how to find product markets and make products people love. Unlike some books that are long in theory and short in practice, this is the first book I’ve read that really explains how to apply Lean Startup concepts in a practical, step-by-step manner. I’m so glad to have the book when I have the passion to build something inspired. </p><p>And this note is more about listing some topics I really love in this book.</p><h1 id="Core-concepts"><a href="#Core-concepts" class="headerlink" title="Core concepts"></a>Core concepts</h1><p>The journey starts with why products failed and followed by the <em><strong>product-market fit</strong></em>, which captures the essence of what it means to build a great product. </p><blockquote><p>Product-market fit:</p><p>My definition of product-market fit…is that you have to build a product that creates significant customer value. This means that your product meets real customer needs and does so in a way that is better than the alternatives.</p></blockquote><p>I used to pay more attention to some ideas or concepts like advertisement, marketing, business models, and how to increase the customer lifetime value, and this definition tells me there is a distinction between <em>creating</em> and <em>capturing</em> values in the process. In order to capture values, you have to create it first.</p><p>The other core concept in this book is the <em><strong>Problem Space and Solution Space</strong></em>. </p><blockquote><p>Any product that you actually build exists in solution space, as do any product designs that you create—such as mockups, wireframes, or prototypes.</p><p>Problem space is where all the customer needs that you’d like your product to deliver live.</p></blockquote><p>And the customer needs can be expanded to some other concepts like job-to-be-done, customer desires, or customer pain points. I attained many courses who starts with idetifying the job-to-be-done, which emphasizes the importace of finding what not be done or well to solve customers real problems. And sometimes I find that some blogs or books often talk about the differences between the “what” versus the “how.” The “what” describes the benefits that the product should give the customer—what the product will accomplish for the user. The “how” is the way in which the product delivers the “what” to the customer. The “how” is the design of the product and the specific technology used to implement the product. “What” is problem space and “how” is solution space.</p><h1 id="The-Lean-product-process"><a href="#The-Lean-product-process" class="headerlink" title="The Lean product process"></a>The Lean product process</h1><p>The content of product management process can be divided into these steps:</p><ul><li><p><strong>Determine your target customers.</strong> This is the bottom of our pyramid since different customers will have different needs and even those who have the same needs may have different views on their relative importance. </p></li><li><p><strong>Identify underserved customer needs.</strong> Before we mover to design a solution, we have to validate the job-to-be-done, customer pain points or customer desires. Since customer needs can seem somewhat fuzzy when we interview them, and they are not skilled at describe the roots of the problem.</p></li><li><p><strong>Define your value proposition.</strong> At this point, we have identified several important customer needs, now we need to decide which ones our product or service will address. </p></li><li><p><strong>Specify your MVP feature set.</strong> Now we have a clear understanding of our value proposition, we have decided on the feature set for our MVP, identifying functionality required to validating we are heading on the right direction.</p></li><li><p><strong>Create your MVP prototype.</strong> At this stage, we want to create a user experience or prototype that we can show to customers so that we can test our ideas.</p></li><li><p><strong>Test your MVP with customers.</strong> In this part, we conduct both qualitative and quotative user test to see how it compares to the target value. </p></li><li><p><strong>Then Iterate to improve product-market fit.</strong> I learned a loop called ‘build-measure-learn’, which means we want to use what we have learned after we receive a round of feedbacks to modify our hypotheses and MVP so that we can test them with customers again. We want to iterate quickly with the goal of improving product-market fit.</p></li></ul><p>Some tasks will be performed in these steps:</p><ul><li><p>Understanding and representing user needs.</p></li><li><p>Defining a vision for a product.</p></li><li><p>Monitoring the market and developing competitive analyses.</p></li><li><p>Aligning stakeholders around the vision for the product.</p></li><li><p>Prioritizing product features and capabilities.</p></li><li><p>Creating a shared brain across larger teams to empower independent decision-making.</p></li></ul><p>There are some benefits of Lean or agile management. First, because we are planning in smaller increments, we can react to changes in the market or new information more quickly. Second, our product can reach customers earlier, which means that we can start hearing feedbacks from customers on our actual products sooner, which helps guide our subsequent product management efforts. Third, teams can reduce their margin of error in estimating scope by working in smaller batch sizes.</p><p><em>TBC… Some concepts in Agile will be added, and some tools will be here in the future after I have the experience of the whole loop. Some other ideas can be the link and differences between design thinking and the lean product process. From a lifelong learning perspective, how to be a good problem-solver?</em></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;THE-LEAN-PRODUCT-PROCESS&quot;&gt;&lt;a href=&quot;#THE-LEAN-PRODUCT-PROCESS&quot; class=&quot;headerlink&quot; title=&quot;THE LEAN PRODUCT PROCESS&quot;&gt;&lt;/a&gt;THE LEAN PRODU</summary>
      
    
    
    
    
    <category term="Notes" scheme="http://example.com/tags/Notes/"/>
    
    <category term="PM" scheme="http://example.com/tags/PM/"/>
    
  </entry>
  
  <entry>
    <title>How to solve problems?</title>
    <link href="http://example.com/2022/09/12/How-to-solve-problems/"/>
    <id>http://example.com/2022/09/12/How-to-solve-problems/</id>
    <published>2022-09-12T00:35:54.000Z</published>
    <updated>2022-09-12T00:43:13.939Z</updated>
    
    <content type="html"><![CDATA[<h1 id="How-to-solve-problems-TBC"><a href="#How-to-solve-problems-TBC" class="headerlink" title="How to solve problems? (TBC)"></a>How to solve problems? (TBC)</h1><p>Nowadays, people are hired not only to realize functions, but more to solve problems. Besides that, in personal life, we are around by various problems. One day, if we are given a problem, how to solve it effectively? And how to be a good problem solver?</p><h1 id="1-Types-of-Problems"><a href="#1-Types-of-Problems" class="headerlink" title="1.  Types of Problems"></a>1.  Types of Problems</h1><p>Before rushing into how to solve problems, let’s start with the types of problems.</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://cdn.staticaly.com/gh/AllenZhuer/image-hosting@master/images/Xnip2022-09-12_08-39-34.5kiie8k66og0.webp"                      alt="Types of problems"                ></p><p>This idea is coming form Problem-Solving: The Owner’s Manual by Pierce Howard, which can be combined with Bohn’s talking about the link between causes and effects in class. Figure 1 shows the types of problems with the general activities to solve them in blue rectangles. Problems can be divided into two parts: solutions are known and unknown. In the first part where solutions are known, the ideal type is the best solution to a problem is obvious based on complete knowledge, then we implement it. If the best solution is not obvious, where we have solutions with certain outcomes, then we should compare and decide one best solution. Sometimes, we have solutions with uncertain outcomes. What we must react is to decide which solution has the highest probability of success. One example here can be how to choose the best design of the garden for maximum residents’ satisfaction. If the potential solutions are in a jumbled list without priority, what we do can be determining the priority order based on the effects. But complete knowledge situation rarely happens. Most times problems come in a way we are not familiar with. If the cause is unknown, the first thing we should do is finding the cause. If the cause is known or irrelevant, we can generate ideas that could fix the problem. So, the messages to take can be different based on the types of problems, which depends on our state of knowledge. </p><h1 id="2-The-process-to-solve-problems"><a href="#2-The-process-to-solve-problems" class="headerlink" title="2.  The process to solve problems"></a>2.  The process to solve problems</h1><p>Problem solving is a process, consisting of 4 key stages, which can be divided to problem definition, solutions generation, evaluating and choosing best solution, and solution validation.</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://cdn.staticaly.com/gh/AllenZhuer/image-hosting@master/images/Xnip2022-09-12_08-39-50.3vx50pkfh960.webp"                      alt="stages to solve problems"                ></p><p>Figure 2: 4 stages to solve problems</p><p>To solve problem effectively and creatively, the first stage should be defining the problem we want to solve. Identify the stakeholders who have a say in the way we solve this problem and what they want. Then describe the vision which should include is the trouble trying to solve now. By asking whys to get the potential root causes we need to address. Then try to write down what we want to achieve in the future. Sometimes we can even get the key factors based on prior knowledge and experience. Some tools may help a lot here, like fishbone diagram to the causes. Some people tend to answer the first problem without checking if this is the right we really want to solve.</p><p>In the next stage, generating solutions. After discovering the real problem that we really want to solve, try to concentrate on generating many solutions such as a target 20 potential alternatives and should not evaluate them at all. Many times, an idea that would have been thrown away immediately, if evaluating properly, can develop into an excellent solution.</p><p>The third stage should be evaluating and choosing the best solution. In evaluating step, some constraints should be taken into consideration like cost, safety, policy, etc. Write the good points and bad points of every solution and state the unique point after looking through all the constraints and targets for all potential solutions. Sometimes, numbers and facts will help us decide the final solutions if the solution can show certain outcomes. For solutions with uncertain outcomes, decision to the best one may be made by feeling, which can be the result of our skills, experience, and knowledgement, place in the right limiting circumstance. Some tools like SWOT can help us to shortlist our potential choices. Besides that, we can also ask our team members for voting. If all the solutions cannot meet the needs, we may go back to the start of the cycle to check the problem statement and go the journey again.</p><p>The last stage should be validating the final solution. The beginning of this stage is deciding how we will make the solution happen. Testing will be made serval times and confirming that the solution has been carried out as planned. Ask our stakeholders’ feedbacks and evaluate the results. Then make the next plan to improve and integrate.</p><h1 id="3-How-to-be-a-creative-problem-solver"><a href="#3-How-to-be-a-creative-problem-solver" class="headerlink" title="3.  How to be a creative problem solver?"></a>3.  How to be a creative problem solver?</h1><p>After learning how to apply different models and tools to solve problems, maybe we can solve new problems we are not familiar, but can we always get a creative and effective solution? The answer is no. But we can always get useful solutions by developing our personality traits. </p><p>After the journey to understand what creativity is, I was totally attracted by these brilliant artists and innovators, leading me on a trip to discover what makes us creative, where ideas come from, and how they come to life.</p><p>Creativity is the ability to see a world in a grain of sand and a heaven in a wild flower, hold infinity in the palm of your hand and eternity in an hour. (William Blake) The ability to perceive and produce novel ideas or products that are useful to society. Creativity is the art of problem solving.</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://cdn.staticaly.com/gh/AllenZhuer/image-hosting@master/images/Xnip2022-09-12_08-40-07.196g0gdg8vxc.webp"                      alt="Creativity process and Problem solving"                ></p><p>Figure 3 shows the 4 stages of creativity and problem solving.</p><p>The process starts with preparation, which is all about gathering information. In this stage, we gather related information and materials by conducting users research and empathize with them in order to get the real pain points and define the problem. At this stage, we can use a variety of ideation methods to understand and build problems and ideas from various angles.</p><p>Stage 2 is incubation, where let mind wander. After collecting information in stage 1, take a step away from the ideas before working on them. Open your mind to all ideas unconsciously, and new connections come slowly.</p><p>Next comes the illumination, where shows Eureka. After a period of incubation, insights arise from the deeper layers of the mind, and the ideas show up suddenly. It’s the sudden Eureka moment! </p><p>The fourth stage is about verification and production. Following the Eureka moment, ideas enter your mind and it’s time to evaluate, test, develop, and verify them. At this stage, you will prototype and test your idea to see if it meets the user needs defined in the preparation stage, get feedbacks and learning, then polish it up.</p><p>From these 4 stages, the creativity process oscillates between convergent and divergent thinking. Incubation and illumination are about inspiration, whereas preparation and validation are about generation. To go forward, we should seek the balance between dreaming and doing. Personal traits will show how we will perform the choice. For example, doers will shine in the preparation and verification stages, and dreamers will be the stars in incubation and illumination stages. If you are a doer, sometimes you may find you can really get things done in a brilliant way but lack the new ideas since focusing on the productivity. For a long-term growth, you can read, travel, meet different people, and open your mind to different views. Clean out the space for dream and imagination. For dreamers, who may be stuck too much in their marvelous ideas, try to practice more and apply the skills into real world. </p><p>Quote to end, ‘Know thyself’. We are on a lifelong journey to learn and grow.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;How-to-solve-problems-TBC&quot;&gt;&lt;a href=&quot;#How-to-solve-problems-TBC&quot; class=&quot;headerlink&quot; title=&quot;How to solve problems? (TBC)&quot;&gt;&lt;/a&gt;How to s</summary>
      
    
    
    
    
    <category term="Notes" scheme="http://example.com/tags/Notes/"/>
    
  </entry>
  
  <entry>
    <title>Why I love Coursera</title>
    <link href="http://example.com/2022/08/03/Why-I-love-Coursera/"/>
    <id>http://example.com/2022/08/03/Why-I-love-Coursera/</id>
    <published>2022-08-03T05:58:09.000Z</published>
    <updated>2022-08-04T07:11:52.481Z</updated>
    
    <content type="html"><![CDATA[<h1 id="What-is-my-favourite-application"><a href="#What-is-my-favourite-application" class="headerlink" title="What is my favourite application?"></a>What is my favourite application?</h1><p>Today, I realized I spent more than 10 hours on Coursera every week for the past year and I really enjoyed this platform. Coursera is a platform that you can build skills with online courses from top institutions and universities. I like it since I can learn a lot from top universities whenever and wherever I am, no time, places limits at a low cost or even free. I learnt a lot  courses on it and I really want to introduce it to you. </p><h1 id="How-did-I-know-Coursera"><a href="#How-did-I-know-Coursera" class="headerlink" title="How did I know Coursera?"></a>How did I know Coursera?</h1><p>I still remember the first time I met Coursera when I was learning Deep Learning taught by Andrew Ng, who was a famous professor in Stanford University. At the end of his class in deeplearning.ai, he would introduce us Coursera created by Daphne Koller and himself. MOOC (Massive Open Online Courses) was bringing educational and vocational courses online and Coursera totally won my fragemental time on PC. Similarly, Weixin Reading introduced online reading to the public in China, and became the most visited app on my phone when I was free. </p><h2 id="What-is-the-customer-journey-map"><a href="#What-is-the-customer-journey-map" class="headerlink" title="What is the customer journey map?"></a>What is the customer journey map?</h2><p>The general process of an individual user can be divided into the following steps.</p><ul><li>Log in as individual user or enterprise users</li><li>Find the courses you want by searching jobs, skills, or universities</li><li>Read the introduction of the course and enroll</li><li>Take the course with your customize schedule</li><li>Take all discussions, tests, peer value, and quiz</li><li>Earn a certificate provided by Coursera and institutions</li><li>Share your learning and certificates to LinkedIn or other platforms</li></ul><h1 id="Business-model-of-Coursera"><a href="#Business-model-of-Coursera" class="headerlink" title="Business model of Coursera"></a>Business model of Coursera</h1><h2 id="Who-are-the-customers-Customer-Segments"><a href="#Who-are-the-customers-Customer-Segments" class="headerlink" title="Who are the customers? (Customer Segments)"></a>Who are the customers? (Customer Segments)</h2><p>Coursera provides services to two main kinds customer segments. The first one is educational institutions like universitties and companies, which wants to share course resources with a fully access. The other one can be the end-users who want to learn some skills in specific fields but may find it difficult to get access to educational resources. </p><p>The following types of people can be their potential end-users:</p><ul><li>High school students who want to take college courses in advance.</li><li>College students who want to take online courses rather than offline courses.</li><li>College students who want to learn extracurricular skills.</li><li>Fresh graduate students who want to decide or change career path.</li><li>People who already have a job want to strengthen their existing knowledge.</li><li>People who already have jobs and want to learn new skills.</li></ul><p>Try to imagine this scenario: more and more people realize the importance of life-long learning. As a fresh graduate, Allen is pursuing  a full-time job related to product management, business development. For a better fit, he chooses courses related to digital product managemt, Data analysis, and project managemtn on Coursera and spends 10 hours per week on it. </p><h2 id="What-value-does-Coursera-deliver-to-customers-Value-Propositions"><a href="#What-value-does-Coursera-deliver-to-customers-Value-Propositions" class="headerlink" title="What value does Coursera deliver to customers? (Value Propositions)"></a>What value does Coursera deliver to customers? (Value Propositions)</h2><p>When it comes to what customer needs Coursera want to satisfy, it allows end-users like me easy and complete access to top educational online resources that  many constomers may find unaffordable or inaccessable through its website and application.</p><p>Sharing high-level courses also means that universities and insititutions can reach a greater number of consumers more easily, futhering their individual brand identities. </p><h2 id="How-does-Coursera-make-money-Revenue-Streams"><a href="#How-does-Coursera-make-money-Revenue-Streams" class="headerlink" title="How does Coursera make money? (Revenue Streams)"></a>How does Coursera make money? (Revenue Streams)</h2><p>Courserea starts its journey with the goal of providing high-level education resources for free to anyone looking for it. Now, Coursera charges from several channels to improve its services. </p><h3 id="Free-access"><a href="#Free-access" class="headerlink" title="Free access"></a>Free access</h3><p>There are many ways to get access to some courses for free since some courses are provided to everyone for free. Besides that,  people can choose audition when choose whether to pay for the course so that  they still have access to most course materials for this course without a certificate.</p><h3 id="Certificates"><a href="#Certificates" class="headerlink" title="Certificates"></a>Certificates</h3><p>Most courses are offered with no charge. However, people who want to earn a certificate of completion which is a trusted and shareable way to showcase their new skills can choose to pay the course fee. This feature was landed in 2013, giving paying students access to graded assignments, homework, and examinations. If completed satisfactorily, they can earn a verified certificate at the end of the course, and they can visit the course in the future with no limits. The course fee varies between from <strong>$39</strong> to <strong>$89</strong> depending on the course. Nowadays, people can choose to be a membership with no limits to a sea of courses at a cost of <strong>$39</strong> per month. </p><p>If you have planty time during some period so that you can finish the course within the7-days trail, once finished, you can also get the certificate and have no limits when revisit the course in the future.</p><h3 id="Specializations"><a href="#Specializations" class="headerlink" title="Specializations"></a>Specializations</h3><p>This feature is a bundle of different but related courses designed to help customers  deepen their expertise in a specific area. For example, DeFi: The future Finance Specialization consisted of 4 courses aims to provide learners a much deeper understanding of both the infrastructure of DeFi and leading applications. Customers can pay monthly subscription fees between <strong>$39</strong> to <strong>$89</strong>, depending on the specialization, for access to all resources and specialization certificates.</p><h3 id="Coursera-for-Business"><a href="#Coursera-for-Business" class="headerlink" title="Coursera for Business"></a>Coursera for Business</h3><p>Coursera for Business enables businesses to offer their employees or teams access to a sea of courses to boost their expertise. Compared to traditional certificates, Coursera provides this service with a lower fee of <strong>$400</strong> per employee per year. Enterprise packages comes with higher cost than personal or teams’ plan. Coursera for Campus provides silimar option to the government and universities.</p><p>Since higher price and growing customers in this segment, it has quickly become a key channel of revenue.</p><h3 id="University-Degrees"><a href="#University-Degrees" class="headerlink" title="University Degrees"></a>University Degrees</h3><p>Collaborated with top universities world wide, Coursera offers students a chance to complete a fully recognized Bachelor’s or Master’s Degree, including MBA from UIUC. The price is far lower compared to degrees in campus, but with the same instructors who teach in person. Scholarships are provided. It’s reported that Coursera generated <a href="(https://s27.q4cdn.com/928340662/files/doc_financials/2021/q1/COUR_Earnings-Infographic_Q1-FY21.pdf)">$12.0M</a> in revenue in Q1, 2021 with 13,493 students enrolled. </p><h1 id="Competitve-analysis"><a href="#Competitve-analysis" class="headerlink" title="Competitve analysis"></a>Competitve analysis</h1><h2 id="SWOT-Analysis"><a href="#SWOT-Analysis" class="headerlink" title="SWOT Analysis"></a>SWOT Analysis</h2><h3 id="Strength"><a href="#Strength" class="headerlink" title="Strength"></a>Strength</h3><ul><li>Strong relationships with its key partners to offer colourful and high-level courses.</li><li>First-move advantages: kind of becoming market standard. After Coursera, many universities or educational angency like MOOC China, and MIT opensource, built their own platform which follows these standards. What’s more, the earlier Coursera ran into the market, the bigger market share it would take. Some MOOC platforms like Xue Tang Zai Xian created by Chinese government and universities has limited market usually in domestic rather than a golbal market. </li><li>Good environment to spread courses. Multi-lanuage courses are provided to learners from all over the world. At the same time, Coursera advocates learners to take part in translation event so that subttitle will be translate into their mother language and would be offered after reviews.</li><li>Diverse revenue channels. Multiple revenue model enables Couresera great resources to not only keep competitve to the threat of newcomers but also invest into R&amp;D.</li></ul><h3 id="Weakness"><a href="#Weakness" class="headerlink" title="Weakness"></a>Weakness</h3><ul><li>Services Coursera provided are more focus on solution space, which means more about what can they do. Sometimes they ignore the interests of external stakeholders.</li><li>Poor visual design. Terrible design on both Andriod and iPhone.</li></ul><h3 id="Opportunities"><a href="#Opportunities" class="headerlink" title="Opportunities"></a>Opportunities</h3><ul><li>Covid-19 has changed many people’s lifestyle. Some universities promoted hybrid teaching model, and sea of students were trying to get access to online learning. Even after Covid, people really enjoy online learning since they can follow their own schedule.</li><li>Globalization enables Coursera more international markets. Growth in international market can also help Coursera diversify the risk as it will be less dependent on domestic market for revenue.</li><li>Technology empowering more possibilities. With the development and application of Machine Learning and AI, more smart features including recommendation can be offered to keep customers active.</li></ul><h3 id="Threats"><a href="#Threats" class="headerlink" title="Threats"></a>Threats</h3><ul><li>Government Regulations. Coursera should keep a close eye on the fast changing government regulations under the growing conflicts environment .</li><li>Customers are moving toward mobile  which can hamper the growth as Coursera  still hasn’t got a comprehensive mobile strategy.</li></ul><h1 id="What-can-be-improved"><a href="#What-can-be-improved" class="headerlink" title="What can be improved?"></a>What can be improved?</h1><h2 id="Nice-improvements-from-my-experiences"><a href="#Nice-improvements-from-my-experiences" class="headerlink" title="Nice improvements from my experiences"></a>Nice improvements from my experiences</h2><p>As a customer, although Coursera provides great services,  for more effience and easy of use, some nice improvments can be the following:</p><ul><li><p><strong>A calendar that shows due dates for assignments and quizzes across all of your courses</strong>. When I signed up for 3-4 classes, it’s hard to keep track of what is due and when. Optional email reminders would be a nice touch.</p></li><li><p><strong>Quick feedbacks and colorful interations to keep users active.</strong> Solving small but not so primitive tasks by students could help feeling the progress. And the more types of such quizzes are presented (not just a multiple choice), the more learners will be willing to join. What’s more, peer value and discussion can be offered for students to communicate more to help each other so that they will spend more time on it to get a better understanding and build the willing to learn other related courses.</p></li><li><p><strong>Better interaction design on Mobile application.</strong> A terrible journey starts when open Coursea on both Android and iPhone. It’s difficult for me to get what I want.</p></li><li><p><strong>Better recommendation performance.</strong> Sometimes, after finishing one course, if learners want a deep understanding of specific field, a good recommendation at the end of learning will keep them engaged.</p></li></ul><p>More testing should be conducted so that we can learn from the feedbacks from users and provide higher-perforance services.</p><h2 id="Future-plan"><a href="#Future-plan" class="headerlink" title="Future plan"></a>Future plan</h2><p>Some plans can be taken into consideration for further marketing.</p><ul><li>Conduct geographic analysis to move to new markets.</li><li>Conduct IP analysis to keep competition out of the door.</li><li> Invest more in building an easy-use mobile application.</li><li>Keep existing customers active so that improve their customers life value. Some nice try can be introducing a new product or service of Coursera to existing customers, improving interaction and visual design.</li><li>Improve customer loyalty of Coursera Coursera’s customers. Marketing efforts can be used to increase loyalty among existing customers by organizing events, and by providing post purchase information and services.</li><li>Increase brand awareness. Features like stronger and multiple sharing channels can reach new audiences on social media. What’s more, newcomers have a better chance of choosing a course through a friend’s recommendation.</li></ul><p>More info and market analysis should be conducted to build a good strategy. Will be added soon.</p><h1 id="Thank-you-for-your-time"><a href="#Thank-you-for-your-time" class="headerlink" title="Thank you for your time : )"></a>Thank you for your time : )</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;What-is-my-favourite-application&quot;&gt;&lt;a href=&quot;#What-is-my-favourite-application&quot; class=&quot;headerlink&quot; title=&quot;What is my favourite applica</summary>
      
    
    
    
    
    <category term="PM" scheme="http://example.com/tags/PM/"/>
    
    <category term="Business Model" scheme="http://example.com/tags/Business-Model/"/>
    
  </entry>
  
  <entry>
    <title>如何在Mac上使用Hexo开启博客之旅</title>
    <link href="http://example.com/2022/01/12/%E5%A6%82%E4%BD%95%E5%9C%A8Mac%E4%B8%8A%E4%BD%BF%E7%94%A8Hexo%E5%BC%80%E5%90%AF%E5%8D%9A%E5%AE%A2%E4%B9%8B%E6%97%85/"/>
    <id>http://example.com/2022/01/12/%E5%A6%82%E4%BD%95%E5%9C%A8Mac%E4%B8%8A%E4%BD%BF%E7%94%A8Hexo%E5%BC%80%E5%90%AF%E5%8D%9A%E5%AE%A2%E4%B9%8B%E6%97%85/</id>
    <published>2022-01-12T01:43:07.000Z</published>
    <updated>2022-01-12T08:09:13.422Z</updated>
    
    <content type="html"><![CDATA[<p>之前使用了搭建在阿里云的轻量级服务器中的WordPress开启了我的第一个博客，可是由于使用起来体验不好，困扰我的可能有以下几点：</p><ol><li>使用阿里云轻量级服务器的过程中，一直出现服务器或数据库连接错误的情况。</li><li>在国内的域名需要备案，出国后就希望能够开启一个比较宽松自由的风格。</li><li>我的需求很简单，只是需要有个简洁的地方写点小东西罢了，倘或碰巧被你看见了，那便是我的幸运吧。虽然WordPress可以提供非常丰富的功能，但是嘞，对我来说有些 ‘乱花渐欲迷人眼’。</li></ol><p>于是转到了使用Hexo + Github Pages的静态博客组合，简单明了。</p><h1 id="什么是Hexo呢？"><a href="#什么是Hexo呢？" class="headerlink" title="什么是Hexo呢？"></a>什么是Hexo呢？</h1><p><a class="link"   href="https://hexo.io/zh-cn/" >Hexo<i class="fas fa-external-link-alt"></i></a> 是一个快速、简洁且高效的博客框架。嗯，有道理，我喜欢。感兴趣的可以在官网上自行查看更多介绍。</p><h1 id="如何配置环境"><a href="#如何配置环境" class="headerlink" title="如何配置环境"></a>如何配置环境</h1><p>安装 Hexo 相当简单，只需要先安装下列应用程序即可：</p><ul><li><p><a class="link"   href="http://nodejs.org/" >Node.js<i class="fas fa-external-link-alt"></i></a> </p><p>使用使用 <a class="link"   href="https://brew.sh/" >Homebrew<i class="fas fa-external-link-alt"></i></a>安装</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew install node</span><br></pre></td></tr></table></figure></li><li><p><a class="link"   href="http://git-scm.com/" >Git<i class="fas fa-external-link-alt"></i></a></p><p>使用 <a class="link"   href="http://mxcl.github.com/homebrew/" >Homebrew<i class="fas fa-external-link-alt"></i></a>, <a class="link"   href="http://www.macports.org/" >MacPorts<i class="fas fa-external-link-alt"></i></a> 或者下载 <a class="link"   href="http://sourceforge.net/projects/git-osx-installer/" >安装程序<i class="fas fa-external-link-alt"></i></a>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew install git</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>安装Hexo</p><p>所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install -g hexo-cli</span><br></pre></td></tr></table></figure></li></ul><h1 id="如何搭建博客"><a href="#如何搭建博客" class="headerlink" title="如何搭建博客"></a>如何搭建博客</h1><ol><li><p>在你喜欢的地方建立一个名字为 ‘blog’ 的文件夹（名字也可按你喜欢的来）</p></li><li><p>进入blog文件夹：<code>cd blog</code></p></li><li><p>初始化：<code>hexo init</code></p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/AllenZhuer/image-hosting/master/images/hexo.1u14tv7unpxc.webp"                      alt="hexo" style="zoom:80%;"                 ></li><li><p>启动Hexo：<code>hexo s</code></p><p>之后会出现一个默认的博文，浏览器输入：<a class="link"   href="http://localhost:4000/" >http://localhost:4000<i class="fas fa-external-link-alt"></i></a>，便可查看。</p></li><li><p>新建一篇博文：<code>hexo n &#39;如何在Mac上使用Hexo开启博客之旅&#39;</code></p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/AllenZhuer/image-hosting/master/images/hexo-n.1qlk84t6ch5s.webp"                      alt="hexo-n"                ></p></li></ol><p>​    接下来就在自己喜欢的编辑器里编写吧，我自己使用过Vscode、Typora、以及Obsidian等来写Markdown以及管理自己的笔记。个人推荐Typora：界面简洁，所见即所得。Typora最近开始收费喽，大家看看自己的荷包吧，Obsidian目前仍然免费，但是功能强大，适合建立自己的知识网络，用来写博客是在大材小用。</p><ol start="6"><li><p>在写完后，同样进入到blog文件夹下，运行</p><p>清理：<code>hexo clean</code></p><p>启动：<code>hexo g</code></p><p>生成：<code>hexo s</code></p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/AllenZhuer/image-hosting/master/images/hexo-s.3j4892swnoe0.webp"                      alt="hexo-s"                ></p><p>再次进入<a class="link"   href="http://localhost:4000/" >http://localhost:4000<i class="fas fa-external-link-alt"></i></a>，便可查看你写好的博客了</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/AllenZhuer/image-hosting/master/images/blog.70psm0hpv200.webp"                      alt="blog"                ></p></li></ol><p>到这里搭建博客就已经成功了，接下来是将博客部署到Github上</p><h1 id="Hexo-Github"><a href="#Hexo-Github" class="headerlink" title="Hexo + Github"></a>Hexo + Github</h1><ol><li>**登录github：<a class="link"   href="https://github.com/" >https://github.com<i class="fas fa-external-link-alt"></i></a>，没账号的新建一个，新创建一个仓库New repository，仓库名必须为：<code>昵称.github.io</code>:**，输入描述，创建：</li></ol><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/AllenZhuer/image-hosting/master/images/github.5adlppym0i00.webp"                      alt="github" style="zoom:50%;"                 ><p>​                在这里因为我已经创建过了，所以会显示已经存在账户。</p><ol start="2"><li><p>接着回答terminal中，crtl + c 结束之前的运行，依旧在blog文件夹下部署github的插件：<code>npm install hexo-deployer-git --save</code></p></li><li><p>接着更改blog文件夹下面**_config.yml**的配置，如下</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/AllenZhuer/image-hosting/master/images/blog2.65i7zq6f44c0.webp"                      alt="blog2"                ></p></li></ol><p>​        可以从这里复制：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: https://github.com/AllenZhuer/AllenZhuer.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure><ol start="4"><li>最后回到terminal，使用 <code>hexo d</code> 将博客部署到远端。中间会让输入github密码，输入即可，后续部署完成，即可访问你的远端地址。例子就是这样的: <a class="link"   href="https://allenzhuer.github.io/" >https://allenzhuer.github.io/<i class="fas fa-external-link-alt"></i></a></li></ol><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/AllenZhuer/image-hosting/master/images/Xnip2022-01-12_15-51-30.7aop23brb5k0.webp"                      alt="Xnip2022-01-12_15-51-30"                ></p><p>这里呢，由于我使用了主题可能会和你的页面有一些差异，不过也没有问题，接下来我们将开始部署主题。</p><h1 id="选个喜欢的主题吧"><a href="#选个喜欢的主题吧" class="headerlink" title="选个喜欢的主题吧"></a>选个喜欢的主题吧</h1><ol><li>最重要的就是选主题了：<a class="link"   href="https://hexo.io/themes/%EF%BC%8C" >https://hexo.io/themes/，<i class="fas fa-external-link-alt"></i></a> 超多主题随你选择。</li></ol><p>我选择的主题为 <a class="link"   href="https://keep-docs.xpoet.cn/" >keep<i class="fas fa-external-link-alt"></i></a>，里面有非常详细的介绍，请移步吧。</p><ol start="2"><li>铛铛，部署完主题后，别忘了它们：</li></ol><p>清理：<code>hexo clean</code></p><p>启动：<code>hexo g</code></p><p>生成：<code>hexo s</code></p><p>部署：<code>hexo d</code></p><p>部署之后，可能有一点点延迟，等一小会儿就行了，捏一捏桌子上的小黄鸭吧。</p><p>如果在你的博客中发现图片加载较慢的问题，推荐一个图床平台 <a class="link"   href="https://picx.xpoet.cn/#/upload" >Picx<i class="fas fa-external-link-alt"></i></a> , 希望能够帮到你。</p><h1 id="域名"><a href="#域名" class="headerlink" title="域名"></a>域名</h1><p>最后一步是购买一个自己喜欢的域名，因为我思来想去的域名已经被别人购买了，并且也没有抱有此博客被许多人看到的想法，所以域名购买暂时告一段落。等到哪天域名过期或者是想到新的域名再来继续更新吧。</p><p>一些购买域名的网站推荐：<a class="link"   href="https://www.tkqlhce.com/jg117mu2-u1HQNLIIIIHJLKORNNO" >NameCheap<i class="fas fa-external-link-alt"></i></a> , <a class="link"   href="https://www.kqzyfj.com/r975xdmjdl09641111021627938" >Godaddy<i class="fas fa-external-link-alt"></i></a> , <a class="link"   href="https://www.namesilo.com/?rid=791ee74iy" >Namesilo<i class="fas fa-external-link-alt"></i></a> 等等。</p><p>如果这篇博客恰巧被你看到并且对你有帮助的话，我将十分开心。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;之前使用了搭建在阿里云的轻量级服务器中的WordPress开启了我的第一个博客，可是由于使用起来体验不好，困扰我的可能有以下几点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用阿里云轻量级服务器的过程中，一直出现服务器或数据库连接错误的情况。&lt;/li&gt;
&lt;li&gt;在国内的域名需要备案，出国</summary>
      
    
    
    
    
    <category term="blog" scheme="http://example.com/tags/blog/"/>
    
  </entry>
  
  <entry>
    <title>What is a Product Roadmap?</title>
    <link href="http://example.com/2022/01/12/What-is-a-Product-Roadmap/"/>
    <id>http://example.com/2022/01/12/What-is-a-Product-Roadmap/</id>
    <published>2022-01-12T01:04:14.000Z</published>
    <updated>2022-01-12T01:35:18.787Z</updated>
    
    <content type="html"><![CDATA[<h1 id="What-is-Product-Roadmap"><a href="#What-is-Product-Roadmap" class="headerlink" title="What is Product Roadmap?"></a>What is Product Roadmap?</h1><p>Product Roadmap is a long-term product development plan that gives all product stakeholders the infomation they need to coordinate plans.</p><h2 id="Why-we-need-a-Product-Roadmap"><a href="#Why-we-need-a-Product-Roadmap" class="headerlink" title="Why we need a Product Roadmap?"></a>Why we need a Product Roadmap?</h2><p>A product roadmap provides a prediction to the product development process.</p><h2 id="Who-are-your-product-stakeholders"><a href="#Who-are-your-product-stakeholders" class="headerlink" title="Who are your product stakeholders?"></a>Who are your product stakeholders?</h2><ul><li>Customers</li><li>Customers-facing groups like sales, marketing, and customer support</li><li>Investors, and sponsors</li><li>Architects, designers, and engineers.</li><li>Human resources</li><li>Legal team</li><li>……</li></ul><h2 id="We-need-both-Product-backlog-and-Product-roadmap"><a href="#We-need-both-Product-backlog-and-Product-roadmap" class="headerlink" title="We need both Product backlog and Product roadmap."></a>We need both Product backlog and Product roadmap.</h2><ul><li>You estimate the dates of the product milestones on the roadmap using a completely independent set of top-down estimates from your product development leader. In the end, these estimates are inaccurate, of course. But as long as they’re close, they’ll serve the purposes of the product roadmap, to allow coordination among the different product stakeholders. Increasingly, your backlog will be directed by the product roadmap,which is one of the main ways you ensure that the activities of the product development team are aligned with the needs and goals of the whole company.<h1 id="Laying-the-grooundwork"><a href="#Laying-the-grooundwork" class="headerlink" title="Laying the grooundwork"></a>Laying the grooundwork</h1><h2 id="Why-the-process-is-important"><a href="#Why-the-process-is-important" class="headerlink" title="Why the process is important?"></a>Why the process is important?</h2></li><li> <em><strong>A roadmap has no value if the stakeholders are not aligned in advance.</strong></em></li><li> <em><strong>The fail usually comes from engineers, designers, and the whole team not being exposed to the project early.</strong></em><h2 id="What-are-the-product-development-purposes"><a href="#What-are-the-product-development-purposes" class="headerlink" title="What are the product development purposes?"></a>What are the product development purposes?</h2></li><li>Serve the business.</li><li>Support overall business strategy.</li><li>Win customer adoption, loyalty.</li><li>Generate revenue.<h2 id="How-to-create-alignment"><a href="#How-to-create-alignment" class="headerlink" title="How to create alignment?"></a>How to create alignment?</h2></li><li>Include stakeholders early.</li><li>Ask for, address, feedback.</li><li>Send updates.<h2 id="What-a-successful-Roadmap-need"><a href="#What-a-successful-Roadmap-need" class="headerlink" title="What a successful Roadmap need?"></a>What a successful Roadmap need?</h2></li><li>Sound strategy.</li><li>Realistic.</li><li>Fully supported.<h2 id="How-to-avoid-failures"><a href="#How-to-avoid-failures" class="headerlink" title="How to avoid failures?"></a>How to avoid failures?</h2></li><li>Spend time with stakeholders at the beginning of the process.</li><li>Ask for underlying thoughts.</li><li>Explain the importance of including others.</li><li>Estimate the development time.<h2 id="Select-your-stakeholders"><a href="#Select-your-stakeholders" class="headerlink" title="Select your stakeholders"></a>Select your stakeholders</h2></li><li>Business leader: allocate the resources and prioritize the tasks are necessary to the product sucess.</li><li>Sales leader: They have the target to hit. And they have the ability to rally resources to make the roadmap sucessful.</li><li>Product development leader: They are response for organizing and motivting their team to hit the roadmap milestones.</li><li>When working across groups, use the sales leader as the your business leader.</li></ul><h2 id="Customers-research"><a href="#Customers-research" class="headerlink" title="Customers research"></a>Customers research</h2><p>There are some question have to figure out:</p><ul><li>What decisions do your customer need to make in choosing your product?</li><li>What problem are they trying to solve?</li><li>What other options are available to them?</li></ul><p>And luckily,  there are some ways you can get to your customers:</p><ul><li>Initiate a research group.</li><li>Ask your customer yourself like make an appointment, a phone call, a zoom meeting, etc. Reach out to your customers directly may be the best way to get insights.</li><li>Participate in sales or customer service meetings.</li></ul><h2 id="Start-with-product-strategy"><a href="#Start-with-product-strategy" class="headerlink" title="Start with product strategy"></a>Start with product strategy</h2><p>A product strategy describes how your company will achieve its business goals. And a product strategy should answer the following questions:</p><ul><li>What are your goals?</li><li>How will you measure your sucess?</li><li>Who are your target customers? Who are you really want to win?</li><li>What are the key needs of your target customers you want to meet?</li><li>What benifit do you provide to customers?</li><li>Who are your competitors?</li><li>What options do your customers have? the alternatives?</li><li>What is the key different between you and your competitors?</li></ul><h1 id="Decision-making-and-building-alignment"><a href="#Decision-making-and-building-alignment" class="headerlink" title="Decision-making and building alignment"></a>Decision-making and building alignment</h1><h2 id="Identify-key-milestones"><a href="#Identify-key-milestones" class="headerlink" title="Identify key milestones"></a>Identify key milestones</h2><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/AllenZhuer/image-hosting/master/images/Product_Milestones.3eqajoine2m0.webp"                      alt="Product Milestone" style="zoom:80%;"                 ><p>Some milestones may address the pain points of existing customers, increasing their usage and reduce the churn. And other milestones may allow your customers use your product in new ways. Some may even do not benifit your customers directly, like open new distribution channel, reducing costs.</p><ul><li>Record the strategy objectives it supports.</li><li>When you finished brainstorms your milestones, give it a priority and meet with your business leader. An one-to-one meeting may increase the chance they share true feedbacks.</li></ul><h2 id="Estimate-leves-of-effort"><a href="#Estimate-leves-of-effort" class="headerlink" title="Estimate leves of effort"></a>Estimate leves of effort</h2><ul><li>First, estimate the development capacity of your team. It usually in the form of developer days, weeks, and months.</li><li>The next is estimate the development time for each milestones. One thing we have to know is not all time is spent on developping. It may cost plenty of time to fix bugs, maintain products, and engineering.</li></ul><h2 id="Building-the-strawman-——-the-first-version"><a href="#Building-the-strawman-——-the-first-version" class="headerlink" title="Building the strawman —— the first version"></a>Building the strawman —— the first version</h2><ul><li>The first step is to roughly sequence your milestones in terms of their value in supporting your product strategy.</li><li>The next step is to schedule these milestones into your roadmap. You can start with the most basic approach to scheduling. Look at your first, highest priority milestone. Use the effort estimate provided by your product development leader,as well as the development capacity of the team, to figure out when on the calendar you could expect it to be delivered.</li><li>Next, you pick up the second highest priority milestone and assuming that the first one is completed on time, schedule that one in the same manner.</li><li>Once done, it may look like this.<br><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/AllenZhuer/image-hosting/master/images/Product_Milestones.3eqajoine2m0.webp"                      alt="Product Milestones"                ></li></ul><p>​        <img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/AllenZhuer/image-hosting/master/images/Milestone_1.74ktqexkz340.webp"                      alt="Milestones" style="zoom:80%;"                 ></p><ul><li>If these milestones are for different products,  you may separate them like this.<br><img                       lazyload                     src="/images/loading.svg"                     data-src="https://raw.githubusercontent.com/AllenZhuer/image-hosting/master/images/Product_Milestones.3eqajoine2m0.webp"                      alt="Milestone_3.png"                ></li></ul><h2 id="Here-comes-the-product-roadmap-meeting"><a href="#Here-comes-the-product-roadmap-meeting" class="headerlink" title="Here comes the product roadmap meeting"></a>Here comes the product roadmap meeting</h2><ul><li>Explain the goal</li><li>Quickly review your product strategy </li><li>Review the development capacity of your team</li><li>Walk through your product roadmap strawman</li><li>Ask the team waht they wish was different</li><li>Modify the roadmap directly, so everyone can see the consequences.</li></ul><p>This process reminds me of the RACI chart, and get all the group members in a warm meeting.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;What-is-Product-Roadmap&quot;&gt;&lt;a href=&quot;#What-is-Product-Roadmap&quot; class=&quot;headerlink&quot; title=&quot;What is Product Roadmap?&quot;&gt;&lt;/a&gt;What is Product </summary>
      
    
    
    
    
    <category term="Notes" scheme="http://example.com/tags/Notes/"/>
    
    <category term="PM" scheme="http://example.com/tags/PM/"/>
    
    <category term="Lifelong_Learning" scheme="http://example.com/tags/Lifelong-Learning/"/>
    
  </entry>
  
</feed>
